{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"IglizQxNGRaf"},"outputs":[],"source":["from IPython.display import HTML, display\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TxIudI7OILnV"},"outputs":[],"source":["!pip install openai docx2txt transformers bert-score --upgrade --quiet"]},{"cell_type":"markdown","metadata":{"id":"sYflNkwXH3lE"},"source":["## Import the modules and define the variables"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":207,"status":"ok","timestamp":1709651114374,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"7j-LACynHuIN","outputId":"bff967bc-4d70-4093-d1b9-c6537fc08f8e"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["import os\n","import shutil\n","import json\n","import time\n","from tqdm.notebook import tqdm\n","\n","import zipfile\n","import docx2txt\n","\n","import pandas as pd\n","\n","from openai import OpenAI\n","from transformers import BertTokenizer, BertForMaskedLM, BertModel\n","from bert_score import BERTScorer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cetHd90mH2LV"},"outputs":[],"source":["os.environ[\"OPENAI_API_KEY\"] = \"\"\n","client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGjMepl3VZ7_"},"outputs":[],"source":["scorer = BERTScorer(lang=\"en\")"]},{"cell_type":"markdown","metadata":{"id":"_OKJNID_KLMc"},"source":["## The Dataset - Transcripts and the human generated summaries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UAWCqDr0xZQF"},"outputs":[],"source":["# !rm -rf /content/few_shot_training_data/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_PbHXN4XIBqh"},"outputs":[],"source":["# If the dataset is in zip format\n","\n","dataset_paths = ['/content/few shot training data.zip']\n","directory_to_extract_to = '/content'\n","\n","for path_to_zip_file in dataset_paths:\n","    with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n","        zip_ref.extractall(directory_to_extract_to)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R7tfoaPRm3x3"},"outputs":[],"source":["# shutil.rmtree('/content/few shot training data/human video w- reading summaries/.ipynb_checkpoints')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1709650091375,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"kEowXQiZKROl","outputId":"67af9a20-4349-4638-fc75-c6cfac2d190d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Size of the dataset: 17\n"]}],"source":["print('Size of the dataset:', len(os.listdir('/content/few shot training data/human video w- reading summaries')))"]},{"cell_type":"markdown","metadata":{"id":"l-8I86RnOzK9"},"source":["### Clean the dataset folder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BGnJcwSAOyx2"},"outputs":[],"source":["# text_files = os.listdir('/content/dataset/text')\n","# summaries = os.listdir('/content/dataset/human generated summaries')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zHHIDCYDQWwv"},"outputs":[],"source":["# os.mkdir('/content/dataset/transcripts')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V22XZH0lPio4"},"outputs":[],"source":["# for files in summaries:\n","#     shutil.copyfile('/content/dataset/text/'+files[:-13]+'.txt', '/content/dataset/transcripts/'+files[:-13]+'.txt')"]},{"cell_type":"markdown","metadata":{"id":"KYA4YzyhRdAO"},"source":["### Load to a dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JdZuVPGmKf0l"},"outputs":[],"source":["def load_to_dataframe(path_to_transcript, path_to_reading_material, path_to_human_summary):\n","\n","    data = pd.DataFrame(columns=['transcript', 'reading_material', 'human_summary'])\n","\n","    summary_files = os.listdir(path_to_human_summary)\n","    transcript_list = []\n","    summary_list = []\n","    reading_list = []\n","\n","    for files in summary_files:\n","        transcript_files = str(files[:-4]+'__transcript.txt').replace(' ', '_')\n","        reading_files = str(files[:-4]+'_rm.txt').replace(' ', '_')\n","\n","        summ_files = os.path.join(path_to_human_summary, files)\n","        transcript_files = os.path.join(path_to_transcript, transcript_files)\n","        reading_files = os.path.join(path_to_reading_material, reading_files)\n","\n","        with open(summ_files, 'r') as fp:\n","            summary_list.append(fp.read())\n","\n","        with open(transcript_files, 'r') as fp:\n","            transcript_list.append(fp.read())\n","\n","        with open(reading_files, 'r') as fp:\n","            reading_list.append(fp.read())\n","\n","    data['transcript'] = pd.Series(transcript_list)\n","    data['reading_material'] = pd.Series(reading_list)\n","    data['human_summary'] = pd.Series(summary_list)\n","\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lBIGnFgBLkiE"},"outputs":[],"source":["path_to_transcript = '/content/few shot training data/transcript text/'\n","path_to_human_summary = '/content/few shot training data/human video w- reading summaries'\n","path_to_reading_material = '/content/few shot training data/reading material'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-nGpMtfmL3r8"},"outputs":[],"source":["data = load_to_dataframe(path_to_transcript, path_to_reading_material, path_to_human_summary)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":246,"status":"ok","timestamp":1709650204977,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"fsDN0XJhOmQW","outputId":"c46c3fca-65b0-43ba-9ab6-2ca464c7c51d"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"data\",\n  \"rows\": 17,\n  \"fields\": [\n    {\n      \"column\": \"transcript\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17,\n        \"samples\": [\n          \"Welcome to the WebAudio API lesson! I personnally love this API, playing with it is a lot of fun as you will discover! I hope you will like it as much as I do! The audio and video elements are used for playing streamed content, but we do not have a real control on the audio. They come with a powerful API as we saw during the previous course and the previous lessons of this course: we can build a custom user interface, make our own play, stop, pause buttons. We can control the video from JavaScript, listen to events and manage playlists, etc. However, we have no real control on the audio signal: fancy visualizations are impossible to do. The ones that dance with the music, and sound effects such as reverberation, delay, make an equalizer, control the stereo, put the signal on the left or on the right is impossible. Furthermore, playing multiple sounds in sync is nearly impossible due to the streamed nature of the signal. For video games, we will need to play very fast different sounds, and you can not wait for the stream to arrive before starting playing it. Web Audio is the solution to all these needs and with Web Audio you will be able to get the output signal from the audio and video elements, and process it with multiple effects. You will be able to work with samples loaded in memory. This will enable perfect syncing, accurate loops, you will be able to mix sounds, etc. You can also generate music programmatically for creating synthetic sounds or virtual instruments. This part will not be covered by this course even if I give links to interesting libraries and demos that do that. Let\\u00e2\\u20ac\\u2122s have a look at some applications. The first thing I wanted to show you is just want we can do with the standard audio element. So this is the standard audio element [music] that just plays a guitar riff that is coming from a server, but we can get control on the audio stream and do such things like that [music]. As you can see I control the stereo balancing here, and we have a real time waveform and volume meters visualization. Another thing we can do is that we can load samples in memory. This is an application I wrote for playing multitracks songs. So we are loading MP3s and decoding them in memory so that we can click anywhere on the song, I can make loops like this [music]. As you can see, we can isolate the tracks, we can mix them in real time. Another application that works with samples in memory is this small example you will learn how to write it in the course: we loaded two different short sounds [sounds] in memory and we can play them repeatedly [sounds] or we can add effects like changing the pitch, changing the volume with some random values and play them with random intervals [sounds]. We can see that the application to video games is straightforward. Another thing you can do is use synthetics sounds, we will not cover the techniques, but you can use some libraries. This is a library that works with synthetic sounds, you do not have to load a file for having these sounds [sounds]. This is a library for making 8 bits sounds like the very first computers and video games in the 80's used to produce. You can also make very complex application, like a vocoder [sounds], or a synthesizer music instrument [sounds]. Ok you have got the idea. This is all the interesting things you can do, and you can also learn how to debug such applications. I will make a video especially for that, but using FireFox, you can activate, in the setting of the dev tools, the Web Audio debug tab. So I clicked here on Web Audio, and this added a new tab here Web Audio, and if I reload the page, I can see the graph corresponding to the route of the signal. Here we have got a source -this is called the audio graph- so we've got the source, and we've got a destination. The source is the original sound. In that case it is a mediaElementAudioSource node that corresponds to the audio element here. The signal goes to an other node that is provided by the Web Audio API and implemented natively in your browser, it is a StereoPanner for separating the sound between left and right. So then it goes to an analyser here that will draw the blue waveform and finally to the destination, and the destination is the speakers. I also routed the signal to another part of the graph just for displaying two different analysers corresponding to the left and right channels. This is for the volume meters here [music]. And if you click on a node, you can see that some node have parameters. On the stereoPanner, that enables me to balance the sound to the left or to the right, you can see if I change that and click again, I can debug the different properties of each node. You will learn how to build this graph, how to assemble the different nodes, what are the most useful nodes for adding effects, controlling the volume, controlling the stereo, making a equalizer, creating fancy visualizations, and so on. Welcome to the Web Audio world and during a few lessons, you will learn step by step how to do such an application.\",\n          \"in this video I'm going to define whether it's probably the most common type of machine learning problem which is supervised learning I'll define supervised learning more formally later but it's probably best to explain I'll start with an example of what it is and we'll do the formal definition later let's say you want to predict housing prices a while back a student collected data sets from the city of Portland Oregon and let's say you plot the data set and it looks like this here on the horizontal axis the size of different houses and square feet and on the vertical axis the practice of different houses in thousands of dollars so given this data let's say you have a friend who owns a house that is saying 750 square feet and they're hoping to sell the house and they want to know how much they can get for the house so how can the learning algorithm help you one thing a learning algorithm might be able to do is put a straight line through the date arrow so the fit a straight line to the data and based on that it looks like maybe their holes can be so full maybe about 150 thousand dollars but maybe this isn't the only learning algorithm you can use and it might be a better one for example instead of fitting a straight line to the data we might decide that it's better to fill a quadratic function or a second-order polynomial to this data and if you do that to make a prediction here then it looks like well maybe they can sell the house well closer to $200,000 one of the things we'll talk about later is how to choose and how to decide do you want to fit a straight line to the data or do you want to fit a quadratic function the data and there's no fair picking whichever one gives your friend the better house to sell but each of these would be a fine example of a learning algorithm so this is an example of a supervised learning algorithm and the term supervised learning refers to the fact that we gave the algorithm a data set in which the right answers were given that is we gave it a data set of houses in which for every exam all in this data set we told it what is the right price what was the actual price that that holds so for and the TAS of the algorithm was to just produce more of these right answers such as for this new house you know that your friend may be trying to sell to define a bit more terminology this is also called a regression problem and by regression problem I mean we're trying to predict a continuous value output namely the price so technically against prices can be rounded off to the nearest cent so maybe prices are actually discrete value but usually we think of the price of a house as a roll number was a scalar value as a continuous value number and the term regression refers to the fact that we're trying to predict this sort of a continuous value to attribute here's another supervised learning examples some friends and I were actually working on this earlier let's say you want to look at medical records and try to predictive a breast cancer as malignant or benign if someone discovers a breast tumor a lump in their breast a malignant tumor is a tumor that is harmful and dangerous and a benign tumor is a tumor this is harmless so obviously people care a lot about this let's see collect the data set and suppose you're in your data set you have on your horizontal axis the size of the tumor and on the vertical axis I'm going to plot 1 or 0 yes or no whether or not these are examples of tumors we've seen before are malignant which is 1 or 0 of not malignant or benign so let's say your data set looks like this where we saw a tumor of this size that turned out to be benign one of this size one of this size and so on and sadly we also saw a few malignant tumors so one of that size one of that size one of that size so on so in this example I have 5 examples of benign tumors shown down here and 5 examples of malignant tumors shown with a vertical axis value of 1 and let's name a friend who tragically has a breast tumor and let's say her breast tumor size maybe somewhere around this value the machine learning question is can you estimate what is the probability what's the chance that the tumor is malignant versus benign to introduce a bit more terminology this is an example of a classification problem the term classification refers to the fact that here we're trying to predict a discrete value output zero or one malignant or benign and it turns out that in classification problem sometimes you can have more than two values for the two possible values for the output as a complete example maybe there are three types of breast cancers and so you may try to predict the discrete value output 0 1 2 or 3 where 0 may mean benign benign tumor so no cancer and one may mean a type 1 cancer I give three types of cancer whatever type one means and two may mean the second type of cancer and 3 may mean a third type of cancer but this would also be a classification problem because this other discrete value set of outputs corresponding to no cancer or cancer type 1 or cancer type 2 or type 3 in classification problems there is another way to plot this data let me show you what I mean I'm going to use a slightly different set of symbols to plot this data so if tumor science is going to be the attribute that I'm going to use to predict malignancy or benign Ness I can also draw my data like this I'm going to use different symbols to denote my benign and malignant or my a negative and positive examples so instead of drawing crosses I'm now going to draw holes for the benign tumors like so and I'm going to keep using X's to denote my malignant tumors ok I hope this make it make sense all I did was I took you know these my data set on top and I just mapped it down to this real line like so and started to use different symbol circles and crosses to denote malignant versus benign examples now in this example we use only one feature or one attribute namely the tumor size in order to predict whether tumor is malignant or benign in other machine learning problems we may have more than one feature or more than one attribute here's an example let's say that instead of just knowing the tumor size we know about the age of the patient and the tumor size in that case maybe your data set would look like this where I may have a set of patients with those ages in that tumor size and look like this and different set of patients that look a little different whose tumors turn out to be malignant as denoted by the crosses so let's say you have a friend who tragically has a tumor and maybe their tumor size and age falls around there so given the data set like this what the learning algorithm may do is for the straight line to the data to try to separate out the malignant tumors from the benign ones and so the learning algorithm may decide to for the straight line like that to separate out the two classes of tumors and you know with this hopefully we can decide that your friend's tumor is more likely so if it was over there then hopefully in the learning algorithm will say that your friend's tumor falls on this benign side and is therefore more likely to be benign than malignant in this example we had two features namely the age of the patient and the size of the tumor in other machine learning problems we will often have more features and my friends that work on this problem they actually use other features like these which is clump thickness clump thickness of the breast tumor uniformity of cell size of the tumor uniformity of cell shape of the tumor and so on and other features as well and it turns out one of the interests most interesting learning algorithms that we'll see in this false as a learning algorithm they can deal with not just two or three or five features but an infinite features on this slide I've listed a total of five different features right - on the axes and three more up here but it turns out that for some learning problems what you really want is not to use like three or five features but instead you want to use an infinite number of features an infinite number of attributes so that your learning algorithm has lots of attributes or features or cues with which to make those predictions so how do you deal with an infinite number of features and so how do you even store an infinite number of things on a computer and your computer can run on the memory well it turns out that when we talk about an algorithm called the support vector machine there will be a neat mathematical trick that will allow a computer to deal with an infinite number of features imagine that I didn't just write down you know two features here and three features on the right but imagine that I wrote down an infinitely long list I just kept writing more and more and more feature that's like an infinitely long list of features turns out we'll be able to come up with an algorithm that can deal with that so just to recap in this class we'll talk about supervised learning and the idea is that in supervised learning in every example in our dataset we are told what is the correct answer that we would have quite like the algorithms are predicted on that example such as the price of the house or whether a tumor is malignant or benign we also talked about the regression problem and by regression that means that our goal is to predict a continuous valued output and we talked about the classification problem where the goal is to predict a discrete value output just a quick wrap-up question suppose you're running a company and you want to develop learning algorithms to address each of two problems in the first problem you have a large inventory of identical items so imagine that you have thousands of copies of some identical item to sell and you want to predict how many of these items you sell over the next three months in the second problem problem to you you light you you have lots of users and you want to write software to examine each individual of your customers accounts so each one of your customers account and for each account decide whether or not the account has been hacked or compromised so for each of these problems should they be treated as a classification problem or as a regression problem when the video pauses please use your mouse to select whichever of these four options on the left you think is the correct answer so hopefully you got that this is the answer for problem 1 I would treat this as a regression problem because if I have you know thousands of items well I would probably just treat this as a real value as a continuous value and treat therefore the number of items I sell as a continuous value and for the second problem I would treat that as a classification problem because I might say set the value I want to predict to be 0 to denote the account has not been hacked and set the value 1 to denote an account that has been hacked into so just like your breast cancers right 0 or benign 1 is malignant so I might set this be 0 or 1 depending whether it's been hacked and have an algorithm try to predict each one of these two discrete values and because that's a small number of discrete values I would therefore treat it as a classification problem so that's it for supervised learning and in the next video I will talk about unsupervised learning which is the other major category of learning algorithm\",\n          \"our first learning algorithm will be linear regression in this video you see what the model looks like and more importantly you also see what the overall process of supervised learning you'll flip let's use a motivating example of predicting housing prices we're going to use a data set of housing prices from the city of Portland Oregon and you're going to plot my data set of a number of houses there were different sizes that were so for a range of different prices let's say that given this data set you have a friend that's trying to sell a house and let's see your friend's house is of size 1,250 square feet and you want to tell them how much they might be able to sell the house for well one thing you could do is fit to a model maybe put a straight line to this data then write something like that and based on that maybe you could tell your friend that looks likely to maybe sell the whole square around to $18,000 so this is an example of a supervised learning algorithm and it's supervised learning because we're given the quote right answer for each of our examples namely were told what was the actual house what was the actual price that each of the houses in our data set was so for and moreover does an example of a regression problem where the term regression refers to the fact that we're predicting a real-valued output maybe the price and then just remind you the other type the other most common type of supervised learning problem is called the classification problem where we predict discrete values outputs such as if we are looking at cancer tumors and trying to decide if a tumor is malignant or benign so there's a 0 1 value distrito more formally in supervised learning we have a data set and this data set is called a training set so for a housing price an example we have a training set of different housing prices and our job is to learn from this data how to predict the prices of the houses let's define some notation that we're using throughout this course I'm going to define quite a lot of symbols is okay if you don't remember all the symbols right now but as the course progresses so be useful with a convenient notation so I'm going to use lowercase M throughout this course to denote the number of training examples so in this data set if I have you know let's say 47 rows in this table then I have 47 training examples and M equals 47 let me use lowercase X to denote the input variables often also called the features so though the X's here will be on input features and I'm going to use Y to denote my output variables or the target variable region to predict its Alexis second column here a little bit more notation I'm going to use X comma Y to denote a single training example so a single row in this table corresponds to a single training example and to refer to a specific training example I'm going to use this notation X I comma Y I and going to use this to refer to the training example so this superscript I over here this is not exponentiation right this X I Y I the superscript I in parenthesis that's just an index into my training set and it refers to the I row in this table okay so this is not except our of iy ^ I instead X I Y I just refers to the I fro of this table so for example X 1 you know refers to the input value for the first training example so that's 21 0 4 represents X to the first row X 2 would be equal to 14 16 right the second x and y 1 will be equal to 460 with that's the first the Y value for my first training example that's what that one refers to so as I mentioned occasionally I'll ask you a question to let you check your own understanding and a few seconds in this video a multiple-choice question will pop up in the video when it does please use your mouse to select what you think is the right answer we're defined by the training set is and so here's how a supervised learning works we solve them of a training set like our training set of housing prices and we feed that to our learning algorithm is the job of a learning algorithm to then output a function which by convention is usually denoted lowercase H and H stands for hypothesis and what the job of the hypothesis is is is a function that takes as input the size of a house like maybe the size of a new house that your friend is trying to sell it takes in a value of X and it tries to output the estimated value of y for their corresponding house so H is a function that map's from X's to YS um people often ask me you know why is this function called a hypothesis some of you may know the meaning of the term hypothesis from the dictionary or from signs or whatever it turns out that the machine learning this is a name that was used in the early days of machine learning and this kind of stuff because maybe not a great name for this sort of function for mapping from sizes of houses to the predictions but you know I think the term hypothesis maybe isn't the best possible name for this but is what this is the standard terminology that people using here you know so don't worry do it don't worry too much about why people call it that when designing a learning algorithm the next thing we need to decide is how do we represent this hypothesis H for this in the next few videos I'm going to choose our initial choice for representing the hypothesis will be the following going to represent H as follows and with the right of this subscript theta of x equals theta 0 plus theta 1 of X and as a shorthand sometimes instead of writing you know H subscript theta of X sometimes it's a shorthand I'll just write this is H of X but more often our rector the subscript theta over there and plotting to some pictures all this means is that we are going to you know predict that Y is a linear function of X right so there's a data set and what this function is doing is just predicting that Y is some straight line function of X s of x equals 3 0 plus theta 1 X ok and why a linear function well sometimes we'll want to fit more complicated perhaps nonlinear functions as well but since this linear case is the simple building block we'll start with this example first so fitting linear functions and we'll build on this to eventually have more complex models in more complex learning algorithms let me also give this particular model a name small though is called linear regression or district example is a actually linear regression with one variable will be variable being X some connecting housing prices functions in one variable X and another name for this model is you need the area linear regression and you need area is just you know a fancy way of saying one variable so that's linear regression in the next video we'll start to talk about just how to go about implementing this model\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reading_material\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17,\n        \"samples\": [\n          \"HTML5 Audio is a subject of the HTML5 specification, incorporating audio input, playback, and synthesis, as well as in the browser. iOS \\n\\n\\n== <audio> element ==\\nThe <audio> element represents a sound, or an audio stream. It is commonly used to play back a single audio file within a web page, showing a GUI widget with play/pause/volume controls.\\nThe <audio> element has these attributes:\\n\\nglobal attributes (accesskey; class; contenteditable; contextmenu; dir; draggable; dropzone; hidden; id; lang; spellcheck; style; tabindex; title; translate)\\nautoplay = \\\"autoplay\\\" or \\\"\\\" (empty string) or emptyInstructs the User-Agent to automatically begin playback of the audio stream as soon as it can do so without stopping.\\npreload = \\\"none\\\" or \\\"metadata\\\" or \\\"auto\\\" or \\\"\\\" (empty string) or emptyRepresents a hint to the User-Agent about whether optimistic downloading of the audio stream itself or its metadata is considered worthwhile.\\n\\\"none\\\": Hints to the User-Agent that the user is not expected to need the audio stream, or that minimizing unnecessary traffic is desirable.\\n\\\"metadata\\\": Hints to the User-Agent that the user is not expected to need the audio stream, but that fetching its metadata (duration and so on) is desirable.\\n\\\"auto\\\": Hints to the User-Agent that optimistically downloading the entire audio stream is considered desirable.\\ncontrols = \\\"controls\\\" or \\\"\\\" (empty string) or emptyInstructs the User-Agent to expose a user interface for controlling playback of the audio stream.\\nloop = \\\"loop\\\" or \\\"\\\" (empty string) or emptyInstructs the User-Agent to seek back to the start of the audio stream upon reaching the end.\\nmediagroup = stringInstructs the User-Agent to link multiple videos and/or audio streams together.\\nmuted = \\\"muted\\\" or \\\"\\\" (empty string) or emptyRepresents the default state of the audio stream, potentially overriding user preferences.\\nsrc = non-empty [URL] potentially surrounded by spacesThe URL for the audio stream.Example:\\n\\n\\n=== Supporting browsers ===\\nOn PC:\\n\\nGoogle Chrome\\nInternet Explorer 9\\nFirefox 3.5\\nOpera 10.5\\nGoogle\\nSafari 3.1On mobile devices:\\n\\nAndroid Browser 2.3\\n\\nGoogle Chrome\\nInternet Explorer Mobile 9\\nSafari 4\\nFirefox\\nOpera Mobile 11\\n\\n\\n== Supported audio coding formats ==\\nThe adoption of HTML5 audio, as with HTML5 video, has become polarized between proponents of free and patent-encumbered formats. In 2007, the recommendation to use Vorbis was retracted from the specification by the W3C together with that to use Ogg Theora, citing the lack of a format accepted by all the major browser vendors.\\nApple and Microsoft support the ISO/IEC-defined formats AAC and the older MP3. Mozilla and Opera support the free and open, royalty-free Vorbis format in Ogg and WebM containers, and criticize the patent-encumbered nature of MP3 and AAC, which are guaranteed to be \\u201cnon-free\\u201d. Google has so far provided support for all common formats.\\nMost AAC files with finite length are wrapped in an MPEG-4 container (.mp4, .m4a), which is supported natively in Internet Explorer, Safari, and Chrome, and supported by the OS in Firefox and Opera. Most AAC live streams with infinite length are wrapped in an Audio Data Transport Stream container (.aac, .adts), which is supported by Chrome, Safari, Firefox and Edge.Many browsers also support uncompressed PCM audio in a WAVE container.In 2012, the free and open royalty-free Opus format was released and standardized by IETF. It is supported by Mozilla, Google, Opera and Edge.This table documents the current support for audio coding formats by the <audio> element.\\n\\n\\n== Web Audio API and MediaStream Processing API ==\\nThe Web Audio API specification developed by W3C describes a high-level JavaScript API for processing and synthesizing audio in web applications. The primary paradigm is of an audio routing graph, where a number of AudioNode objects are connected together to define the overall audio rendering. The actual processing will primarily take place in the underlying implementation (typically optimized Assembly / C / C++ code), but direct JavaScript processing and synthesis is also supported.Mozilla's Firefox browser implements a similar Audio Data API extension since version 4, implemented in 2010  and released in 2011, but Mozilla warns it is non-standard and deprecated, and recommends the Web Audio API instead.\\nSome JavaScript audio processing and synthesis libraries such as Audiolet Archived 2013-01-28 at the Wayback Machine support both APIs.\\nThe W3C Audio Working Group is also considering the MediaStream Processing API specification developed by Mozilla.\\nIn addition to audio mixing and processing, it covers more general media streaming, including synchronization with HTML elements, capture of audio and video streams, and peer-to-peer routing of such media streams.\\n\\n\\n=== Supporting browsers ===\\nOn PC:\\n\\nGoogle Chrome 10 (Enabled by default since 14)\\nFirefox 23 (Enabled by default since 25)\\nOpera 15\\nSafari 6\\nMicrosoft Edge 12\\nOpera GX 36On mobile devices:\\n\\nGoogle Chrome for Android 28 (Enabled by default since 29) and Apple iPads\\nSafari 6 (Has restrictions on use (Muted unless user called))\\nFirefox 23 (Enabled by default since 25)\\nTizen\\n\\n\\n== Web Speech API ==\\nThe Web Speech API aims to provide an alternative input method for web applications (without using a keyboard). With this API, developers can give web apps the ability to transcribe voice to text, from the computer's microphone. The recorded audio is sent to speech servers for transcription, after which the text is typed out for the user. The API itself is agnostic of the underlying speech recognition implementation and can support both server based as well as embedded recognizers.\\nThe HTML Speech Incubator group has proposed the implementation of audio-speech technology in browsers in the form of uniform, cross-platform APIs. The API contains both:\\nSpeech Input API\\nText to Speech APIGoogle integrated this feature into Google Chrome in March 2011. Letting its users search the web with their voice with code like:\\n\\n\\n=== Supporting browsers ===\\nSafari 6.1 and up [PARTIAL: speech synthesis only; no recognition]\\nGoogle Chrome 25 and up\\nFirefox Desktop 44.0 and up (Linux and Mac) / 45.0 and up (Windows) [PARTIAL: speech synthesis only; no recognition; currently requires \\\"media.webspeech.recognition.enable\\\" about:config option to be manually changed to \\\"true\\\"]\\n\\n\\n\",\n          \"Supervised learning (SL) is a paradigm in machine learning where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled supervisory signal) train a model. The training data is processed, building a function that maps new data on expected output values.  An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \\\"reasonable\\\" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.\\n\\n\\n== Steps to follow ==\\nTo solve a given problem of supervised learning, one has to perform the following steps:\\n\\nDetermine the type of training examples. Before doing anything else, the user should decide what kind of data is to be used as a training set. In the case of handwriting analysis, for example, this might be a single handwritten character, an entire handwritten word, an entire sentence of handwriting or perhaps a full paragraph of handwriting.\\nGather a training set. The training set needs to be representative of the real-world use of the function. Thus, a set of input objects is gathered and corresponding outputs are also gathered, either from human experts or from measurements.\\nDetermine the input feature representation of the learned function. The accuracy of the learned function depends strongly on how the input object is represented. Typically, the input object is transformed into a feature vector, which contains a number of features that are descriptive of the object. The number of features should not be too large, because of the curse of dimensionality; but should contain enough information to accurately predict the output.\\nDetermine the structure of the learned function and corresponding learning algorithm. For example, the engineer may choose to use support-vector machines or decision trees.\\nComplete the design. Run the learning algorithm on the gathered training set. Some supervised learning algorithms require the user to determine certain control parameters. These parameters may be adjusted by optimizing performance on a subset (called a validation set) of the training set, or via cross-validation.\\nEvaluate the accuracy of the learned function. After parameter adjustment and learning, the performance of the resulting function should be measured on a test set that is separate from the training set.\\n\\n\\n== Algorithm choice ==\\nA wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem).\\nThere are four major issues to consider in supervised learning:\\n\\n\\n=== Bias-variance tradeoff ===\\n\\nA first issue is the tradeoff between bias and variance. Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  . A learning algorithm has high variance for a particular input \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm. Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be \\\"flexible\\\" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).\\n\\n\\n=== Function complexity and amount of training data ===\\nThe second issue is of the amount of training data available relative to the complexity of the \\\"true\\\" function (classifier or regression function). If the true function is simple, then an \\\"inflexible\\\" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn with a large amount of training data paired with a \\\"flexible\\\" learning algorithm with low bias and high variance.\\n\\n\\n=== Dimensionality of the input space ===\\nA third issue is the dimensionality of the input space. If the input feature vectors have large dimensions, learning the function can be difficult even if the true function only depends on a small number of those features. This is because the many \\\"extra\\\" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, input data of large dimensions typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, it will likely improve the accuracy of the learned function. In addition, there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.\\n\\n\\n=== Noise in the output values ===\\nA fourth issue is the degree of noise in the desired output values (the supervisory target variables). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to overfitting. You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled \\\"corrupts\\\" your training data - this phenomenon has been called deterministic noise. When either type of noise is present, it is better to go with a higher bias, lower variance estimator.\\nIn practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance.\\n\\n\\n=== Other factors to consider ===\\nOther factors to consider when choosing and applying a learning algorithm include the following:\\n\\nHeterogeneity of the data. If the feature vectors include features of many different kinds (discrete, discrete ordered, counts, continuous values), some algorithms are easier to apply than others. Many algorithms, including support-vector machines, linear regression, logistic regression, neural networks, and nearest neighbor methods, require that the input features be numerical and scaled to similar ranges (e.g., to the [-1,1] interval). Methods that employ a distance function, such as nearest neighbor methods and support-vector machines with Gaussian kernels, are particularly sensitive to this. An advantage of decision trees is that they easily handle heterogeneous data.\\nRedundancy in the data. If the input features contain redundant information (e.g., highly correlated features), some learning algorithms (e.g., linear regression, logistic regression, and distance based methods) will perform poorly because of numerical instabilities. These problems can often be solved by imposing some form of regularization.\\nPresence of interactions and non-linearities. If each of the features makes an independent contribution to the output, then algorithms based on linear functions (e.g., linear regression, logistic regression, support-vector machines, naive Bayes) and distance functions (e.g., nearest neighbor methods, support-vector machines with Gaussian kernels) generally perform well. However, if there are complex interactions among features, then algorithms such as decision trees and neural networks work better, because they are specifically designed to discover these interactions. Linear methods can also be applied, but the engineer must manually specify the interactions when using them.When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see cross validation). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.\\n\\n\\n=== Algorithms ===\\nThe most widely used learning algorithms are: \\n\\nSupport-vector machines\\nLinear regression\\nLogistic regression\\nNaive Bayes\\nLinear discriminant analysis\\nDecision trees\\nK-nearest neighbor algorithm\\nNeural networks (Multilayer perceptron)\\nSimilarity learning\\n\\n\\n== How supervised learning algorithms work ==\\nGiven a set of \\n  \\n    \\n      \\n        N\\n      \\n    \\n    {\\\\displaystyle N}\\n   training examples of the form \\n  \\n    \\n      \\n        {\\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            1\\n          \\n        \\n        )\\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        (\\n        \\n          x\\n          \\n            N\\n          \\n        \\n        ,\\n        \\n        \\n          y\\n          \\n            N\\n          \\n        \\n        )\\n        }\\n      \\n    \\n    {\\\\displaystyle \\\\{(x_{1},y_{1}),...,(x_{N},\\\\;y_{N})\\\\}}\\n   such that \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   is the feature vector of the \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  -th example and \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}}\\n   is its label (i.e., class), a learning algorithm seeks a function \\n  \\n    \\n      \\n        g\\n        :\\n        X\\n        \\u2192\\n        Y\\n      \\n    \\n    {\\\\displaystyle g:X\\\\to Y}\\n  , where \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   is the input space and \\n  \\n    \\n      \\n        Y\\n      \\n    \\n    {\\\\displaystyle Y}\\n   is the output space. The function \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   is an element of some space of possible functions \\n  \\n    \\n      \\n        G\\n      \\n    \\n    {\\\\displaystyle G}\\n  , usually called the hypothesis space. It is sometimes convenient to represent \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   using a scoring function \\n  \\n    \\n      \\n        f\\n        :\\n        X\\n        \\u00d7\\n        Y\\n        \\u2192\\n        \\n          R\\n        \\n      \\n    \\n    {\\\\displaystyle f:X\\\\times Y\\\\to \\\\mathbb {R} }\\n   such that \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   is defined as returning the \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n   value that gives the highest score: \\n  \\n    \\n      \\n        g\\n        (\\n        x\\n        )\\n        =\\n        \\n          \\n            \\n              arg\\n              \\u2061\\n              max\\n            \\n            y\\n          \\n        \\n        \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle g(x)={\\\\underset {y}{\\\\arg \\\\max }}\\\\;f(x,y)}\\n  . Let \\n  \\n    \\n      \\n        F\\n      \\n    \\n    {\\\\displaystyle F}\\n   denote the space of scoring functions.\\nAlthough \\n  \\n    \\n      \\n        G\\n      \\n    \\n    {\\\\displaystyle G}\\n   and \\n  \\n    \\n      \\n        F\\n      \\n    \\n    {\\\\displaystyle F}\\n   can be any space of functions, many learning algorithms are probabilistic models where \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   takes the form of a conditional probability model \\n  \\n    \\n      \\n        g\\n        (\\n        x\\n        )\\n        =\\n        \\n          \\n            \\n              arg\\n              \\u2061\\n              max\\n            \\n            y\\n          \\n        \\n        \\n        P\\n        (\\n        y\\n        \\n          |\\n        \\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle g(x)={\\\\underset {y}{\\\\arg \\\\max }}\\\\;P(y|x)}\\n  , or \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   takes the form of a joint probability model \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        =\\n        P\\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x,y)=P(x,y)}\\n  . For example, naive Bayes and linear discriminant analysis are joint probability models, whereas logistic regression is a conditional probability model.\\nThere are two basic approaches to choosing \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   or \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  : empirical risk minimization and structural risk minimization. Empirical risk minimization seeks the function that best fits the training data. Structural risk minimization includes a penalty function that controls the bias/variance tradeoff.\\nIn both cases, it is assumed that the training set consists of a sample of independent and identically distributed pairs, \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n        \\n          y\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{i},\\\\;y_{i})}\\n  . In order to measure how well a function fits the training data, a loss function \\n  \\n    \\n      \\n        L\\n        :\\n        Y\\n        \\u00d7\\n        Y\\n        \\u2192\\n        \\n          \\n            R\\n          \\n          \\n            \\u2265\\n            0\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L:Y\\\\times Y\\\\to \\\\mathbb {R} ^{\\\\geq 0}}\\n   is defined. For training example \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n        \\n          y\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{i},\\\\;y_{i})}\\n  , the loss of predicting the value \\n  \\n    \\n      \\n        \\n          \\n            \\n              y\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\hat {y}}}\\n   is \\n  \\n    \\n      \\n        L\\n        (\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n          \\n            \\n              y\\n              ^\\n            \\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle L(y_{i},{\\\\hat {y}})}\\n  .\\nThe risk \\n  \\n    \\n      \\n        R\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle R(g)}\\n   of function \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   is defined as the expected loss of \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  . This can be estimated from the training data as\\n\\n  \\n    \\n      \\n        \\n          R\\n          \\n            e\\n            m\\n            p\\n          \\n        \\n        (\\n        g\\n        )\\n        =\\n        \\n          \\n            1\\n            N\\n          \\n        \\n        \\n          \\u2211\\n          \\n            i\\n          \\n        \\n        L\\n        (\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        ,\\n        g\\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle R_{emp}(g)={\\\\frac {1}{N}}\\\\sum _{i}L(y_{i},g(x_{i}))}\\n  .\\n\\n\\n=== Empirical risk minimization ===\\n\\nIn empirical risk minimization, the supervised learning algorithm seeks the function \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   that minimizes \\n  \\n    \\n      \\n        R\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle R(g)}\\n  . Hence, a supervised learning algorithm can be constructed by applying an optimization algorithm to find \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  .\\nWhen \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   is a conditional probability distribution \\n  \\n    \\n      \\n        P\\n        (\\n        y\\n        \\n          |\\n        \\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle P(y|x)}\\n   and the loss function is the negative log likelihood: \\n  \\n    \\n      \\n        L\\n        (\\n        y\\n        ,\\n        \\n          \\n            \\n              y\\n              ^\\n            \\n          \\n        \\n        )\\n        =\\n        \\u2212\\n        log\\n        \\u2061\\n        P\\n        (\\n        y\\n        \\n          |\\n        \\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle L(y,{\\\\hat {y}})=-\\\\log P(y|x)}\\n  , then empirical risk minimization is equivalent to maximum likelihood estimation.\\nWhen \\n  \\n    \\n      \\n        G\\n      \\n    \\n    {\\\\displaystyle G}\\n   contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well. This is called overfitting.\\n\\n\\n=== Structural risk minimization ===\\nStructural risk minimization seeks to prevent overfitting by incorporating a regularization penalty into the optimization. The regularization penalty can be viewed as implementing a form of Occam's razor that prefers simpler functions over more complex ones.\\nA wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   is a linear function of the form\\n\\n  \\n    \\n      \\n        g\\n        (\\n        x\\n        )\\n        =\\n        \\n          \\u2211\\n          \\n            j\\n            =\\n            1\\n          \\n          \\n            d\\n          \\n        \\n        \\n          \\u03b2\\n          \\n            j\\n          \\n        \\n        \\n          x\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle g(x)=\\\\sum _{j=1}^{d}\\\\beta _{j}x_{j}}\\n  .A popular regularization penalty is \\n  \\n    \\n      \\n        \\n          \\u2211\\n          \\n            j\\n          \\n        \\n        \\n          \\u03b2\\n          \\n            j\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{j}\\\\beta _{j}^{2}}\\n  , which is the squared Euclidean norm of the weights, also known as the \\n  \\n    \\n      \\n        \\n          L\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L_{2}}\\n   norm. Other norms include the \\n  \\n    \\n      \\n        \\n          L\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L_{1}}\\n   norm, \\n  \\n    \\n      \\n        \\n          \\u2211\\n          \\n            j\\n          \\n        \\n        \\n          |\\n        \\n        \\n          \\u03b2\\n          \\n            j\\n          \\n        \\n        \\n          |\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{j}|\\\\beta _{j}|}\\n  , and the \\n  \\n    \\n      \\n        \\n          L\\n          \\n            0\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L_{0}}\\n   \\\"norm\\\", which is the number of non-zero \\n  \\n    \\n      \\n        \\n          \\u03b2\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\beta _{j}}\\n  s. The penalty will be denoted by \\n  \\n    \\n      \\n        C\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle C(g)}\\n  .\\nThe supervised learning optimization problem is to find the function \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   that minimizes\\n\\n  \\n    \\n      \\n        J\\n        (\\n        g\\n        )\\n        =\\n        \\n          R\\n          \\n            e\\n            m\\n            p\\n          \\n        \\n        (\\n        g\\n        )\\n        +\\n        \\u03bb\\n        C\\n        (\\n        g\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle J(g)=R_{emp}(g)+\\\\lambda C(g).}\\n  The parameter \\n  \\n    \\n      \\n        \\u03bb\\n      \\n    \\n    {\\\\displaystyle \\\\lambda }\\n   controls the bias-variance tradeoff. When \\n  \\n    \\n      \\n        \\u03bb\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle \\\\lambda =0}\\n  , this gives empirical risk minimization with low bias and high variance. When \\n  \\n    \\n      \\n        \\u03bb\\n      \\n    \\n    {\\\\displaystyle \\\\lambda }\\n   is large, the learning algorithm will have high bias and low variance. The value of \\n  \\n    \\n      \\n        \\u03bb\\n      \\n    \\n    {\\\\displaystyle \\\\lambda }\\n   can be chosen empirically via cross validation.\\nThe complexity penalty has a Bayesian interpretation as the negative log prior probability of \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  , \\n  \\n    \\n      \\n        \\u2212\\n        log\\n        \\u2061\\n        P\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle -\\\\log P(g)}\\n  , in which case \\n  \\n    \\n      \\n        J\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle J(g)}\\n   is the posterior probability of \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  .\\n\\n\\n== Generative training ==\\nThe training methods described above are discriminative training methods, because they seek to find a function \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   that discriminates well between the different output values (see discriminative model). For the special case where \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        =\\n        P\\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x,y)=P(x,y)}\\n   is a joint probability distribution and the loss function is the negative log likelihood \\n  \\n    \\n      \\n        \\u2212\\n        \\n          \\u2211\\n          \\n            i\\n          \\n        \\n        log\\n        \\u2061\\n        P\\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        )\\n        ,\\n      \\n    \\n    {\\\\displaystyle -\\\\sum _{i}\\\\log P(x_{i},y_{i}),}\\n   a risk minimization algorithm is said to perform generative training, because \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   can be regarded as a generative model that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in naive Bayes and linear discriminant analysis.\\n\\n\\n== Generalizations ==\\nThere are several ways in which the standard supervised learning problem can be generalized:\\n\\nSemi-supervised learning or weak supervision: the desired output values are provided only for a subset of the training data. The remaining data is unlabeled or imprecisely labeled.\\nActive learning: Instead of assuming that all of the training examples are given at the start, active learning algorithms interactively collect new examples, typically by making queries to a human user. Often, the queries are based on unlabeled data, which is a scenario that combines semi-supervised learning with active learning.\\nStructured prediction: When the desired output value is a complex object, such as a parse tree or a labeled graph, then standard methods must be extended.\\nLearning to rank: When the input is a set of objects and the desired output is a ranking of those objects, then again the standard methods must be extended.\\n\\n\\n== Approaches and algorithms ==\\nAnalytical learning\\nArtificial neural network\\nBackpropagation\\nBoosting (meta-algorithm)\\nBayesian statistics\\nCase-based reasoning\\nDecision tree learning\\nInductive logic programming\\nGaussian process regression\\nGenetic programming\\nGroup method of data handling\\nKernel estimators\\nLearning automata\\nLearning classifier systems\\nLearning vector quantization\\nMinimum message length (decision trees, decision graphs, etc.)\\nMultilinear subspace learning\\nNaive Bayes classifier\\nMaximum entropy classifier\\nConditional random field\\nNearest neighbor algorithm\\nProbably approximately correct learning (PAC) learning\\nRipple down rules, a knowledge acquisition methodology\\nSymbolic machine learning algorithms\\nSubsymbolic machine learning algorithms\\nSupport vector machines\\nMinimum complexity machines (MCM)\\nRandom forests\\nEnsembles of classifiers\\nOrdinal classification\\nData pre-processing\\nHandling imbalanced datasets\\nStatistical relational learning\\nProaftn, a multicriteria classification algorithm\\n\\n\\n== Applications ==\\nBioinformatics\\nCheminformatics\\nQuantitative structure\\u2013activity relationship\\nDatabase marketing\\nHandwriting recognition\\nInformation retrieval\\nLearning to rank\\nInformation extraction\\nObject recognition in computer vision\\nOptical character recognition\\nSpam detection\\nPattern recognition\\nSpeech recognition\\nSupervised learning is a special case of downward causation in biological systems\\nLandform classification using satellite imagery\\nSpend classification in procurement processes\\n\\n\\n== General issues ==\\nComputational learning theory\\nInductive bias\\nOverfitting (machine learning)\\n(Uncalibrated) class membership probabilities\\nUnsupervised learning\\nVersion spaces\\n\\n\\n\",\n          \"In statistics, simple linear regression (SLR) is a linear regression model with a single explanatory variable. That is, it concerns two-dimensional sample points with one independent variable and one dependent variable (conventionally, the x and y coordinates in a Cartesian coordinate system) and finds a linear function (a non-vertical straight line) that, as accurately as possible, predicts the dependent variable values as a function of the independent variable.\\nThe adjective simple refers to the fact that the outcome variable is related to a single predictor.\\nIt is common to make the additional stipulation that the ordinary least squares (OLS) method should be used: the accuracy of each predicted value is measured by its squared residual (vertical distance between the point of the data set and the fitted line), and the goal is to make the sum of these squared deviations as small as possible. \\nIn this case, the slope of the fitted line is equal to the correlation between y and x corrected by the ratio of standard deviations of these variables. The intercept of the fitted line is such that the line passes through the center of mass (x, y) of the data points.\\n\\n\\n== Formulation and computation ==\\nConsider the model function\\n\\n  \\n    \\n      \\n        y\\n        =\\n        \\u03b1\\n        +\\n        \\u03b2\\n        x\\n        ,\\n      \\n    \\n    {\\\\displaystyle y=\\\\alpha +\\\\beta x,}\\n  which describes a line with slope \\u03b2 and y-intercept \\u03b1. In general such a relationship may not hold exactly for the largely unobserved population of values of the independent and dependent variables; we call the unobserved deviations from the above equation the errors.   Suppose we observe n data pairs and call them {(xi, yi), i = 1, ..., n}. We can describe the underlying relationship between yi and xi involving this error term \\u03b5i by\\n\\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n        =\\n        \\u03b1\\n        +\\n        \\u03b2\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        +\\n        \\n          \\u03b5\\n          \\n            i\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle y_{i}=\\\\alpha +\\\\beta x_{i}+\\\\varepsilon _{i}.}\\n  This relationship between the true (but unobserved) underlying parameters \\u03b1 and \\u03b2 and the data points is called a linear regression model.\\nThe goal is to find estimated values \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\alpha }}}\\n   and \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}}\\n   for the parameters \\u03b1 and \\u03b2 which would provide the \\\"best\\\" fit in some sense for the data points. As mentioned in the introduction, in this article the \\\"best\\\" fit will be understood as in the least-squares approach: a line that minimizes the sum of squared residuals (see also Errors and residuals) \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\u03b5\\n                ^\\n              \\n            \\n          \\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\varepsilon }}_{i}}\\n   (differences between actual and predicted values of the dependent variable y), each of which is given by, for any candidate parameter values \\n  \\n    \\n      \\n        \\u03b1\\n      \\n    \\n    {\\\\displaystyle \\\\alpha }\\n   and \\n  \\n    \\n      \\n        \\u03b2\\n      \\n    \\n    {\\\\displaystyle \\\\beta }\\n  ,\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\u03b5\\n                ^\\n              \\n            \\n          \\n          \\n            i\\n          \\n        \\n        =\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        \\u2212\\n        \\u03b1\\n        \\u2212\\n        \\u03b2\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\varepsilon }}_{i}=y_{i}-\\\\alpha -\\\\beta x_{i}.}\\n  In other words, \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\alpha }}}\\n   and \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}}\\n   solve the following minimization problem:\\n\\n  \\n    \\n      \\n        (\\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n        ,\\n        \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n        )\\n        =\\n        argmin\\n        \\u2061\\n        \\n          (\\n          \\n            Q\\n            (\\n            \\u03b1\\n            ,\\n            \\u03b2\\n            )\\n          \\n          )\\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle ({\\\\hat {\\\\alpha }},\\\\,{\\\\hat {\\\\beta }})=\\\\operatorname {argmin} \\\\left(Q(\\\\alpha ,\\\\beta )\\\\right),}\\n  where the objective function Q is:\\n\\n  \\n    \\n      \\n        Q\\n        (\\n        \\u03b1\\n        ,\\n        \\u03b2\\n        )\\n        =\\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            n\\n          \\n        \\n        \\n          \\n            \\n              \\n                \\u03b5\\n                ^\\n              \\n            \\n          \\n          \\n            i\\n          \\n          \\n            \\n            2\\n          \\n        \\n        =\\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            n\\n          \\n        \\n        (\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        \\u2212\\n        \\u03b1\\n        \\u2212\\n        \\u03b2\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        \\n          )\\n          \\n            2\\n          \\n        \\n         \\n        .\\n      \\n    \\n    {\\\\displaystyle Q(\\\\alpha ,\\\\beta )=\\\\sum _{i=1}^{n}{\\\\widehat {\\\\varepsilon }}_{i}^{\\\\,2}=\\\\sum _{i=1}^{n}(y_{i}-\\\\alpha -\\\\beta x_{i})^{2}\\\\ .}\\n  By expanding to get a quadratic expression in \\n  \\n    \\n      \\n        \\u03b1\\n      \\n    \\n    {\\\\displaystyle \\\\alpha }\\n   and \\n  \\n    \\n      \\n        \\u03b2\\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\beta ,}\\n   we can derive minimizing values of the function arguments, denoted \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\alpha }}}\\n   and \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}}\\n  :\\nHere we have introduced\\n\\n\\n=== Expanded Formulas ===\\nThe above equations are efficient to use if the mean of the x and y variables  (\\n  \\n    \\n      \\n        \\n          \\n            \\n              x\\n              \\u00af\\n            \\n          \\n        \\n        \\n           and \\n        \\n        \\n          \\n            \\n              y\\n              \\u00af\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\bar {x}}{\\\\text{ and }}{\\\\bar {y}}}\\n  ) are known.  If the means are not known at the time of calculation, it may be more efficient to use the expanded version of the \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n        \\n           and \\n        \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\alpha }}{\\\\text{ and }}{\\\\widehat {\\\\beta }}}\\n   equations.  These expanded equations may be derived from the more general polynomial regression equations by defining the regression polynomial to be of order 1, as follows.\\n\\n  \\n    \\n      \\n        \\n          \\n            [\\n            \\n              \\n                \\n                  n\\n                \\n                \\n                  \\n                    \\u2211\\n                    \\n                      i\\n                      =\\n                      1\\n                    \\n                    \\n                      n\\n                    \\n                  \\n                  \\n                    x\\n                    \\n                      i\\n                    \\n                  \\n                \\n              \\n              \\n                \\n                  \\n                    \\u2211\\n                    \\n                      i\\n                      =\\n                      1\\n                    \\n                    \\n                      n\\n                    \\n                  \\n                  \\n                    x\\n                    \\n                      i\\n                    \\n                  \\n                \\n                \\n                  \\n                    \\u2211\\n                    \\n                      i\\n                      =\\n                      1\\n                    \\n                    \\n                      n\\n                    \\n                  \\n                  \\n                    x\\n                    \\n                      i\\n                    \\n                    \\n                      2\\n                    \\n                  \\n                \\n              \\n            \\n            ]\\n          \\n        \\n        \\n          \\n            [\\n            \\n              \\n                \\n                  \\n                    \\n                      \\n                        \\u03b1\\n                        ^\\n                      \\n                    \\n                  \\n                \\n              \\n              \\n                \\n                  \\n                    \\n                      \\n                        \\u03b2\\n                        ^\\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            ]\\n          \\n        \\n        =\\n        \\n          \\n            [\\n            \\n              \\n                \\n                  \\n                    \\u2211\\n                    \\n                      i\\n                      =\\n                      0\\n                    \\n                    \\n                      n\\n                    \\n                  \\n                  \\n                    y\\n                    \\n                      i\\n                    \\n                  \\n                \\n              \\n              \\n                \\n                  \\n                    \\u2211\\n                    \\n                      i\\n                      =\\n                      0\\n                    \\n                    \\n                      n\\n                    \\n                  \\n                  \\n                    y\\n                    \\n                      i\\n                    \\n                  \\n                  \\n                    x\\n                    \\n                      i\\n                    \\n                  \\n                \\n              \\n            \\n            ]\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{bmatrix}n&\\\\sum _{i=1}^{n}x_{i}\\\\\\\\\\\\sum _{i=1}^{n}x_{i}&\\\\sum _{i=1}^{n}x_{i}^{2}\\\\end{bmatrix}}{\\\\begin{bmatrix}{\\\\widehat {\\\\alpha }}\\\\\\\\{\\\\widehat {\\\\beta }}\\\\end{bmatrix}}={\\\\begin{bmatrix}\\\\sum _{i=0}^{n}y_{i}\\\\\\\\\\\\sum _{i=0}^{n}y_{i}x_{i}\\\\end{bmatrix}}}\\n  \\nThe above system of linear equations may be solved directly, or stand-alone equations for \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n        \\n           and \\n        \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\alpha }}{\\\\text{ and }}{\\\\widehat {\\\\beta }}}\\n   may be derived by expanding the matrix equations above.  The resultant equations are algebraically equivalent to the ones shown in the prior paragraph, and are shown below without proof.\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n              \\n                \\n                \\n                  \\n                    \\n                      \\u03b1\\n                      ^\\n                    \\n                  \\n                \\n                =\\n                \\n                  \\n                    \\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        y\\n                        \\n                          i\\n                        \\n                      \\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                        \\n                          2\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\n                        y\\n                        \\n                          i\\n                        \\n                      \\n                    \\n                    \\n                      n\\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                        \\n                          2\\n                        \\n                      \\n                      \\u2212\\n                      (\\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n            \\n            \\n              \\n              \\n                \\n                \\n                  \\n                    \\n                      \\u03b2\\n                      ^\\n                    \\n                  \\n                \\n                =\\n                \\n                  \\n                    \\n                      n\\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\n                        y\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        y\\n                        \\n                          i\\n                        \\n                      \\n                    \\n                    \\n                      n\\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                        \\n                          2\\n                        \\n                      \\n                      \\u2212\\n                      (\\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}&\\\\qquad {\\\\widehat {\\\\alpha }}={\\\\frac {\\\\sum _{i=1}^{n}y_{i}\\\\sum _{i=1}^{n}x_{i}^{2}-\\\\sum _{i=1}^{n}x_{i}\\\\sum _{i=1}^{n}x_{i}y_{i}}{n\\\\sum _{i=1}^{n}x_{i}^{2}-(\\\\sum _{i=1}^{n}x_{i})^{2}}}\\\\\\\\[5pt]\\\\\\\\&\\\\qquad {\\\\widehat {\\\\beta }}={\\\\frac {n\\\\sum _{i=1}^{n}x_{i}y_{i}-\\\\sum _{i=1}^{n}x_{i}\\\\sum _{i=1}^{n}y_{i}}{n\\\\sum _{i=1}^{n}x_{i}^{2}-(\\\\sum _{i=1}^{n}x_{i})^{2}}}\\\\\\\\&\\\\qquad \\\\end{aligned}}}\\n  \\n\\n\\n== Interpretation ==\\n\\n\\n=== Relationship with the sample covariance matrix ===\\nThe solution can be reformulated using elements of the covariance matrix:\\n\\nwhere\\n\\nSubstituting the above expressions for \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\alpha }}}\\n   and \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}}\\n   into the original solution yields\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  \\n                    y\\n                    ^\\n                  \\n                \\n              \\n              \\u2212\\n              \\n                \\n                  \\n                    y\\n                    \\u00af\\n                  \\n                \\n              \\n            \\n            \\n              s\\n              \\n                y\\n              \\n            \\n          \\n        \\n        =\\n        \\n          r\\n          \\n            x\\n            y\\n          \\n        \\n        \\n          \\n            \\n              x\\n              \\u2212\\n              \\n                \\n                  \\n                    x\\n                    \\u00af\\n                  \\n                \\n              \\n            \\n            \\n              s\\n              \\n                x\\n              \\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {{\\\\hat {y}}-{\\\\bar {y}}}{s_{y}}}=r_{xy}{\\\\frac {x-{\\\\bar {x}}}{s_{x}}}.}\\n  This shows that rxy is the slope of the regression line of the standardized data points (and that this line passes through the origin). Since \\n  \\n    \\n      \\n        \\u2212\\n        1\\n        \\u2264\\n        \\n          r\\n          \\n            x\\n            y\\n          \\n        \\n        \\u2264\\n        1\\n      \\n    \\n    {\\\\displaystyle -1\\\\leq r_{xy}\\\\leq 1}\\n   then we get that if x is some measurement and y is a followup measurement from the same item, then we expect that y (on average) will be closer to the mean measurement than it was to the original value of x. This phenomenon is known as regressions toward the mean.\\nGeneralizing the \\n  \\n    \\n      \\n        \\n          \\n            \\n              x\\n              \\u00af\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\bar {x}}}\\n   notation, we can write a horizontal bar over an expression to indicate the average value of that expression over the set of samples. For example:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              x\\n              y\\n            \\n            \\u00af\\n          \\n        \\n        =\\n        \\n          \\n            1\\n            n\\n          \\n        \\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            n\\n          \\n        \\n        \\n          x\\n          \\n            i\\n          \\n        \\n        \\n          y\\n          \\n            i\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle {\\\\overline {xy}}={\\\\frac {1}{n}}\\\\sum _{i=1}^{n}x_{i}y_{i}.}\\n  This notation allows us a concise formula for rxy:\\n\\n  \\n    \\n      \\n        \\n          r\\n          \\n            x\\n            y\\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                \\n                  \\n                    x\\n                    y\\n                  \\n                  \\u00af\\n                \\n              \\n              \\u2212\\n              \\n                \\n                  \\n                    x\\n                    \\u00af\\n                  \\n                \\n              \\n              \\n                \\n                  \\n                    y\\n                    \\u00af\\n                  \\n                \\n              \\n            \\n            \\n              \\n                (\\n                \\n                  \\n                    \\n                      \\n                        x\\n                        \\n                          2\\n                        \\n                      \\n                      \\u00af\\n                    \\n                  \\n                  \\u2212\\n                  \\n                    \\n                      \\n                        \\n                          x\\n                          \\u00af\\n                        \\n                      \\n                    \\n                    \\n                      2\\n                    \\n                  \\n                \\n                )\\n              \\n              \\n                (\\n                \\n                  \\n                    \\n                      \\n                        y\\n                        \\n                          2\\n                        \\n                      \\n                      \\u00af\\n                    \\n                  \\n                  \\u2212\\n                  \\n                    \\n                      \\n                        \\n                          y\\n                          \\u00af\\n                        \\n                      \\n                    \\n                    \\n                      2\\n                    \\n                  \\n                \\n                )\\n              \\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle r_{xy}={\\\\frac {{\\\\overline {xy}}-{\\\\bar {x}}{\\\\bar {y}}}{\\\\sqrt {\\\\left({\\\\overline {x^{2}}}-{\\\\bar {x}}^{2}\\\\right)\\\\left({\\\\overline {y^{2}}}-{\\\\bar {y}}^{2}\\\\right)}}}.}\\n  The coefficient of determination (\\\"R squared\\\") is equal to \\n  \\n    \\n      \\n        \\n          r\\n          \\n            x\\n            y\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle r_{xy}^{2}}\\n   when the model is linear with a single independent variable. See sample correlation coefficient for additional details.\\n\\n\\n=== Interpretation about the slope ===\\nBy multiplying all members of the summation in the numerator by : \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  \\n                    \\n                      (\\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      )\\n                    \\n                    \\n                      (\\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      )\\n                    \\n                  \\n                \\n                =\\n                1\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}{\\\\frac {(x_{i}-{\\\\bar {x}})}{(x_{i}-{\\\\bar {x}})}}=1\\\\end{aligned}}}\\n   (thereby not changing it):\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  \\n                    \\n                      \\u03b2\\n                      ^\\n                    \\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      (\\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      )\\n                      (\\n                      \\n                        y\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            y\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      )\\n                    \\n                    \\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      (\\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n                =\\n                \\n                  \\n                    \\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      (\\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                      \\n                        \\n                          \\n                            (\\n                            \\n                              y\\n                              \\n                                i\\n                              \\n                            \\n                            \\u2212\\n                            \\n                              \\n                                \\n                                  y\\n                                  \\u00af\\n                                \\n                              \\n                            \\n                            )\\n                          \\n                          \\n                            (\\n                            \\n                              x\\n                              \\n                                i\\n                              \\n                            \\n                            \\u2212\\n                            \\n                              \\n                                \\n                                  x\\n                                  \\u00af\\n                                \\n                              \\n                            \\n                            )\\n                          \\n                        \\n                      \\n                    \\n                    \\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      (\\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n                =\\n                \\n                  \\u2211\\n                  \\n                    i\\n                    =\\n                    1\\n                  \\n                  \\n                    n\\n                  \\n                \\n                \\n                  \\n                    \\n                      (\\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                    \\n                      \\n                        \\u2211\\n                        \\n                          j\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      (\\n                      \\n                        x\\n                        \\n                          j\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n                \\n                  \\n                    \\n                      (\\n                      \\n                        y\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            y\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      )\\n                    \\n                    \\n                      (\\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      )\\n                    \\n                  \\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}{\\\\widehat {\\\\beta }}&={\\\\frac {\\\\sum _{i=1}^{n}(x_{i}-{\\\\bar {x}})(y_{i}-{\\\\bar {y}})}{\\\\sum _{i=1}^{n}(x_{i}-{\\\\bar {x}})^{2}}}={\\\\frac {\\\\sum _{i=1}^{n}(x_{i}-{\\\\bar {x}})^{2}{\\\\frac {(y_{i}-{\\\\bar {y}})}{(x_{i}-{\\\\bar {x}})}}}{\\\\sum _{i=1}^{n}(x_{i}-{\\\\bar {x}})^{2}}}=\\\\sum _{i=1}^{n}{\\\\frac {(x_{i}-{\\\\bar {x}})^{2}}{\\\\sum _{j=1}^{n}(x_{j}-{\\\\bar {x}})^{2}}}{\\\\frac {(y_{i}-{\\\\bar {y}})}{(x_{i}-{\\\\bar {x}})}}\\\\\\\\[6pt]\\\\end{aligned}}}\\n  We can see that the slope (tangent of angle) of the regression line is the weighted average of \\n  \\n    \\n      \\n        \\n          \\n            \\n              (\\n              \\n                y\\n                \\n                  i\\n                \\n              \\n              \\u2212\\n              \\n                \\n                  \\n                    y\\n                    \\u00af\\n                  \\n                \\n              \\n              )\\n            \\n            \\n              (\\n              \\n                x\\n                \\n                  i\\n                \\n              \\n              \\u2212\\n              \\n                \\n                  \\n                    x\\n                    \\u00af\\n                  \\n                \\n              \\n              )\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {(y_{i}-{\\\\bar {y}})}{(x_{i}-{\\\\bar {x}})}}}\\n   that is the slope (tangent of angle) of the line that connects the i-th point to the average of all points, weighted by \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        \\u2212\\n        \\n          \\n            \\n              x\\n              \\u00af\\n            \\n          \\n        \\n        \\n          )\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle (x_{i}-{\\\\bar {x}})^{2}}\\n   because the further the point is the more \\\"important\\\" it is, since small errors in its position will affect the slope connecting it to the center point more.\\n\\n\\n=== Interpretation about the intercept ===\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  \\n                    \\n                      \\u03b1\\n                      ^\\n                    \\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      y\\n                      \\u00af\\n                    \\n                  \\n                \\n                \\u2212\\n                \\n                  \\n                    \\n                      \\u03b2\\n                      ^\\n                    \\n                  \\n                \\n                \\n                \\n                  \\n                    \\n                      x\\n                      \\u00af\\n                    \\n                  \\n                \\n                ,\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}{\\\\widehat {\\\\alpha }}&={\\\\bar {y}}-{\\\\widehat {\\\\beta }}\\\\,{\\\\bar {x}},\\\\\\\\[5pt]\\\\end{aligned}}}\\n  Given \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n        =\\n        tan\\n        \\u2061\\n        (\\n        \\u03b8\\n        )\\n        =\\n        d\\n        y\\n        \\n          /\\n        \\n        d\\n        x\\n        \\u2192\\n        d\\n        y\\n        =\\n        d\\n        x\\n        \\u00d7\\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}=\\\\tan(\\\\theta )=dy/dx\\\\rightarrow dy=dx\\\\times {\\\\widehat {\\\\beta }}}\\n   with \\n  \\n    \\n      \\n        \\u03b8\\n      \\n    \\n    {\\\\displaystyle \\\\theta }\\n   the angle the line makes with the positive x axis, \\nwe have \\n  \\n    \\n      \\n        \\n          y\\n          \\n            \\n              i\\n              n\\n              t\\n              e\\n              r\\n              s\\n              e\\n              c\\n              t\\n              i\\n              o\\n              n\\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              y\\n              \\u00af\\n            \\n          \\n        \\n        \\u2212\\n        d\\n        x\\n        \\u00d7\\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              y\\n              \\u00af\\n            \\n          \\n        \\n        \\u2212\\n        d\\n        y\\n      \\n    \\n    {\\\\displaystyle y_{\\\\rm {intersection}}={\\\\bar {y}}-dx\\\\times {\\\\widehat {\\\\beta }}={\\\\bar {y}}-dy}\\n  \\n\\n\\n=== Interpretation about the correlation ===\\nIn the above formulation, notice that each \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   is a constant (\\\"known upfront\\\") value, while the \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}}\\n   are random variables that depend on the linear function of \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   and the random term \\n  \\n    \\n      \\n        \\n          \\u03b5\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\varepsilon _{i}}\\n  . This assumption is used when deriving the standard error of the slope and showing that it is unbiased.\\nIn this framing, when \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   is not actually a random variable, what type of parameter does the empirical correlation \\n  \\n    \\n      \\n        \\n          r\\n          \\n            x\\n            y\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle r_{xy}}\\n   estimate? The issue is that for each value i we'll have: \\n  \\n    \\n      \\n        E\\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        )\\n        =\\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle E(x_{i})=x_{i}}\\n   and \\n  \\n    \\n      \\n        V\\n        a\\n        r\\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        )\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle Var(x_{i})=0}\\n  . A possible interpretation of \\n  \\n    \\n      \\n        \\n          r\\n          \\n            x\\n            y\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle r_{xy}}\\n   is to imagine that \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   defines a random variable drawn from the empirical distribution of the x values in our sample. For example, if x had 10 values from the natural numbers: [1,2,3...,10], then we can imagine x to be a Discrete uniform distribution. Under this interpretation all \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   have the same expectation and some positive variance. With this interpretation we can think of \\n  \\n    \\n      \\n        \\n          r\\n          \\n            x\\n            y\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle r_{xy}}\\n   as the estimator of the Pearson's correlation between the random variable y and the random variable x (as we just defined it).\\n\\n\\n== Numerical properties ==\\n\\n\\n== Statistical properties ==\\nDescription of the statistical properties of estimators from the simple linear regression estimates requires the use of a statistical model. The following is based on assuming the validity of a model under which the estimates are optimal. It is also possible to evaluate the properties under other assumptions, such as inhomogeneity, but this is discussed elsewhere.\\n\\n\\n=== Unbiasedness ===\\nThe estimators \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\alpha }}}\\n   and \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}}\\n   are unbiased.\\nTo formalize this assertion we must define a framework in which these estimators are random variables. We consider the residuals \\u03b5i as random variables drawn independently from some distribution with mean zero. In other words, for each value of x, the corresponding value of y is generated as a mean response  \\u03b1 + \\u03b2x plus an additional random variable \\u03b5 called the error term, equal to zero on average. Under such interpretation, the least-squares estimators \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\alpha }}}\\n   and \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}}\\n   will themselves be random variables whose means will equal the \\\"true values\\\" \\u03b1 and \\u03b2. This is the definition of an unbiased estimator.\\n\\n\\n=== Confidence intervals ===\\n\\nThe formulas given in the previous section allow one to calculate the point estimates of \\u03b1 and \\u03b2 \\u2014 that is, the coefficients of the regression line for the given set of data. However, those formulas don't tell us how precise the estimates are, i.e., how much the estimators \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\alpha }}}\\n   and \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}}\\n   vary from sample to sample for the specified sample size. Confidence intervals were devised to give a plausible set of values to the estimates one might have if one repeated the experiment a very large number of times.\\nThe standard method of constructing confidence intervals for linear regression coefficients relies on the normality assumption, which is justified if either:\\n\\nthe errors in the regression are normally distributed (the so-called classic regression assumption), or\\nthe number of observations n is sufficiently large, in which case the estimator is approximately normally distributed.The latter case is justified by the central limit theorem.\\n\\n\\n==== Normality assumption ====\\nUnder the first assumption above, that of the normality of the error terms, the estimator of the slope coefficient will itself be normally distributed with mean \\u03b2 and variance \\n  \\n    \\n      \\n        \\n          \\u03c3\\n          \\n            2\\n          \\n        \\n        \\n          /\\n          \\n            \\u2211\\n            (\\n            \\n              x\\n              \\n                i\\n              \\n            \\n            \\u2212\\n            \\n              \\n                \\n                  x\\n                  \\u00af\\n                \\n              \\n            \\n            \\n              )\\n              \\n                2\\n              \\n            \\n          \\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\sigma ^{2}\\\\left/\\\\sum (x_{i}-{\\\\bar {x}})^{2}\\\\right.,}\\n   where \\u03c32 is the variance of the error terms (see Proofs involving ordinary least squares).  At the same time the sum of squared residuals Q is distributed proportionally to \\u03c72 with n \\u2212 2 degrees of freedom, and independently from \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}}\\n  . This allows us to construct a t-value\\n\\n  \\n    \\n      \\n        t\\n        =\\n        \\n          \\n            \\n              \\n                \\n                  \\n                    \\u03b2\\n                    ^\\n                  \\n                \\n              \\n              \\u2212\\n              \\u03b2\\n            \\n            \\n              s\\n              \\n                \\n                  \\n                    \\u03b2\\n                    ^\\n                  \\n                \\n              \\n            \\n          \\n        \\n         \\n        \\u223c\\n         \\n        \\n          t\\n          \\n            n\\n            \\u2212\\n            2\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle t={\\\\frac {{\\\\widehat {\\\\beta }}-\\\\beta }{s_{\\\\widehat {\\\\beta }}}}\\\\ \\\\sim \\\\ t_{n-2},}\\n  where\\n\\n  \\n    \\n      \\n        \\n          s\\n          \\n            \\n              \\n                \\u03b2\\n                ^\\n              \\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                \\n                  \\n                    1\\n                    \\n                      n\\n                      \\u2212\\n                      2\\n                    \\n                  \\n                \\n                \\n                  \\u2211\\n                  \\n                    i\\n                    =\\n                    1\\n                  \\n                  \\n                    n\\n                  \\n                \\n                \\n                  \\n                    \\n                      \\n                        \\u03b5\\n                        ^\\n                      \\n                    \\n                  \\n                  \\n                    i\\n                  \\n                  \\n                    \\n                    2\\n                  \\n                \\n              \\n              \\n                \\n                  \\u2211\\n                  \\n                    i\\n                    =\\n                    1\\n                  \\n                  \\n                    n\\n                  \\n                \\n                (\\n                \\n                  x\\n                  \\n                    i\\n                  \\n                \\n                \\u2212\\n                \\n                  \\n                    \\n                      x\\n                      \\u00af\\n                    \\n                  \\n                \\n                \\n                  )\\n                  \\n                    2\\n                  \\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle s_{\\\\widehat {\\\\beta }}={\\\\sqrt {\\\\frac {{\\\\frac {1}{n-2}}\\\\sum _{i=1}^{n}{\\\\widehat {\\\\varepsilon }}_{i}^{\\\\,2}}{\\\\sum _{i=1}^{n}(x_{i}-{\\\\bar {x}})^{2}}}}}\\n  is the unbiased standard error estimator of the estimator \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}}\\n  .\\nThis t-value has a Student's t-distribution with n \\u2212 2 degrees of freedom. Using it we can construct a confidence interval for \\u03b2:\\n\\n  \\n    \\n      \\n        \\u03b2\\n        \\u2208\\n        \\n          [\\n          \\n            \\n              \\n                \\n                  \\u03b2\\n                  ^\\n                \\n              \\n            \\n            \\u2212\\n            \\n              s\\n              \\n                \\n                  \\n                    \\u03b2\\n                    ^\\n                  \\n                \\n              \\n            \\n            \\n              t\\n              \\n                n\\n                \\u2212\\n                2\\n              \\n              \\n                \\u2217\\n              \\n            \\n            ,\\n             \\n            \\n              \\n                \\n                  \\u03b2\\n                  ^\\n                \\n              \\n            \\n            +\\n            \\n              s\\n              \\n                \\n                  \\n                    \\u03b2\\n                    ^\\n                  \\n                \\n              \\n            \\n            \\n              t\\n              \\n                n\\n                \\u2212\\n                2\\n              \\n              \\n                \\u2217\\n              \\n            \\n          \\n          ]\\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\beta \\\\in \\\\left[{\\\\widehat {\\\\beta }}-s_{\\\\widehat {\\\\beta }}t_{n-2}^{*},\\\\ {\\\\widehat {\\\\beta }}+s_{\\\\widehat {\\\\beta }}t_{n-2}^{*}\\\\right],}\\n  at confidence level (1 \\u2212 \\u03b3), where \\n  \\n    \\n      \\n        \\n          t\\n          \\n            n\\n            \\u2212\\n            2\\n          \\n          \\n            \\u2217\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle t_{n-2}^{*}}\\n   is the \\n  \\n    \\n      \\n        \\n          \\n            (\\n            \\n              1\\n              \\n              \\u2212\\n              \\n              \\n                \\n                  \\u03b3\\n                  2\\n                \\n              \\n            \\n            )\\n          \\n          \\n            -th\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\scriptstyle \\\\left(1\\\\;-\\\\;{\\\\frac {\\\\gamma }{2}}\\\\right){\\\\text{-th}}}\\n   quantile of the tn\\u22122 distribution. For example, if \\u03b3 = 0.05 then the confidence level is 95%.\\nSimilarly, the confidence interval for the intercept coefficient \\u03b1 is given by\\n\\n  \\n    \\n      \\n        \\u03b1\\n        \\u2208\\n        \\n          [\\n          \\n            \\n              \\n                \\n                  \\u03b1\\n                  ^\\n                \\n              \\n            \\n            \\u2212\\n            \\n              s\\n              \\n                \\n                  \\n                    \\u03b1\\n                    ^\\n                  \\n                \\n              \\n            \\n            \\n              t\\n              \\n                n\\n                \\u2212\\n                2\\n              \\n              \\n                \\u2217\\n              \\n            \\n            ,\\n             \\n            \\n              \\n                \\n                  \\u03b1\\n                  ^\\n                \\n              \\n            \\n            +\\n            \\n              s\\n              \\n                \\n                  \\n                    \\u03b1\\n                    ^\\n                  \\n                \\n              \\n            \\n            \\n              t\\n              \\n                n\\n                \\u2212\\n                2\\n              \\n              \\n                \\u2217\\n              \\n            \\n          \\n          ]\\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\alpha \\\\in \\\\left[{\\\\widehat {\\\\alpha }}-s_{\\\\widehat {\\\\alpha }}t_{n-2}^{*},\\\\ {\\\\widehat {\\\\alpha }}+s_{\\\\widehat {\\\\alpha }}t_{n-2}^{*}\\\\right],}\\n  at confidence level (1 \\u2212 \\u03b3), where\\n\\n  \\n    \\n      \\n        \\n          s\\n          \\n            \\n              \\n                \\u03b1\\n                ^\\n              \\n            \\n          \\n        \\n        =\\n        \\n          s\\n          \\n            \\n              \\n                \\u03b2\\n                ^\\n              \\n            \\n          \\n        \\n        \\n          \\n            \\n              \\n                1\\n                n\\n              \\n            \\n            \\n              \\u2211\\n              \\n                i\\n                =\\n                1\\n              \\n              \\n                n\\n              \\n            \\n            \\n              x\\n              \\n                i\\n              \\n              \\n                2\\n              \\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                1\\n                \\n                  n\\n                  (\\n                  n\\n                  \\u2212\\n                  2\\n                  )\\n                \\n              \\n            \\n            \\n              (\\n              \\n                \\n                  \\u2211\\n                  \\n                    i\\n                    =\\n                    1\\n                  \\n                  \\n                    n\\n                  \\n                \\n                \\n                  \\n                    \\n                      \\n                        \\u03b5\\n                        ^\\n                      \\n                    \\n                  \\n                  \\n                    i\\n                  \\n                  \\n                    \\n                    2\\n                  \\n                \\n              \\n              )\\n            \\n            \\n              \\n                \\n                  \\n                    \\u2211\\n                    \\n                      i\\n                      =\\n                      1\\n                    \\n                    \\n                      n\\n                    \\n                  \\n                  \\n                    x\\n                    \\n                      i\\n                    \\n                    \\n                      2\\n                    \\n                  \\n                \\n                \\n                  \\n                    \\u2211\\n                    \\n                      i\\n                      =\\n                      1\\n                    \\n                    \\n                      n\\n                    \\n                  \\n                  (\\n                  \\n                    x\\n                    \\n                      i\\n                    \\n                  \\n                  \\u2212\\n                  \\n                    \\n                      \\n                        x\\n                        \\u00af\\n                      \\n                    \\n                  \\n                  \\n                    )\\n                    \\n                      2\\n                    \\n                  \\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle s_{\\\\widehat {\\\\alpha }}=s_{\\\\widehat {\\\\beta }}{\\\\sqrt {{\\\\frac {1}{n}}\\\\sum _{i=1}^{n}x_{i}^{2}}}={\\\\sqrt {{\\\\frac {1}{n(n-2)}}\\\\left(\\\\sum _{i=1}^{n}{\\\\widehat {\\\\varepsilon }}_{i}^{\\\\,2}\\\\right){\\\\frac {\\\\sum _{i=1}^{n}x_{i}^{2}}{\\\\sum _{i=1}^{n}(x_{i}-{\\\\bar {x}})^{2}}}}}}\\n  The confidence intervals for \\u03b1 and \\u03b2 give us the general idea where these regression coefficients are most likely to be. For example, in the Okun's law regression shown here the point estimates are\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n        =\\n        0.859\\n        ,\\n        \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n        =\\n        \\u2212\\n        1.817.\\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\alpha }}=0.859,\\\\qquad {\\\\widehat {\\\\beta }}=-1.817.}\\n  The 95% confidence intervals for these estimates are\\n\\n  \\n    \\n      \\n        \\u03b1\\n        \\u2208\\n        \\n          [\\n          \\n            \\n            0.76\\n            ,\\n            0.96\\n          \\n          ]\\n        \\n        ,\\n        \\n        \\u03b2\\n        \\u2208\\n        \\n          [\\n          \\n            \\u2212\\n            2.06\\n            ,\\n            \\u2212\\n            1.58\\n            \\n          \\n          ]\\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\alpha \\\\in \\\\left[\\\\,0.76,0.96\\\\right],\\\\qquad \\\\beta \\\\in \\\\left[-2.06,-1.58\\\\,\\\\right].}\\n  In order to represent this information graphically, in the form of the confidence bands around the regression line, one has to proceed carefully and account for the joint distribution of the estimators. It can be shown that at confidence level (1 \\u2212 \\u03b3) the confidence band has hyperbolic form given by the equation\\n\\n  \\n    \\n      \\n        (\\n        \\u03b1\\n        +\\n        \\u03b2\\n        \\u03be\\n        )\\n        \\u2208\\n        \\n          [\\n          \\n            \\n            \\n              \\n                \\n                  \\u03b1\\n                  ^\\n                \\n              \\n            \\n            +\\n            \\n              \\n                \\n                  \\u03b2\\n                  ^\\n                \\n              \\n            \\n            \\u03be\\n            \\u00b1\\n            \\n              t\\n              \\n                n\\n                \\u2212\\n                2\\n              \\n              \\n                \\u2217\\n              \\n            \\n            \\n              \\n                \\n                  (\\n                  \\n                    \\n                      \\n                        1\\n                        \\n                          n\\n                          \\u2212\\n                          2\\n                        \\n                      \\n                    \\n                    \\u2211\\n                    \\n                      \\n                        \\n                          \\n                            \\u03b5\\n                            ^\\n                          \\n                        \\n                      \\n                      \\n                        i\\n                      \\n                      \\n                        \\n                        2\\n                      \\n                    \\n                  \\n                  )\\n                \\n                \\u22c5\\n                \\n                  (\\n                  \\n                    \\n                      \\n                        1\\n                        n\\n                      \\n                    \\n                    +\\n                    \\n                      \\n                        \\n                          (\\n                          \\u03be\\n                          \\u2212\\n                          \\n                            \\n                              \\n                                x\\n                                \\u00af\\n                              \\n                            \\n                          \\n                          \\n                            )\\n                            \\n                              2\\n                            \\n                          \\n                        \\n                        \\n                          \\u2211\\n                          (\\n                          \\n                            x\\n                            \\n                              i\\n                            \\n                          \\n                          \\u2212\\n                          \\n                            \\n                              \\n                                x\\n                                \\u00af\\n                              \\n                            \\n                          \\n                          \\n                            )\\n                            \\n                              2\\n                            \\n                          \\n                        \\n                      \\n                    \\n                  \\n                  )\\n                \\n              \\n            \\n            \\n          \\n          ]\\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle (\\\\alpha +\\\\beta \\\\xi )\\\\in \\\\left[\\\\,{\\\\widehat {\\\\alpha }}+{\\\\widehat {\\\\beta }}\\\\xi \\\\pm t_{n-2}^{*}{\\\\sqrt {\\\\left({\\\\frac {1}{n-2}}\\\\sum {\\\\widehat {\\\\varepsilon }}_{i}^{\\\\,2}\\\\right)\\\\cdot \\\\left({\\\\frac {1}{n}}+{\\\\frac {(\\\\xi -{\\\\bar {x}})^{2}}{\\\\sum (x_{i}-{\\\\bar {x}})^{2}}}\\\\right)}}\\\\,\\\\right].}\\n  When the model assumed the intercept is fixed and equal to 0 (\\n  \\n    \\n      \\n        \\u03b1\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle \\\\alpha =0}\\n  ), the standard error of the slope turns into:\\n\\n  \\n    \\n      \\n        \\n          s\\n          \\n            \\n              \\n                \\u03b2\\n                ^\\n              \\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                1\\n                \\n                  n\\n                  \\u2212\\n                  1\\n                \\n              \\n            \\n            \\n              \\n                \\n                  \\n                    \\u2211\\n                    \\n                      i\\n                      =\\n                      1\\n                    \\n                    \\n                      n\\n                    \\n                  \\n                  \\n                    \\n                      \\n                        \\n                          \\u03b5\\n                          ^\\n                        \\n                      \\n                    \\n                    \\n                      i\\n                    \\n                    \\n                      \\n                      2\\n                    \\n                  \\n                \\n                \\n                  \\n                    \\u2211\\n                    \\n                      i\\n                      =\\n                      1\\n                    \\n                    \\n                      n\\n                    \\n                  \\n                  \\n                    x\\n                    \\n                      i\\n                    \\n                    \\n                      2\\n                    \\n                  \\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle s_{\\\\widehat {\\\\beta }}={\\\\sqrt {{\\\\frac {1}{n-1}}{\\\\frac {\\\\sum _{i=1}^{n}{\\\\widehat {\\\\varepsilon }}_{i}^{\\\\,2}}{\\\\sum _{i=1}^{n}x_{i}^{2}}}}}}\\n  With: \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\u03b5\\n                ^\\n              \\n            \\n          \\n          \\n            i\\n          \\n        \\n        =\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        \\u2212\\n        \\n          \\n            \\n              \\n                y\\n                ^\\n              \\n            \\n          \\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\hat {\\\\varepsilon }}_{i}=y_{i}-{\\\\hat {y}}_{i}}\\n  \\n\\n\\n==== Asymptotic assumption ====\\nThe alternative second assumption states that when the number of points in the dataset is \\\"large enough\\\", the law of large numbers and the central limit theorem become applicable, and then the distribution of the estimators is approximately normal. Under this assumption all formulas derived in the previous section remain valid, with the only exception that the quantile t*n\\u22122 of Student's t distribution is replaced with the quantile q* of the standard normal distribution. Occasionally the fraction 1/n\\u22122 is replaced with 1/n. When n is large such a change does not alter the results appreciably.\\n\\n\\n== Numerical example ==\\n\\nThis data set gives average masses for women as a function of their height in a sample of American women of age 30\\u201339. Although the OLS article argues that it would be more appropriate to run a quadratic regression for this data, the simple linear regression model is applied here instead.\\n\\nThere are n = 15 points in this data set. Hand calculations would be started by finding the following five sums:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  S\\n                  \\n                    x\\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\u2211\\n                \\n                  x\\n                  \\n                    i\\n                  \\n                \\n                \\n                =\\n                24.76\\n                ,\\n                \\n                \\n                  S\\n                  \\n                    y\\n                  \\n                \\n                =\\n                \\u2211\\n                \\n                  y\\n                  \\n                    i\\n                  \\n                \\n                \\n                =\\n                931.17\\n                ,\\n              \\n            \\n            \\n              \\n                \\n                  S\\n                  \\n                    x\\n                    x\\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\u2211\\n                \\n                  x\\n                  \\n                    i\\n                  \\n                  \\n                    2\\n                  \\n                \\n                =\\n                41.0532\\n                ,\\n                \\n                \\n                \\n                \\n                  S\\n                  \\n                    y\\n                    y\\n                  \\n                \\n                =\\n                \\u2211\\n                \\n                  y\\n                  \\n                    i\\n                  \\n                  \\n                    2\\n                  \\n                \\n                =\\n                58498.5439\\n                ,\\n              \\n            \\n            \\n              \\n                \\n                  S\\n                  \\n                    x\\n                    y\\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\u2211\\n                \\n                  x\\n                  \\n                    i\\n                  \\n                \\n                \\n                  y\\n                  \\n                    i\\n                  \\n                \\n                =\\n                1548.2453\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}S_{x}&=\\\\sum x_{i}\\\\,=24.76,\\\\qquad S_{y}=\\\\sum y_{i}\\\\,=931.17,\\\\\\\\[5pt]S_{xx}&=\\\\sum x_{i}^{2}=41.0532,\\\\;\\\\;\\\\,S_{yy}=\\\\sum y_{i}^{2}=58498.5439,\\\\\\\\[5pt]S_{xy}&=\\\\sum x_{i}y_{i}=1548.2453\\\\end{aligned}}}\\n  These quantities would be used to calculate the estimates of the regression coefficients, and their standard errors.\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  \\n                    \\n                      \\u03b2\\n                      ^\\n                    \\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      n\\n                      \\n                        S\\n                        \\n                          x\\n                          y\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        S\\n                        \\n                          x\\n                        \\n                      \\n                      \\n                        S\\n                        \\n                          y\\n                        \\n                      \\n                    \\n                    \\n                      n\\n                      \\n                        S\\n                        \\n                          x\\n                          x\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        S\\n                        \\n                          x\\n                        \\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n                =\\n                61.272\\n              \\n            \\n            \\n              \\n                \\n                  \\n                    \\n                      \\u03b1\\n                      ^\\n                    \\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    1\\n                    n\\n                  \\n                \\n                \\n                  S\\n                  \\n                    y\\n                  \\n                \\n                \\u2212\\n                \\n                  \\n                    \\n                      \\u03b2\\n                      ^\\n                    \\n                  \\n                \\n                \\n                  \\n                    1\\n                    n\\n                  \\n                \\n                \\n                  S\\n                  \\n                    x\\n                  \\n                \\n                =\\n                \\u2212\\n                39.062\\n              \\n            \\n            \\n              \\n                \\n                  s\\n                  \\n                    \\u03b5\\n                  \\n                  \\n                    2\\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    1\\n                    \\n                      n\\n                      (\\n                      n\\n                      \\u2212\\n                      2\\n                      )\\n                    \\n                  \\n                \\n                \\n                  [\\n                  \\n                    n\\n                    \\n                      S\\n                      \\n                        y\\n                        y\\n                      \\n                    \\n                    \\u2212\\n                    \\n                      S\\n                      \\n                        y\\n                      \\n                      \\n                        2\\n                      \\n                    \\n                    \\u2212\\n                    \\n                      \\n                        \\n                          \\n                            \\u03b2\\n                            ^\\n                          \\n                        \\n                      \\n                      \\n                        2\\n                      \\n                    \\n                    (\\n                    n\\n                    \\n                      S\\n                      \\n                        x\\n                        x\\n                      \\n                    \\n                    \\u2212\\n                    \\n                      S\\n                      \\n                        x\\n                      \\n                      \\n                        2\\n                      \\n                    \\n                    )\\n                  \\n                  ]\\n                \\n                =\\n                0.5762\\n              \\n            \\n            \\n              \\n                \\n                  s\\n                  \\n                    \\n                      \\n                        \\u03b2\\n                        ^\\n                      \\n                    \\n                  \\n                  \\n                    2\\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      n\\n                      \\n                        s\\n                        \\n                          \\u03b5\\n                        \\n                        \\n                          2\\n                        \\n                      \\n                    \\n                    \\n                      n\\n                      \\n                        S\\n                        \\n                          x\\n                          x\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        S\\n                        \\n                          x\\n                        \\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n                =\\n                3.1539\\n              \\n            \\n            \\n              \\n                \\n                  s\\n                  \\n                    \\n                      \\n                        \\u03b1\\n                        ^\\n                      \\n                    \\n                  \\n                  \\n                    2\\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\n                  s\\n                  \\n                    \\n                      \\n                        \\u03b2\\n                        ^\\n                      \\n                    \\n                  \\n                  \\n                    2\\n                  \\n                \\n                \\n                  \\n                    1\\n                    n\\n                  \\n                \\n                \\n                  S\\n                  \\n                    x\\n                    x\\n                  \\n                \\n                =\\n                8.63185\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}{\\\\widehat {\\\\beta }}&={\\\\frac {nS_{xy}-S_{x}S_{y}}{nS_{xx}-S_{x}^{2}}}=61.272\\\\\\\\[8pt]{\\\\widehat {\\\\alpha }}&={\\\\frac {1}{n}}S_{y}-{\\\\widehat {\\\\beta }}{\\\\frac {1}{n}}S_{x}=-39.062\\\\\\\\[8pt]s_{\\\\varepsilon }^{2}&={\\\\frac {1}{n(n-2)}}\\\\left[nS_{yy}-S_{y}^{2}-{\\\\widehat {\\\\beta }}^{2}(nS_{xx}-S_{x}^{2})\\\\right]=0.5762\\\\\\\\[8pt]s_{\\\\widehat {\\\\beta }}^{2}&={\\\\frac {ns_{\\\\varepsilon }^{2}}{nS_{xx}-S_{x}^{2}}}=3.1539\\\\\\\\[8pt]s_{\\\\widehat {\\\\alpha }}^{2}&=s_{\\\\widehat {\\\\beta }}^{2}{\\\\frac {1}{n}}S_{xx}=8.63185\\\\end{aligned}}}\\n  The 0.975 quantile of Student's t-distribution with 13 degrees of freedom is t*13 = 2.1604, and thus the 95% confidence intervals for \\u03b1 and \\u03b2 are\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n              \\n                \\u03b1\\n                \\u2208\\n                [\\n                \\n                \\n                  \\n                    \\n                      \\u03b1\\n                      ^\\n                    \\n                  \\n                \\n                \\u2213\\n                \\n                  t\\n                  \\n                    13\\n                  \\n                  \\n                    \\u2217\\n                  \\n                \\n                \\n                  s\\n                  \\n                    \\u03b1\\n                  \\n                \\n                \\n                ]\\n                =\\n                [\\n                \\n                \\n                  \\u2212\\n                  45.4\\n                \\n                ,\\n                 \\n                \\n                  \\u2212\\n                  32.7\\n                \\n                \\n                ]\\n              \\n            \\n            \\n              \\n              \\n                \\u03b2\\n                \\u2208\\n                [\\n                \\n                \\n                  \\n                    \\n                      \\u03b2\\n                      ^\\n                    \\n                  \\n                \\n                \\u2213\\n                \\n                  t\\n                  \\n                    13\\n                  \\n                  \\n                    \\u2217\\n                  \\n                \\n                \\n                  s\\n                  \\n                    \\u03b2\\n                  \\n                \\n                \\n                ]\\n                =\\n                [\\n                \\n                57.4\\n                ,\\n                 \\n                65.1\\n                \\n                ]\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}&\\\\alpha \\\\in [\\\\,{\\\\widehat {\\\\alpha }}\\\\mp t_{13}^{*}s_{\\\\alpha }\\\\,]=[\\\\,{-45.4},\\\\ {-32.7}\\\\,]\\\\\\\\[5pt]&\\\\beta \\\\in [\\\\,{\\\\widehat {\\\\beta }}\\\\mp t_{13}^{*}s_{\\\\beta }\\\\,]=[\\\\,57.4,\\\\ 65.1\\\\,]\\\\end{aligned}}}\\n  The product-moment correlation coefficient might also be calculated:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              r\\n              ^\\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              n\\n              \\n                S\\n                \\n                  x\\n                  y\\n                \\n              \\n              \\u2212\\n              \\n                S\\n                \\n                  x\\n                \\n              \\n              \\n                S\\n                \\n                  y\\n                \\n              \\n            \\n            \\n              (\\n              n\\n              \\n                S\\n                \\n                  x\\n                  x\\n                \\n              \\n              \\u2212\\n              \\n                S\\n                \\n                  x\\n                \\n                \\n                  2\\n                \\n              \\n              )\\n              (\\n              n\\n              \\n                S\\n                \\n                  y\\n                  y\\n                \\n              \\n              \\u2212\\n              \\n                S\\n                \\n                  y\\n                \\n                \\n                  2\\n                \\n              \\n              )\\n            \\n          \\n        \\n        =\\n        0.9946\\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {r}}={\\\\frac {nS_{xy}-S_{x}S_{y}}{\\\\sqrt {(nS_{xx}-S_{x}^{2})(nS_{yy}-S_{y}^{2})}}}=0.9946}\\n  \\n\\n\\n== Alternatives ==\\nIn SLR, there is an underlying assumption that only the dependent variable contains measurement error; if the explanatory variable is also measured with error, then simple regression is not appropriate for estimating the underlying relationship because it will be biased due to regression dilution. \\nOther estimation methods that can be used in place of ordinary least squares include least absolute deviations (minimizing the sum of absolute values of residuals) and the Theil\\u2013Sen estimator (which chooses a line whose slope is the median of the slopes determined by pairs of sample points). \\nDeming regression (total least squares) also finds a line that fits a set of two-dimensional sample points, but (unlike ordinary least squares, least absolute deviations, and median slope regression) it is not really an instance of simple linear regression, because it does not separate the coordinates into one dependent and one independent variable and could potentially return a vertical line as its fit. can lead to a model that attempts to fit the outliers more than the data.\\n\\n\\n=== Line fitting ===\\n\\n\\n=== Simple linear regression without the intercept term (single regressor) ===\\nSometimes it is appropriate to force the regression line to pass through the origin, because x and y are assumed to be proportional. For the model without the intercept term, y = \\u03b2x, the OLS estimator for \\u03b2 simplifies to\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                \\u2211\\n                \\n                  i\\n                  =\\n                  1\\n                \\n                \\n                  n\\n                \\n              \\n              \\n                x\\n                \\n                  i\\n                \\n              \\n              \\n                y\\n                \\n                  i\\n                \\n              \\n            \\n            \\n              \\n                \\u2211\\n                \\n                  i\\n                  =\\n                  1\\n                \\n                \\n                  n\\n                \\n              \\n              \\n                x\\n                \\n                  i\\n                \\n                \\n                  2\\n                \\n              \\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                x\\n                y\\n              \\n              \\u00af\\n            \\n            \\n              \\n                x\\n                \\n                  2\\n                \\n              \\n              \\u00af\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}={\\\\frac {\\\\sum _{i=1}^{n}x_{i}y_{i}}{\\\\sum _{i=1}^{n}x_{i}^{2}}}={\\\\frac {\\\\overline {xy}}{\\\\overline {x^{2}}}}}\\n  Substituting (x \\u2212 h, y \\u2212 k) in place of (x, y) gives the regression through (h, k):\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  \\n                    \\n                      \\u03b2\\n                      ^\\n                    \\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      (\\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      h\\n                      )\\n                      (\\n                      \\n                        y\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      k\\n                      )\\n                    \\n                    \\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      (\\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      h\\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n                =\\n                \\n                  \\n                    \\n                      \\n                        (\\n                        x\\n                        \\u2212\\n                        h\\n                        )\\n                        (\\n                        y\\n                        \\u2212\\n                        k\\n                        )\\n                      \\n                      \\u00af\\n                    \\n                    \\n                      \\n                        (\\n                        x\\n                        \\u2212\\n                        h\\n                        \\n                          )\\n                          \\n                            2\\n                          \\n                        \\n                      \\n                      \\u00af\\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      \\n                        \\n                          \\n                            x\\n                            y\\n                          \\n                          \\u00af\\n                        \\n                      \\n                      \\u2212\\n                      k\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\u2212\\n                      h\\n                      \\n                        \\n                          \\n                            y\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      +\\n                      h\\n                      k\\n                    \\n                    \\n                      \\n                        \\n                          \\n                            x\\n                            \\n                              2\\n                            \\n                          \\n                          \\u00af\\n                        \\n                      \\n                      \\u2212\\n                      2\\n                      h\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      +\\n                      \\n                        h\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      \\n                        \\n                          \\n                            x\\n                            y\\n                          \\n                          \\u00af\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\n                        \\n                          \\n                            y\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      +\\n                      (\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\u2212\\n                      h\\n                      )\\n                      (\\n                      \\n                        \\n                          \\n                            y\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\u2212\\n                      k\\n                      )\\n                    \\n                    \\n                      \\n                        \\n                          \\n                            x\\n                            \\n                              2\\n                            \\n                          \\n                          \\u00af\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            \\n                              x\\n                              \\u00af\\n                            \\n                          \\n                        \\n                        \\n                          2\\n                        \\n                      \\n                      +\\n                      (\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\u2212\\n                      h\\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      Cov\\n                      \\u2061\\n                      (\\n                      x\\n                      ,\\n                      y\\n                      )\\n                      +\\n                      (\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\u2212\\n                      h\\n                      )\\n                      (\\n                      \\n                        \\n                          \\n                            y\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\u2212\\n                      k\\n                      )\\n                    \\n                    \\n                      Var\\n                      \\u2061\\n                      (\\n                      x\\n                      )\\n                      +\\n                      (\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\u2212\\n                      h\\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n                ,\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}{\\\\widehat {\\\\beta }}&={\\\\frac {\\\\sum _{i=1}^{n}(x_{i}-h)(y_{i}-k)}{\\\\sum _{i=1}^{n}(x_{i}-h)^{2}}}={\\\\frac {\\\\overline {(x-h)(y-k)}}{\\\\overline {(x-h)^{2}}}}\\\\\\\\[6pt]&={\\\\frac {{\\\\overline {xy}}-k{\\\\bar {x}}-h{\\\\bar {y}}+hk}{{\\\\overline {x^{2}}}-2h{\\\\bar {x}}+h^{2}}}\\\\\\\\[6pt]&={\\\\frac {{\\\\overline {xy}}-{\\\\bar {x}}{\\\\bar {y}}+({\\\\bar {x}}-h)({\\\\bar {y}}-k)}{{\\\\overline {x^{2}}}-{\\\\bar {x}}^{2}+({\\\\bar {x}}-h)^{2}}}\\\\\\\\[6pt]&={\\\\frac {\\\\operatorname {Cov} (x,y)+({\\\\bar {x}}-h)({\\\\bar {y}}-k)}{\\\\operatorname {Var} (x)+({\\\\bar {x}}-h)^{2}}},\\\\end{aligned}}}\\n  where Cov and Var refer to the covariance and variance of the sample data (uncorrected for bias).\\nThe last form above demonstrates how moving the line away from the center of mass of the data points affects the slope.\\n\\n\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"human_summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17,\n        \"samples\": [\n          \"The Web Audio API specification developed by W3C describes a high-level JavaScript API for processing and synthesizing audio in web applications. The primary paradigm is of an audio routing graph, where a number of AudioNode objects are connected together to define the overall audio rendering.\\n\\nThe Web Audio API allows developers to perform tasks such as:\\n* Process and synthesize audio in web applications\\n* Load, play, loop, and process sound samples\\n* Apply sound effects such as reverberation, delay, graphic equalizer, compressor, distortion, etc.\\n* Create real-time audio visualizations like dancing frequency graphs, animated waveforms that dance with the music, or generate music programmatically.\\n\\nThese capabilities make WebAudioAPI suitable for games and music applications. WebAudio API is available on web browsers such as Google Chrome, Firefox, Opera, Safari, Microsoft Edge and Opera GX and also available on Google Chrome, Firefox and Safari on mobile devices.\\n\\nAnother particular suitable multimedia feature is Track API. Using Track API, you will be able to  synchronize video with other elements in the document, such as display a Google Map, an HTML description or a Wikipedia page beside the video, while it\\u2019s playing.\\n\",\n          \"\\ufeffSupervised learning, formally, is a machine learning approach where input feature objects are paired with labeled desired output in order to train a model. When this training data is processed, the model builds a function that maps new data to expected output values based on parameters that were learned through training. Essentially, we are giving the algorithm a dataset where each data point has \\u201cright answers\\u201d given, and the task of the algorithm is to produce more of these right answers.\\n\\nOne popular use case for this algorithm is for a regression problem, where you are trying to predict a continuous valued output based on the given inputs. For example, suppose your friend is looking to sell their house and looking to see how much their house is worth. What they have is a dataset consisting of labeled pairs of the sizes of houses and its associated price. A supervised learning algorithm will learn from these pairs and will try to learn a mapping from the sizes of a house to the price of a house from the dataset. The model can then be used to predict the value of your friend\\u2019s house based on the size of their house.\\n\\nAnother use case of supervised learning is for classification problems, where you are trying to predict a discrete value output. For example, they give a breast cancer diagnosis problem where you are trying to classify whether a tumor is malignant or benign. Given the size of the tumor and the age of the patient labeled with true answers, the model will learn a mapping that separates between classes and determine whether or not a new patient has a malignant or benign tumor.\\n\\nMachine learning algorithms can utilize multiple features or attributes to make predictions, and for some problems, you want an \\u201cinfinite\\u201d number of features or cues in order to make more accurate predictions. One of the ways you can achieve this is through the support vector machine mechanism, however, it is important to keep in mind that this might not be the case for other problems. You should have a number of features that are descriptive of the object. The number of features should not be too large, because of the curse of dimensionality; but should contain enough information to accurately predict the output.\\n\",\n          \"\\ufeffLinear regression plays an important role in the subfield of artificial intelligence known as machine learning. The linear regression algorithm is one of the fundamental supervised machine-learning algorithms due to its relative simplicity and well-known properties. Linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. \\n\\nThis video introduces linear regression as the first learning algorithm, showcasing its process in supervised learning. In supervised learning, we are training a model using a dataset such that each example of the data is given the \\u201cright answer\\u201d to find the function that we want to discover. \\n\\nFor example, suppose your friend is looking to sell their house and looking to see how much their house is worth. What they have is a dataset consisting of labeled pairs of the sizes of houses and its associated price. A supervised learning algorithm will learn from these pairs and will try to learn a mapping from the sizes of a house to the price of a house from the dataset. The model can then be used to predict the value of your friend\\u2019s house based on the size of their house.\\n\\nThis problem is also known as a regression problem, as the term regression refers to the fact that we are trying to predict real-valued output.\\n\\nIn supervised learning, we have a training set (in this example, it is the set of the prices of the house) and our job is to learn from this data how to predict the outputs (the house prices). For this, we are going to introduce some notation:\\n* \\u201cm\\u201d will refer to the number of training examples\\n* \\u201cx\\u201d will refer to an input variables/features\\n* \\u201cy\\u201d will refer to the output/target variables\\n* (x, y) will refer to a single training example (input/output pair)\\n* (x(i), y(i)) will refer to the i-th specific training example\\n\\nIn supervised learning, we start off with a training set and we feed that to our learning algorithm. It is the job of the learning algorithm to then output a function which by convection is usually denoted \\u201ch\\u201d where \\u201ch\\u201d stands for hypothesis. The hypothesis is a function, in the housing example, is to take the size of the house(x) and try to output the estimated value (y). In summary, \\u201ch\\u201d is a function that maps x\\u2019s to y\\u2019s. \\n\\nIn linear regression, the hypothesis function is assumed to be a linear relationship between X and Y. We can represent this relationship as function h(x) = \\u03980 + \\u03981 * x, where \\u03980 and \\u03981 are parameters learned by the learning algorithm. Linear regression with one variable is also known as simple linear regression or univariate linear regression.\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"data"},"text/html":["\n","  <div id=\"df-3d4972ae-0cf0-4c66-83eb-b18939850b3e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>transcript</th>\n","      <th>reading_material</th>\n","      <th>human_summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Welcome to the WebAudio API lesson! I personna...</td>\n","      <td>HTML5 Audio is a subject of the HTML5 specific...</td>\n","      <td>The Web Audio API specification developed by W...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>in this video I'm going to define whether it's...</td>\n","      <td>Supervised learning (SL) is a paradigm in mach...</td>\n","      <td>Supervised learning, formally, is a machine l...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Our story begins 20 years ago. Boris Yeltsin w...</td>\n","      <td>Linux ( LIN-uuks) is a family of open-source U...</td>\n","      <td>In 1991, Linus Torvalds created Linux, an open...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>in this video we'll talk about the second majo...</td>\n","      <td>Unsupervised learning is a method in machine l...</td>\n","      <td>Unsupervised learning is a method in machine ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Hi Im Carrie Anne, this is Crash Course Compu...</td>\n","      <td>A computer number format is the internal repre...</td>\n","      <td>Computers are complex systems which require me...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>our first learning algorithm will be linear re...</td>\n","      <td>In statistics, simple linear regression (SLR) ...</td>\n","      <td>Linear regression plays an important role in ...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>what this machine learning in this video we wi...</td>\n","      <td>Machine learning (ML) is a field of study in a...</td>\n","      <td>Machine learning is a field of study in artif...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Hi, I'm Carrie Anne and welcome to Crash Cours...</td>\n","      <td>A programming language is a system of notation...</td>\n","      <td>Computers today use complex circuits in order...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Hi, Im Carrie Anne and this is Crash Course C...</td>\n","      <td>In computer programming, machine code is compu...</td>\n","      <td>CPUs combine an ALU, control unit, memory, an...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Hello world, Im Carrie Anne, and welcome to C...</td>\n","      <td>The history of computing hardware covers the d...</td>\n","      <td>Computers have become essential for various a...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Hi, Im Carrie Anne and welcome to CrashCourse...</td>\n","      <td>In electronics, computer science and computer ...</td>\n","      <td>The instruction set contains expensive operat...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Hi, Im Carrie Anne and welcome to Crash Cours...</td>\n","      <td>In mathematics and mathematical logic, Boolean...</td>\n","      <td>Computers use binary (1s and 0s) to represent...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Hi, Im Carrie Ann and this is Crash Course Co...</td>\n","      <td>A computer is a machine that can be programmed...</td>\n","      <td>An ALU (Arithmetic and Logic Unit) is the \"ma...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Expression is a combination of objects and ope...</td>\n","      <td>Python is a high-level, general-purpose progra...</td>\n","      <td>Expressions are a combination of objects and ...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>&gt;&gt; Classical machine learning can take you so ...</td>\n","      <td>Deep learning is the subset of machine learnin...</td>\n","      <td>Classical machine learning involves defining f...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Hi, Im Carrie Anne, this is Crash Course Comp...</td>\n","      <td>A central processing unit (CPU)also called a ...</td>\n","      <td>A central processing unit (CPU)also called a ...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>You use Linux every day, whether you know it o...</td>\n","      <td>Linux ( LIN-uuks) is a family of open-source U...</td>\n","      <td>Today, Linux is prevalent, with over 850,000 ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d4972ae-0cf0-4c66-83eb-b18939850b3e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-3d4972ae-0cf0-4c66-83eb-b18939850b3e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-3d4972ae-0cf0-4c66-83eb-b18939850b3e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-c9a60cb6-9291-4850-b547-38f11c75fc96\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c9a60cb6-9291-4850-b547-38f11c75fc96')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-c9a60cb6-9291-4850-b547-38f11c75fc96 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["                                           transcript  \\\n","0   Welcome to the WebAudio API lesson! I personna...   \n","1   in this video I'm going to define whether it's...   \n","2   Our story begins 20 years ago. Boris Yeltsin w...   \n","3   in this video we'll talk about the second majo...   \n","4   Hi Im Carrie Anne, this is Crash Course Compu...   \n","5   our first learning algorithm will be linear re...   \n","6   what this machine learning in this video we wi...   \n","7   Hi, I'm Carrie Anne and welcome to Crash Cours...   \n","8   Hi, Im Carrie Anne and this is Crash Course C...   \n","9   Hello world, Im Carrie Anne, and welcome to C...   \n","10  Hi, Im Carrie Anne and welcome to CrashCourse...   \n","11  Hi, Im Carrie Anne and welcome to Crash Cours...   \n","12  Hi, Im Carrie Ann and this is Crash Course Co...   \n","13  Expression is a combination of objects and ope...   \n","14  >> Classical machine learning can take you so ...   \n","15  Hi, Im Carrie Anne, this is Crash Course Comp...   \n","16  You use Linux every day, whether you know it o...   \n","\n","                                     reading_material  \\\n","0   HTML5 Audio is a subject of the HTML5 specific...   \n","1   Supervised learning (SL) is a paradigm in mach...   \n","2   Linux ( LIN-uuks) is a family of open-source U...   \n","3   Unsupervised learning is a method in machine l...   \n","4   A computer number format is the internal repre...   \n","5   In statistics, simple linear regression (SLR) ...   \n","6   Machine learning (ML) is a field of study in a...   \n","7   A programming language is a system of notation...   \n","8   In computer programming, machine code is compu...   \n","9   The history of computing hardware covers the d...   \n","10  In electronics, computer science and computer ...   \n","11  In mathematics and mathematical logic, Boolean...   \n","12  A computer is a machine that can be programmed...   \n","13  Python is a high-level, general-purpose progra...   \n","14  Deep learning is the subset of machine learnin...   \n","15  A central processing unit (CPU)also called a ...   \n","16  Linux ( LIN-uuks) is a family of open-source U...   \n","\n","                                        human_summary  \n","0   The Web Audio API specification developed by W...  \n","1   Supervised learning, formally, is a machine l...  \n","2   In 1991, Linus Torvalds created Linux, an open...  \n","3   Unsupervised learning is a method in machine ...  \n","4   Computers are complex systems which require me...  \n","5   Linear regression plays an important role in ...  \n","6   Machine learning is a field of study in artif...  \n","7   Computers today use complex circuits in order...  \n","8   CPUs combine an ALU, control unit, memory, an...  \n","9   Computers have become essential for various a...  \n","10  The instruction set contains expensive operat...  \n","11  Computers use binary (1s and 0s) to represent...  \n","12  An ALU (Arithmetic and Logic Unit) is the \"ma...  \n","13  Expressions are a combination of objects and ...  \n","14  Classical machine learning involves defining f...  \n","15  A central processing unit (CPU)also called a ...  \n","16  Today, Linux is prevalent, with over 850,000 ...  "]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["data"]},{"cell_type":"markdown","metadata":{"id":"lV1eHh4RRiwe"},"source":["## Zero shot learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s77Va0ReRigl"},"outputs":[],"source":["def get_prompt(row):\n","    return [\n","        {\"role\": \"system\", \"content\": \"Zero shot model.\"},\n","        {\n","            \"role\": \"user\",\n","            \"content\": f\"\"\"\n","                          Please write a 500 word summary on the following Transcript based on the Reading Material.\n","                          Pick relevant information from the Reading Material and add it to the summary of the Transcript.\n","                          The Summary should be only of the Transcript with a few information from the Reading Material.\n","                          If you are not sure of the summary, say 'I don't know'.\n","\n","                          Transcript: {row.transcript}\\n\\n\n","                          Reading Material: {row.reading_material}\\n\\n\n","                          Answer:\\n\"\"\",\n","        },\n","    ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z1J2eWBC6v10"},"outputs":[],"source":["# def get_prompt(row):\n","#     return [\n","#         {\"role\": \"system\", \"content\": \"Zero shot model.\"},\n","#         {\n","#             \"role\": \"user\",\n","#             \"content\": f\"\"\"can you summarise a Transcript for me? The summary has to be 500 words long\"\"\",\n","#         },\n","#         {\n","#             \"role\": \"user\",\n","#             \"content\": f\"\"\"Transcript: {row.transcript}\"\"\",\n","#         },\n","#         {\n","#             \"role\": \"user\",\n","#             \"content\": f\"\"\"Can you enhance the previous generated summary with the help of Reading Material? Pick some contents from the\n","#                            Reading Material which are relevant to the concepts discussed in the Transcript.\n","#                            Let the enhanced summary be 600 words\"\"\",\n","#         },\n","#         {\n","#             \"role\": \"user\",\n","#             \"content\": f\"\"\"Reading Material: {row.transcript}\"\"\",\n","#         },\n","#     ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_v72NKPlUB-f"},"outputs":[],"source":["def api_call(messages, model):\n","    return client.chat.completions.create(\n","        model=model,\n","        messages=messages\n","    )\n","\n","\n","# Main function to answer question\n","def generate_summary(row, prompt_func=get_prompt, model=\"gpt-3.5-turbo\"):\n","    messages = prompt_func(row)\n","    response = api_call(messages, model)\n","    return response.choices[0].message.content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fssrd8jyUK3Q"},"outputs":[],"source":["zero_shot_model_summ = generate_summary(data.iloc[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vNB0R6H8nrqv"},"outputs":[],"source":["print(zero_shot_model_summ)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0c_zqf7bUd4W"},"outputs":[],"source":["human_summ = data.iloc[0]['human_summary']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Te2GUdWloKpo"},"outputs":[],"source":["print(human_summ)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ToY815L-VJ_-"},"outputs":[],"source":["# scorer = BERTScorer(model_type='bert-base-uncased')\n","P, R, F1 = scorer.score([zero_shot_model_summ], [human_summ])\n","print(f\"BERTScore Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"yUP2dcZhVxZH"},"source":["## Few Shot Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9__GZVLyVxA_"},"outputs":[],"source":["def get_prompt_for_fine_tuning(row):\n","    return [\n","        {\n","            \"role\": \"user\",\n","            \"content\": f\"\"\"Transcript: {row.transcript}\\n\\n\n","                           Reading Material: {row.reading_material}\\n\\n\n","                           Answer: \\n\\n\"\"\",\n","        },\n","        {\"role\": \"assistant\", \"content\": row.human_summary},\n","    ]\n","\n","def get_training_data(row):\n","\n","    prompts = []\n","    instruction = '''\n","                      Please write a 500 word summary on the following Transcript based on the Reading Material.\n","                      Pick relevant information from the Reading Material and add it to the summary of the Transcript.\n","                      The Summary should be only of the Transcript with a few information from the Reading Material.\n","                      If you are not sure of the summary, say 'I don't know'.\n","    '''\n","\n","    prompts += get_prompt_for_fine_tuning(row)\n","\n","    prompts = [{\"role\": \"system\", \"content\": instruction}] + prompts\n","    return prompts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tAXYM7ZDYpcY"},"outputs":[],"source":["data[\"few_shot_prompt\"] = data.apply(get_training_data, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Cj06lO3Zrcf"},"outputs":[],"source":["def dataframe_to_jsonl(df):\n","\n","    def create_jsonl_entry(row):\n","        messages = row[\"few_shot_prompt\"]\n","        return json.dumps({\"messages\": messages})\n","\n","    jsonl_output = df.apply(create_jsonl_entry, axis=1)\n","    return \"\\n\".join(jsonl_output)\n","\n","with open(\"17_train_few_shot.jsonl\", \"w\") as f:\n","    f.write(dataframe_to_jsonl(data))"]},{"cell_type":"markdown","metadata":{"id":"jG93r8GMbaN_"},"source":["### OpenAI Fine Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4yp8bk_7hLpw"},"outputs":[],"source":["class OpenAIFineTuner:\n","    \"\"\"\n","    Class to fine tune OpenAI models\n","    \"\"\"\n","    def __init__(self, training_file_path, model_name, suffix):\n","        self.training_file_path = training_file_path\n","        self.model_name = model_name\n","        self.suffix = suffix\n","        self.file_object = None\n","        self.fine_tuning_job = None\n","        self.model_id = None\n","\n","    def create_openai_file(self):\n","        self.file_object = client.files.create(\n","            file=open(self.training_file_path, \"rb\"),\n","            purpose=\"fine-tune\",\n","        )\n","\n","    def wait_for_file_processing(self, sleep_time=45):\n","        while self.file_object.status != 'processed':\n","            time.sleep(sleep_time)\n","            self.file_object.refresh()\n","            print(\"File Status: \", self.file_object.status)\n","\n","    def create_fine_tuning_job(self):\n","        self.fine_tuning_job = client.fine_tuning.jobs.create(\n","            training_file=self.file_object.id,\n","            model=self.model_name,\n","            suffix=self.suffix,\n","        )\n","\n","    def wait_for_fine_tuning(self, sleep_time=45):\n","        i = 1\n","        while self.fine_tuning_job.status != 'succeeded':\n","            time.sleep(sleep_time)\n","            self.fine_tuning_job = client.fine_tuning.jobs.retrieve(self.fine_tuning_job.id)\n","            print(\"Time: \", i*45, \"s, Job Status: \", self.fine_tuning_job.status)\n","            i += 1\n","\n","    def retrieve_fine_tuned_model(self):\n","        self.model_id = self.fine_tuning_job.fine_tuned_model\n","        return self.model_id\n","\n","    def fine_tune_model(self):\n","        self.create_openai_file()\n","        self.wait_for_file_processing()\n","        self.create_fine_tuning_job()\n","        self.wait_for_fine_tuning()\n","        return self.retrieve_fine_tuned_model()\n","\n","fine_tuner = OpenAIFineTuner(\n","        training_file_path=\"17_train_few_shot.jsonl\",\n","        model_name=\"gpt-3.5-turbo-1106\",\n","        suffix=\"17fewshot20240304\"\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":226},"executionInfo":{"elapsed":499403,"status":"ok","timestamp":1709616387663,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"lVR0f4_yjEXC","outputId":"e684999a-9977-47f6-8770-f327570e62bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time:  45 s, Job Status:  running\n","Time:  90 s, Job Status:  running\n","Time:  135 s, Job Status:  running\n","Time:  180 s, Job Status:  running\n","Time:  225 s, Job Status:  running\n","Time:  270 s, Job Status:  running\n","Time:  315 s, Job Status:  running\n","Time:  360 s, Job Status:  running\n","Time:  405 s, Job Status:  running\n","Time:  450 s, Job Status:  running\n","Time:  495 s, Job Status:  succeeded\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'ft:gpt-3.5-turbo-1106:ucsc:17fewshot20240304:8zHZ2yJ7'"]},"execution_count":108,"metadata":{},"output_type":"execute_result"}],"source":["model_id = fine_tuner.fine_tune_model()\n","model_id"]},{"cell_type":"markdown","metadata":{"id":"vaHoj4Pvmbwr"},"source":["#### Trained model ID:\n","model_id = 'ft:gpt-3.5-turbo-0125:ucsc:26fewshot20240302:8yOzIOdp'\n","model_id = 'ft:gpt-3.5-turbo-0125:ucsc:26fewshot20240302:8zFmJEY4'\n","model_id = 'ft:gpt-3.5-turbo-1106:ucsc:11fewshot20240304:8zG6iQyx'"]},{"cell_type":"markdown","metadata":{"id":"-yI1OpAyovuf"},"source":["## Validating the fine-tuned model"]},{"cell_type":"markdown","metadata":{"id":"UpZWJ1pNIagH"},"source":["### On an existing transcript"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4112,"status":"ok","timestamp":1709650440036,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"xoDgsFBP8Not","outputId":"69bb70de-f2da-41f1-bd06-e7a482164ce2"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["zero_shot_model_summ = generate_summary(data.iloc[3])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11017,"status":"ok","timestamp":1709650508478,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"tFROOYwfnL3Q","outputId":"f68d2833-07ee-4079-a23f-8c3d0e38ad1c"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["few_shot_model_summ_11 = generate_summary(data.iloc[3], model='ft:gpt-3.5-turbo-1106:ucsc:11fewshot20240304:8zG6iQyx')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10505,"status":"ok","timestamp":1709650518971,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"cYj1rGwLGXlB","outputId":"1891fefd-66bd-41a1-8e49-747043ceab51"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["few_shot_model_summ_17 = generate_summary(data.iloc[3], model='ft:gpt-3.5-turbo-1106:ucsc:17fewshot20240304:8zHZ2yJ7')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1709650458452,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"b5fiuxdZ_eb9","outputId":"7a466cd0-5508-42a8-b5e1-0c0aef8ecf5d"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["The transcript discusses the concept of unsupervised learning, contrasting it with supervised learning where algorithms learn patterns from unlabeled data. Unsupervised learning involves finding structure in data without explicit guidance on what the correct answers are. Clustering algorithms are a popular unsupervised learning tool, used in various applications such as organizing news stories on Google News or grouping individuals based on genomics data.\n","\n","The reading material provides additional insights into unsupervised learning methods such as neural networks, including discriminative and generative tasks. Hebbian learning principles, self-organizing maps, and adaptive resonance theory are also mentioned as common unsupervised learning methods. The reading material highlights probabilistic methods like clustering and anomaly detection as approaches used in unsupervised learning. The method of moments is discussed as a statistical approach for estimating unknown parameters in latent variable models.\n","\n","Overall, both the transcript and reading material emphasize the importance of unsupervised learning in discovering patterns and structures within data without the need for labeled information, showcasing its potential applications across various domains.\n"]}],"source":["print(zero_shot_model_summ)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1709650525820,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"hmvXBLwo_j-o","outputId":"b5db7edf-824c-4c6d-f021-a0189eef01be"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Unsupervised learning is a method in machine learning where, in contrast to supervised learning, the machine learning algorithms learn patterns exclusively from unlabeled data. The algorithms learn patterns exclusively from unlabeled data and build a concise representation of its world, generating imaginative content from it. \n","\n","This method is particularly useful for tasks such as clustering, where the algorithm identifies inherent structures or groupings within a dataset without any prior knowledge. In machine learning, it is a way of teaching neural networks to mimic the ways that human brains learn and group data according to \n","commonalities they see between inputs. This is also how we arrive at creativity in machine learning, where the machine learning model is trained to generate data from its concise representation. Applying this to audio input, we can use the clustering model to separate audio sources from a recording, for example, separating voices and music from an audio recording. \n","\n","Some other applications of unsupervised learning include the following:\n","- Organizing large computer clusters\n","- Social network analysis to identify cohesive groups of individuals\n","- Market segmentation by identifying customer segments\n","- Astronomical data analysis to generate theories of how galaxies are formed.\n","\n","Neural networks can perform tasks that are often categorized as discriminative (recognition) or generative (imagination). Often but not always, discriminative tasks use supervised methods and generative tasks use unsupervised; however, the separation is very hazy. During the learning phase, an unsupervised network tries to mimic the data it's given and uses the error in its mimicked output to correct itself Sometimes the error is expressed as a low probability that the erroneous output occurs, or it might be expressed as an unstable high energy state in the network\n","\n","Unsupervised learning, including clustering, is used in various domains, such as genomics, news aggregation (e.g., Google News), organizing computer clusters, social network analysis, market segmentation, and astronomical data analysis. \n","\n","While these algorithms may sound complex, they can be implemented using only a few lines of code, particularly when built in a suitable programming environment. For this reason, this course will be using Octave, a free, open-source programming environment. By using tools like Octave, developers can prototype machine learning algorithms more quickly and efficiently. It is for this reason that developers often first prototype their machine learning algorithms on Octave before migrating to other programming languages.\n"]}],"source":["print(few_shot_model_summ_11)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":221,"status":"ok","timestamp":1709650530327,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"O6ydNtfjGv0g","outputId":"663dd6a6-5fbe-44a1-c336-9e7ebf65f949"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Unsupervised learning is a method in machine learning where, in contrast to supervised learning, algorithms learn patterns exclusively from unlabeled data. The hope is that through mimicry, which is an important mode of learning in people, the machine is forced to build a concise representation of its world and then generate imaginative content from it. Unsupervised learning identifies commonalities in the data and reacts based on the presence or absence of such commonalities respond to feedback rather than explicitly look for patterns like we are clustering data.\n","\n","One type of unsupervised learning algorithm is a clustering algorithm. Unsupervised learning can be used to automatically find structure in the data and automatically cluster individuals into these types that we don't know in advance using clustering algorithms.\n","\n","Some of the common applications of Unsupervised learning are:\n","\tOrganizing large computer clusters\n","\tSocial network analysis\n","\tMarket segmentation\n","\tUnderstanding genomics\n","\tAstronomical data analysis\n","\n","For example, Google News uses unsupervised learning to cluster the daily news stories that it collects in order to group together stories on the same topic. \n","\n","In this demo, he provided another example on how unsupervised learning algorithm separates sound sources from two microphones which helps us automate separating and hearing some of these highly overlapped sound sources. It's really important to realize we're not running any audio processing or really complicated audio separation techniques in this case. It turns out the algorithm that you heard really can be done within the line of code and that's one of the reasons why in this class we're using octave as a programming environment because it makes many machine learning algorithms very, very simple.\n","\n","Neural network tasks are often categorized as discriminative (recognition) or generative (imagination).  Often but not always, discriminative tasks use supervised methods and generative tasks use unsupervised (see Venn diagram); however, the separation is very hazy.  For example, object recognition favors supervised learning but unsupervised learning can also cluster objects into groups.  Furthermore, as progress marches onward some tasks employ both methods, and some tasks swing from one to another.\n"]}],"source":["print(few_shot_model_summ_17)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":219,"status":"ok","timestamp":1709650554952,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"pnkjIFkmrH4a","outputId":"e2e6bdd6-dd79-457b-9d1a-f8ba18d03860"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["human_summ = data.iloc[3]['human_summary']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12377,"status":"ok","timestamp":1709650596170,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"cwaMj-uS7Ggv","outputId":"b3b9cb01-26d5-4705-f034-54df7a1f1bfb"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["BERTScore Precision: 0.8708, Recall: 0.8338, F1: 0.8519\n"]}],"source":["P, R, F1 = scorer.score([zero_shot_model_summ], [human_summ])\n","print(f\"BERTScore Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14362,"status":"ok","timestamp":1709650578425,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"ALoKn-4B78R8","outputId":"6ce545af-1867-4633-e0b3-f09e780c4e4b"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["BERTScore Precision: 0.8914, Recall: 0.8954, F1: 0.8934\n"]}],"source":["P, R, F1 = scorer.score([few_shot_model_summ_11], [human_summ])\n","print(f\"BERTScore Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13304,"status":"ok","timestamp":1709650615267,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"3YNJm3eFHbim","outputId":"54594f66-1443-4654-a2cc-20ce8009f20a"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["BERTScore Precision: 0.8908, Recall: 0.8887, F1: 0.8898\n"]}],"source":["P, R, F1 = scorer.score([few_shot_model_summ_17], [human_summ])\n","print(f\"BERTScore Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"5Go6STVSIO9e"},"source":["### On a new transcript"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":251,"status":"ok","timestamp":1709650632230,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"u989_9Qj8dWE","outputId":"660e2d8b-91c9-4830-8435-5af604f61fad"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["def load_all_to_dataframe(path_to_transcript, path_to_reading_material, path_to_human_summary):\n","\n","    data = pd.DataFrame(columns=['transcript', 'reading_material', 'human_summary'])\n","\n","    transcript_files = os.listdir(path_to_transcript)\n","    transcript_list = []\n","    summary_list = []\n","    reading_list = []\n","\n","    for files in transcript_files:\n","        transcript_files = files\n","        reading_files = files[:-16]+'_rm.txt'\n","        summ_files = files[:-16]+'.txt'\n","\n","        if summ_files in os.listdir(path_to_human_summary):\n","            summ_files = os.path.join(path_to_human_summary, summ_files)\n","\n","            with open(summ_files, 'r') as fp:\n","                summary_list.append(fp.read())\n","        else:\n","            summary_list = ['']\n","\n","        transcript_files = os.path.join(path_to_transcript, transcript_files)\n","        reading_files = os.path.join(path_to_reading_material, reading_files)\n","\n","        with open(transcript_files, 'r') as fp:\n","            transcript_list.append(fp.read())\n","\n","        with open(reading_files, 'r') as fp:\n","            reading_list.append(fp.read())\n","\n","    data['transcript'] = pd.Series(transcript_list)\n","    data['reading_material'] = pd.Series(reading_list)\n","    data['human_summary'] = pd.Series(summary_list)\n","\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":226,"status":"ok","timestamp":1709650638488,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"hd6_4ng59btc","outputId":"62b40517-173f-40bd-e74f-3e69204cae61"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["data_all = load_all_to_dataframe(path_to_transcript, path_to_reading_material, path_to_human_summary)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":582},"executionInfo":{"elapsed":247,"status":"ok","timestamp":1709650641376,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"-ieWqFEt9gND","outputId":"b526be27-d6c5-4b45-9a27-4132c1835053"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"data_all\",\n  \"rows\": 17,\n  \"fields\": [\n    {\n      \"column\": \"transcript\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17,\n        \"samples\": [\n          \"in this video I'm going to define whether it's probably the most common type of machine learning problem which is supervised learning I'll define supervised learning more formally later but it's probably best to explain I'll start with an example of what it is and we'll do the formal definition later let's say you want to predict housing prices a while back a student collected data sets from the city of Portland Oregon and let's say you plot the data set and it looks like this here on the horizontal axis the size of different houses and square feet and on the vertical axis the practice of different houses in thousands of dollars so given this data let's say you have a friend who owns a house that is saying 750 square feet and they're hoping to sell the house and they want to know how much they can get for the house so how can the learning algorithm help you one thing a learning algorithm might be able to do is put a straight line through the date arrow so the fit a straight line to the data and based on that it looks like maybe their holes can be so full maybe about 150 thousand dollars but maybe this isn't the only learning algorithm you can use and it might be a better one for example instead of fitting a straight line to the data we might decide that it's better to fill a quadratic function or a second-order polynomial to this data and if you do that to make a prediction here then it looks like well maybe they can sell the house well closer to $200,000 one of the things we'll talk about later is how to choose and how to decide do you want to fit a straight line to the data or do you want to fit a quadratic function the data and there's no fair picking whichever one gives your friend the better house to sell but each of these would be a fine example of a learning algorithm so this is an example of a supervised learning algorithm and the term supervised learning refers to the fact that we gave the algorithm a data set in which the right answers were given that is we gave it a data set of houses in which for every exam all in this data set we told it what is the right price what was the actual price that that holds so for and the TAS of the algorithm was to just produce more of these right answers such as for this new house you know that your friend may be trying to sell to define a bit more terminology this is also called a regression problem and by regression problem I mean we're trying to predict a continuous value output namely the price so technically against prices can be rounded off to the nearest cent so maybe prices are actually discrete value but usually we think of the price of a house as a roll number was a scalar value as a continuous value number and the term regression refers to the fact that we're trying to predict this sort of a continuous value to attribute here's another supervised learning examples some friends and I were actually working on this earlier let's say you want to look at medical records and try to predictive a breast cancer as malignant or benign if someone discovers a breast tumor a lump in their breast a malignant tumor is a tumor that is harmful and dangerous and a benign tumor is a tumor this is harmless so obviously people care a lot about this let's see collect the data set and suppose you're in your data set you have on your horizontal axis the size of the tumor and on the vertical axis I'm going to plot 1 or 0 yes or no whether or not these are examples of tumors we've seen before are malignant which is 1 or 0 of not malignant or benign so let's say your data set looks like this where we saw a tumor of this size that turned out to be benign one of this size one of this size and so on and sadly we also saw a few malignant tumors so one of that size one of that size one of that size so on so in this example I have 5 examples of benign tumors shown down here and 5 examples of malignant tumors shown with a vertical axis value of 1 and let's name a friend who tragically has a breast tumor and let's say her breast tumor size maybe somewhere around this value the machine learning question is can you estimate what is the probability what's the chance that the tumor is malignant versus benign to introduce a bit more terminology this is an example of a classification problem the term classification refers to the fact that here we're trying to predict a discrete value output zero or one malignant or benign and it turns out that in classification problem sometimes you can have more than two values for the two possible values for the output as a complete example maybe there are three types of breast cancers and so you may try to predict the discrete value output 0 1 2 or 3 where 0 may mean benign benign tumor so no cancer and one may mean a type 1 cancer I give three types of cancer whatever type one means and two may mean the second type of cancer and 3 may mean a third type of cancer but this would also be a classification problem because this other discrete value set of outputs corresponding to no cancer or cancer type 1 or cancer type 2 or type 3 in classification problems there is another way to plot this data let me show you what I mean I'm going to use a slightly different set of symbols to plot this data so if tumor science is going to be the attribute that I'm going to use to predict malignancy or benign Ness I can also draw my data like this I'm going to use different symbols to denote my benign and malignant or my a negative and positive examples so instead of drawing crosses I'm now going to draw holes for the benign tumors like so and I'm going to keep using X's to denote my malignant tumors ok I hope this make it make sense all I did was I took you know these my data set on top and I just mapped it down to this real line like so and started to use different symbol circles and crosses to denote malignant versus benign examples now in this example we use only one feature or one attribute namely the tumor size in order to predict whether tumor is malignant or benign in other machine learning problems we may have more than one feature or more than one attribute here's an example let's say that instead of just knowing the tumor size we know about the age of the patient and the tumor size in that case maybe your data set would look like this where I may have a set of patients with those ages in that tumor size and look like this and different set of patients that look a little different whose tumors turn out to be malignant as denoted by the crosses so let's say you have a friend who tragically has a tumor and maybe their tumor size and age falls around there so given the data set like this what the learning algorithm may do is for the straight line to the data to try to separate out the malignant tumors from the benign ones and so the learning algorithm may decide to for the straight line like that to separate out the two classes of tumors and you know with this hopefully we can decide that your friend's tumor is more likely so if it was over there then hopefully in the learning algorithm will say that your friend's tumor falls on this benign side and is therefore more likely to be benign than malignant in this example we had two features namely the age of the patient and the size of the tumor in other machine learning problems we will often have more features and my friends that work on this problem they actually use other features like these which is clump thickness clump thickness of the breast tumor uniformity of cell size of the tumor uniformity of cell shape of the tumor and so on and other features as well and it turns out one of the interests most interesting learning algorithms that we'll see in this false as a learning algorithm they can deal with not just two or three or five features but an infinite features on this slide I've listed a total of five different features right - on the axes and three more up here but it turns out that for some learning problems what you really want is not to use like three or five features but instead you want to use an infinite number of features an infinite number of attributes so that your learning algorithm has lots of attributes or features or cues with which to make those predictions so how do you deal with an infinite number of features and so how do you even store an infinite number of things on a computer and your computer can run on the memory well it turns out that when we talk about an algorithm called the support vector machine there will be a neat mathematical trick that will allow a computer to deal with an infinite number of features imagine that I didn't just write down you know two features here and three features on the right but imagine that I wrote down an infinitely long list I just kept writing more and more and more feature that's like an infinitely long list of features turns out we'll be able to come up with an algorithm that can deal with that so just to recap in this class we'll talk about supervised learning and the idea is that in supervised learning in every example in our dataset we are told what is the correct answer that we would have quite like the algorithms are predicted on that example such as the price of the house or whether a tumor is malignant or benign we also talked about the regression problem and by regression that means that our goal is to predict a continuous valued output and we talked about the classification problem where the goal is to predict a discrete value output just a quick wrap-up question suppose you're running a company and you want to develop learning algorithms to address each of two problems in the first problem you have a large inventory of identical items so imagine that you have thousands of copies of some identical item to sell and you want to predict how many of these items you sell over the next three months in the second problem problem to you you light you you have lots of users and you want to write software to examine each individual of your customers accounts so each one of your customers account and for each account decide whether or not the account has been hacked or compromised so for each of these problems should they be treated as a classification problem or as a regression problem when the video pauses please use your mouse to select whichever of these four options on the left you think is the correct answer so hopefully you got that this is the answer for problem 1 I would treat this as a regression problem because if I have you know thousands of items well I would probably just treat this as a real value as a continuous value and treat therefore the number of items I sell as a continuous value and for the second problem I would treat that as a classification problem because I might say set the value I want to predict to be 0 to denote the account has not been hacked and set the value 1 to denote an account that has been hacked into so just like your breast cancers right 0 or benign 1 is malignant so I might set this be 0 or 1 depending whether it's been hacked and have an algorithm try to predict each one of these two discrete values and because that's a small number of discrete values I would therefore treat it as a classification problem so that's it for supervised learning and in the next video I will talk about unsupervised learning which is the other major category of learning algorithm\",\n          \"our first learning algorithm will be linear regression in this video you see what the model looks like and more importantly you also see what the overall process of supervised learning you'll flip let's use a motivating example of predicting housing prices we're going to use a data set of housing prices from the city of Portland Oregon and you're going to plot my data set of a number of houses there were different sizes that were so for a range of different prices let's say that given this data set you have a friend that's trying to sell a house and let's see your friend's house is of size 1,250 square feet and you want to tell them how much they might be able to sell the house for well one thing you could do is fit to a model maybe put a straight line to this data then write something like that and based on that maybe you could tell your friend that looks likely to maybe sell the whole square around to $18,000 so this is an example of a supervised learning algorithm and it's supervised learning because we're given the quote right answer for each of our examples namely were told what was the actual house what was the actual price that each of the houses in our data set was so for and moreover does an example of a regression problem where the term regression refers to the fact that we're predicting a real-valued output maybe the price and then just remind you the other type the other most common type of supervised learning problem is called the classification problem where we predict discrete values outputs such as if we are looking at cancer tumors and trying to decide if a tumor is malignant or benign so there's a 0 1 value distrito more formally in supervised learning we have a data set and this data set is called a training set so for a housing price an example we have a training set of different housing prices and our job is to learn from this data how to predict the prices of the houses let's define some notation that we're using throughout this course I'm going to define quite a lot of symbols is okay if you don't remember all the symbols right now but as the course progresses so be useful with a convenient notation so I'm going to use lowercase M throughout this course to denote the number of training examples so in this data set if I have you know let's say 47 rows in this table then I have 47 training examples and M equals 47 let me use lowercase X to denote the input variables often also called the features so though the X's here will be on input features and I'm going to use Y to denote my output variables or the target variable region to predict its Alexis second column here a little bit more notation I'm going to use X comma Y to denote a single training example so a single row in this table corresponds to a single training example and to refer to a specific training example I'm going to use this notation X I comma Y I and going to use this to refer to the training example so this superscript I over here this is not exponentiation right this X I Y I the superscript I in parenthesis that's just an index into my training set and it refers to the I row in this table okay so this is not except our of iy ^ I instead X I Y I just refers to the I fro of this table so for example X 1 you know refers to the input value for the first training example so that's 21 0 4 represents X to the first row X 2 would be equal to 14 16 right the second x and y 1 will be equal to 460 with that's the first the Y value for my first training example that's what that one refers to so as I mentioned occasionally I'll ask you a question to let you check your own understanding and a few seconds in this video a multiple-choice question will pop up in the video when it does please use your mouse to select what you think is the right answer we're defined by the training set is and so here's how a supervised learning works we solve them of a training set like our training set of housing prices and we feed that to our learning algorithm is the job of a learning algorithm to then output a function which by convention is usually denoted lowercase H and H stands for hypothesis and what the job of the hypothesis is is is a function that takes as input the size of a house like maybe the size of a new house that your friend is trying to sell it takes in a value of X and it tries to output the estimated value of y for their corresponding house so H is a function that map's from X's to YS um people often ask me you know why is this function called a hypothesis some of you may know the meaning of the term hypothesis from the dictionary or from signs or whatever it turns out that the machine learning this is a name that was used in the early days of machine learning and this kind of stuff because maybe not a great name for this sort of function for mapping from sizes of houses to the predictions but you know I think the term hypothesis maybe isn't the best possible name for this but is what this is the standard terminology that people using here you know so don't worry do it don't worry too much about why people call it that when designing a learning algorithm the next thing we need to decide is how do we represent this hypothesis H for this in the next few videos I'm going to choose our initial choice for representing the hypothesis will be the following going to represent H as follows and with the right of this subscript theta of x equals theta 0 plus theta 1 of X and as a shorthand sometimes instead of writing you know H subscript theta of X sometimes it's a shorthand I'll just write this is H of X but more often our rector the subscript theta over there and plotting to some pictures all this means is that we are going to you know predict that Y is a linear function of X right so there's a data set and what this function is doing is just predicting that Y is some straight line function of X s of x equals 3 0 plus theta 1 X ok and why a linear function well sometimes we'll want to fit more complicated perhaps nonlinear functions as well but since this linear case is the simple building block we'll start with this example first so fitting linear functions and we'll build on this to eventually have more complex models in more complex learning algorithms let me also give this particular model a name small though is called linear regression or district example is a actually linear regression with one variable will be variable being X some connecting housing prices functions in one variable X and another name for this model is you need the area linear regression and you need area is just you know a fancy way of saying one variable so that's linear regression in the next video we'll start to talk about just how to go about implementing this model\",\n          \"Hi, I\\u2019m Carrie Anne and this is Crash Course Computer Science! Last episode, we combined an ALU, control unit, some memory, and a clock together to make a basic, but functional Central Processing Unit \\u2013 or CPU \\u2013 the beating, ticking heart of a computer. We\\u2019ve done all the hard work of building many of these components from the electronic circuits up, and now it\\u2019s time to give our CPU some actual instructions to process! The thing that makes a CPU powerful is the fact that it is programmable \\u2013 if you write a different sequence of instructions, then the CPU will perform a different task. So the CPU is a piece of hardware which is controlled by easy-to-modify software! INTRO Let\\u2019s quickly revisit the simple program that we stepped through last episode. The computer memory looked like this. Each address contained 8 bits of data. For our hypothetical CPU, the first four bits specified the operation code, or opcode, and the second set of four bits specified an address or registers. In memory address zero we have 0010 1110. Again, those first four bits are our opcode which corresponds to a \\u201cLOAD_A\\u201d instruction. This instruction reads data from a location of memory specified in those last four bits of the instruction and saves it into Register A. In this case, 1110, or 14 in decimal. So let\\u2019s not think of this of memory address 0 as \\u201c0010 1110\\u201d, but rather as the instruction \\u201cLOAD_A 14\\u201d. That\\u2019s much easier to read and understand! And for me to say! And we can do the same thing for the rest of the data in memory. In this case, our program is just four instructions long, and we\\u2019ve put some numbers into memory too, 3 and 14. So now let\\u2019s step through this program: First is LOAD_A 14, which takes the value in address 14, which is the number 3, and stores it into Register A. Then we have a \\u201cLOAD_B 15\\u201d instruction, which takes the value in memory location 15, which is the number 14, and saves it into Register B. Okay. Easy enough. But now we have an \\u201cADD\\u201d instruction. This tells the processor to use the ALU to add two registers together, in this case, B and A are specified. The ordering is important, because the resulting sum is saved into the second register that\\u2019s specified. So in this case, the resulting sum is saved into Register A. And finally, our last instruction is \\u201cSTORE_A 13\\u201d, which instructs the CPU to write whatever value is in Register A into memory location 13. Yesss! Our program adds two numbers together. That\\u2019s about as exciting as it gets when we only have four instructions to play with. So let\\u2019s add some more! Now we\\u2019ve got a subtract function, which like ADD, specifies two registers to operate on. We\\u2019ve also got a fancy new instruction called JUMP. As the name implies, this causes the program to \\u201cjump\\u201d to a new location. This is useful if we want to change the order of instructions, or choose to skip some instructions. For example, a JUMP 0, would cause the program to go back to the beginning. At a low level, this is done by writing the value specified in the last four bits into the instruction address register, overwriting the current value. We\\u2019ve also added a special version of JUMP called JUMP_NEGATIVE. This only jumps the program if the ALU\\u2019s negative flag is set to true. As we talked about in Episode 5, the negative flag is only set when the result of an arithmetic operation is negative. If the result of the arithmetic was zero or positive, the negative flag would not be set. So the JUMP NEGATIVE won\\u2019t jump anywhere, and the CPU will just continue on to the next instruction. And finally, computers need to be told when to stop processing, so we need a HALT instruction. Our previous program really should have looked like this to be correct, otherwise the CPU would have just continued on after the STORE instruction, processing all those 0\\u2019s. But there is no instruction with an opcode of 0, and so the computer would have crashed! It\\u2019s important to point out here that we\\u2019re storing both instructions and data in the same memory. There is no difference fundamentally -- it\\u2019s all just binary numbers. So the HALT instruction is really important because it allows us to separate the two. Okay, so let\\u2019s make our program a bit more interesting, by adding a JUMP. We\\u2019ll also modify our two starting values in memory to 1 and 1. Lets step through this program just as our CPU would. First, LOAD_A 14 loads the value 1 into Register A. Next, LOAD_B 15 loads the value 1 into Register B. As before, we ADD registers B and A together, with the sum going into Register A. 1+1 = 2, so now Register A has the value 2 in it (stored in binary of course) Then the STORE instruction saves that into memory location 13. Now we hit a \\u201cJUMP 2\\u201d instruction. This causes the processor to overwrite the value in the instruction address register, which is currently 4, with the new value, 2. Now, on the processor\\u2019s next fetch cycle, we don\\u2019t fetch HALT, instead we fetch the instruction at memory location 2, which is ADD B A. We\\u2019ve jumped! Register A contains the value 2, and register B contains the value 1. So 1+2 = 3, so now Register A has the value 3. We store that into memory. And we\\u2019ve hit the JUMP again, back to ADD B A. 1+3 = 4. So now register A has the value 4. See what's happening here? Every loop, we\\u2019re adding one. Its counting up! Cooooool. But notice there\\u2019s no way to ever escape. We\\u2019re never.. ever.. going to get to that halt instruction, because we\\u2019re always going to hit that JUMP. This is called an infinite loop \\u2013 a program that runs forever\\u2026 ever\\u2026 ever\\u2026 ever\\u2026 ever To break the loop, we need a conditional jump. A jump that only happens if a certain condition is met. Our JUMP_NEGATIVE is one example of a conditional jump, but computers have other types too - like JUMP IF EQUAL and JUMP IF GREATER. So let\\u2019s make our code a little fancier and step through it. Just like before, the program starts by loading values from memory into registers A and B. In this example, the number 11 gets loaded into Register A, and 5 gets loaded into Register B. Now we subtract register B from register A. That\\u2019s 11 minus 5, which is 6, and so 6 gets saved into Register A. Now we hit our JUMP NEGATIVE. The last ALU result was 6. That\\u2019s a positive number, so the the negative flag is false. That means the processor does not jump. So we continue on to the next instruction... ...which is a JUMP 2. No conditional on this one, so we jump to instruction 2 no matter what. Ok, so we\\u2019re back at our SUBTRACT Register B from Register A. 6 minus 5 equals 1. So 1 gets saved into register A. Next instruction. We\\u2019re back again at our JUMP NEGATIVE. 1 is also a positive number, so the CPU continues on to the JUMP 2, looping back around again to the SUBTRACT instruction. This time is different though. 1 minus 5 is negative 4. And so the ALU sets its negative flag to true for the first time. Now, when we advance to the next instruction, JUMP_NEGATIVE 5, the CPU executes the jump to memory location 5. We\\u2019re out of the infinite loop! Now we have a ADD B to A. Negative 4 plus 5, is positive 1, and we save that into Register A. Next we have a STORE instruction that saves Register A into memory address 13. Lastly, we hit our HALT instruction and the computer rests. So even though this program is only 7 instructions long, the CPU ended up executing 13 instructions, and that's because it looped twice internally. This code calculated the remainder if we divide 5 into 11, which is one. With a few extra lines of code, we could also keep track of how many loops we did, the count of which would be how many times 5 went into 11\\u2026 we did two loops, so that means 5 goes into 11 two times... with a remainder of 1. And of course this code could work for any two numbers, which we can just change in memory to whatever we want: 7 and 81, 18 and 54, it doesn\\u2019t matter -- that\\u2019s the power of software! Software also allowed us to do something our hardware could not. Remember, our ALU didn\\u2019t have the functionality to divide two numbers, instead it\\u2019s the program we made that gave us that functionality. And then other programs can use our divide program to do even fancier things. And you know what that means. New levels of abstraction! So, our hypothetical CPU is very basic \\u2013 all of its instructions are 8 bits long, with the opcode occupying only the first four bits. So even if we used every combination of 4 bits, our CPU would only be able to support a maximum of 16 different instructions. On top of that, several of our instructions used the last 4 bits to specify a memory location. But again, 4 bits can only encode 16 different values, meaning we can address a maximum of 16 memory locations - that\\u2019s not a lot to work with. For example, we couldn\\u2019t even JUMP to location 17, because we literally can\\u2019t fit the number 17 into 4 bits. For this reason, real, modern CPUs use two strategies. The most straightforward approach is just to have bigger instructions, with more bits, like 32 or 64 bits. This is called the instruction length. Unsurprisingly. The second approach is to use variable length instructions. For example, imagine a CPU that uses 8 bit opcodes. When the CPU sees an instruction that needs no extra values, like the HALT instruction, it can just execute it immediately. However, if it sees something like a JUMP instruction, it knows it must also fetch the address to jump to, which is saved immediately behind the JUMP instruction in memory. This is called, logically enough, an Immediate Value. In such processor designs, instructions can be any number of bytes long, which makes the fetch cycle of the CPU a tad more complicated. Now, our example CPU and instruction set is hypothetical, designed to illustrate key working principles. So I want to leave you with a real CPU example. In 1971, Intel released the 4004 processor. It was the first CPU put all into a single chip and paved the path to the intel processors we know and love today. It supported 46 instructions, shown here. Which was enough to build an entire working computer. And it used many of the instructions we\\u2019ve talked about like JUMP ADD SUBTRACT and LOAD. It also uses 8-bit immediate values, like we just talked about, for things like JUMPs, in order to address more memory. And processors have come a long way since 1971. A modern computer processor, like an Intel Core i7, has thousands of different instructions and instruction variants, ranging from one to fifteen bytes long. For example, there\\u2019s over a dozens different opcodes just for variants of ADD! And this huge growth in instruction set size is due in large part to extra bells and whistles that have been added to processor designs overtime, which we\\u2019ll talk about next episode. See you next week!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reading_material\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17,\n        \"samples\": [\n          \"Supervised learning (SL) is a paradigm in machine learning where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled supervisory signal) train a model. The training data is processed, building a function that maps new data on expected output values.  An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \\\"reasonable\\\" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.\\n\\n\\n== Steps to follow ==\\nTo solve a given problem of supervised learning, one has to perform the following steps:\\n\\nDetermine the type of training examples. Before doing anything else, the user should decide what kind of data is to be used as a training set. In the case of handwriting analysis, for example, this might be a single handwritten character, an entire handwritten word, an entire sentence of handwriting or perhaps a full paragraph of handwriting.\\nGather a training set. The training set needs to be representative of the real-world use of the function. Thus, a set of input objects is gathered and corresponding outputs are also gathered, either from human experts or from measurements.\\nDetermine the input feature representation of the learned function. The accuracy of the learned function depends strongly on how the input object is represented. Typically, the input object is transformed into a feature vector, which contains a number of features that are descriptive of the object. The number of features should not be too large, because of the curse of dimensionality; but should contain enough information to accurately predict the output.\\nDetermine the structure of the learned function and corresponding learning algorithm. For example, the engineer may choose to use support-vector machines or decision trees.\\nComplete the design. Run the learning algorithm on the gathered training set. Some supervised learning algorithms require the user to determine certain control parameters. These parameters may be adjusted by optimizing performance on a subset (called a validation set) of the training set, or via cross-validation.\\nEvaluate the accuracy of the learned function. After parameter adjustment and learning, the performance of the resulting function should be measured on a test set that is separate from the training set.\\n\\n\\n== Algorithm choice ==\\nA wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem).\\nThere are four major issues to consider in supervised learning:\\n\\n\\n=== Bias-variance tradeoff ===\\n\\nA first issue is the tradeoff between bias and variance. Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  . A learning algorithm has high variance for a particular input \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm. Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be \\\"flexible\\\" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).\\n\\n\\n=== Function complexity and amount of training data ===\\nThe second issue is of the amount of training data available relative to the complexity of the \\\"true\\\" function (classifier or regression function). If the true function is simple, then an \\\"inflexible\\\" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn with a large amount of training data paired with a \\\"flexible\\\" learning algorithm with low bias and high variance.\\n\\n\\n=== Dimensionality of the input space ===\\nA third issue is the dimensionality of the input space. If the input feature vectors have large dimensions, learning the function can be difficult even if the true function only depends on a small number of those features. This is because the many \\\"extra\\\" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, input data of large dimensions typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, it will likely improve the accuracy of the learned function. In addition, there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.\\n\\n\\n=== Noise in the output values ===\\nA fourth issue is the degree of noise in the desired output values (the supervisory target variables). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to overfitting. You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled \\\"corrupts\\\" your training data - this phenomenon has been called deterministic noise. When either type of noise is present, it is better to go with a higher bias, lower variance estimator.\\nIn practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance.\\n\\n\\n=== Other factors to consider ===\\nOther factors to consider when choosing and applying a learning algorithm include the following:\\n\\nHeterogeneity of the data. If the feature vectors include features of many different kinds (discrete, discrete ordered, counts, continuous values), some algorithms are easier to apply than others. Many algorithms, including support-vector machines, linear regression, logistic regression, neural networks, and nearest neighbor methods, require that the input features be numerical and scaled to similar ranges (e.g., to the [-1,1] interval). Methods that employ a distance function, such as nearest neighbor methods and support-vector machines with Gaussian kernels, are particularly sensitive to this. An advantage of decision trees is that they easily handle heterogeneous data.\\nRedundancy in the data. If the input features contain redundant information (e.g., highly correlated features), some learning algorithms (e.g., linear regression, logistic regression, and distance based methods) will perform poorly because of numerical instabilities. These problems can often be solved by imposing some form of regularization.\\nPresence of interactions and non-linearities. If each of the features makes an independent contribution to the output, then algorithms based on linear functions (e.g., linear regression, logistic regression, support-vector machines, naive Bayes) and distance functions (e.g., nearest neighbor methods, support-vector machines with Gaussian kernels) generally perform well. However, if there are complex interactions among features, then algorithms such as decision trees and neural networks work better, because they are specifically designed to discover these interactions. Linear methods can also be applied, but the engineer must manually specify the interactions when using them.When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see cross validation). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.\\n\\n\\n=== Algorithms ===\\nThe most widely used learning algorithms are: \\n\\nSupport-vector machines\\nLinear regression\\nLogistic regression\\nNaive Bayes\\nLinear discriminant analysis\\nDecision trees\\nK-nearest neighbor algorithm\\nNeural networks (Multilayer perceptron)\\nSimilarity learning\\n\\n\\n== How supervised learning algorithms work ==\\nGiven a set of \\n  \\n    \\n      \\n        N\\n      \\n    \\n    {\\\\displaystyle N}\\n   training examples of the form \\n  \\n    \\n      \\n        {\\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            1\\n          \\n        \\n        )\\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        (\\n        \\n          x\\n          \\n            N\\n          \\n        \\n        ,\\n        \\n        \\n          y\\n          \\n            N\\n          \\n        \\n        )\\n        }\\n      \\n    \\n    {\\\\displaystyle \\\\{(x_{1},y_{1}),...,(x_{N},\\\\;y_{N})\\\\}}\\n   such that \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   is the feature vector of the \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  -th example and \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}}\\n   is its label (i.e., class), a learning algorithm seeks a function \\n  \\n    \\n      \\n        g\\n        :\\n        X\\n        \\u2192\\n        Y\\n      \\n    \\n    {\\\\displaystyle g:X\\\\to Y}\\n  , where \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   is the input space and \\n  \\n    \\n      \\n        Y\\n      \\n    \\n    {\\\\displaystyle Y}\\n   is the output space. The function \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   is an element of some space of possible functions \\n  \\n    \\n      \\n        G\\n      \\n    \\n    {\\\\displaystyle G}\\n  , usually called the hypothesis space. It is sometimes convenient to represent \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   using a scoring function \\n  \\n    \\n      \\n        f\\n        :\\n        X\\n        \\u00d7\\n        Y\\n        \\u2192\\n        \\n          R\\n        \\n      \\n    \\n    {\\\\displaystyle f:X\\\\times Y\\\\to \\\\mathbb {R} }\\n   such that \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   is defined as returning the \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n   value that gives the highest score: \\n  \\n    \\n      \\n        g\\n        (\\n        x\\n        )\\n        =\\n        \\n          \\n            \\n              arg\\n              \\u2061\\n              max\\n            \\n            y\\n          \\n        \\n        \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle g(x)={\\\\underset {y}{\\\\arg \\\\max }}\\\\;f(x,y)}\\n  . Let \\n  \\n    \\n      \\n        F\\n      \\n    \\n    {\\\\displaystyle F}\\n   denote the space of scoring functions.\\nAlthough \\n  \\n    \\n      \\n        G\\n      \\n    \\n    {\\\\displaystyle G}\\n   and \\n  \\n    \\n      \\n        F\\n      \\n    \\n    {\\\\displaystyle F}\\n   can be any space of functions, many learning algorithms are probabilistic models where \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   takes the form of a conditional probability model \\n  \\n    \\n      \\n        g\\n        (\\n        x\\n        )\\n        =\\n        \\n          \\n            \\n              arg\\n              \\u2061\\n              max\\n            \\n            y\\n          \\n        \\n        \\n        P\\n        (\\n        y\\n        \\n          |\\n        \\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle g(x)={\\\\underset {y}{\\\\arg \\\\max }}\\\\;P(y|x)}\\n  , or \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   takes the form of a joint probability model \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        =\\n        P\\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x,y)=P(x,y)}\\n  . For example, naive Bayes and linear discriminant analysis are joint probability models, whereas logistic regression is a conditional probability model.\\nThere are two basic approaches to choosing \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   or \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  : empirical risk minimization and structural risk minimization. Empirical risk minimization seeks the function that best fits the training data. Structural risk minimization includes a penalty function that controls the bias/variance tradeoff.\\nIn both cases, it is assumed that the training set consists of a sample of independent and identically distributed pairs, \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n        \\n          y\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{i},\\\\;y_{i})}\\n  . In order to measure how well a function fits the training data, a loss function \\n  \\n    \\n      \\n        L\\n        :\\n        Y\\n        \\u00d7\\n        Y\\n        \\u2192\\n        \\n          \\n            R\\n          \\n          \\n            \\u2265\\n            0\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L:Y\\\\times Y\\\\to \\\\mathbb {R} ^{\\\\geq 0}}\\n   is defined. For training example \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n        \\n          y\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{i},\\\\;y_{i})}\\n  , the loss of predicting the value \\n  \\n    \\n      \\n        \\n          \\n            \\n              y\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\hat {y}}}\\n   is \\n  \\n    \\n      \\n        L\\n        (\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n          \\n            \\n              y\\n              ^\\n            \\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle L(y_{i},{\\\\hat {y}})}\\n  .\\nThe risk \\n  \\n    \\n      \\n        R\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle R(g)}\\n   of function \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   is defined as the expected loss of \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  . This can be estimated from the training data as\\n\\n  \\n    \\n      \\n        \\n          R\\n          \\n            e\\n            m\\n            p\\n          \\n        \\n        (\\n        g\\n        )\\n        =\\n        \\n          \\n            1\\n            N\\n          \\n        \\n        \\n          \\u2211\\n          \\n            i\\n          \\n        \\n        L\\n        (\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        ,\\n        g\\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle R_{emp}(g)={\\\\frac {1}{N}}\\\\sum _{i}L(y_{i},g(x_{i}))}\\n  .\\n\\n\\n=== Empirical risk minimization ===\\n\\nIn empirical risk minimization, the supervised learning algorithm seeks the function \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   that minimizes \\n  \\n    \\n      \\n        R\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle R(g)}\\n  . Hence, a supervised learning algorithm can be constructed by applying an optimization algorithm to find \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  .\\nWhen \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   is a conditional probability distribution \\n  \\n    \\n      \\n        P\\n        (\\n        y\\n        \\n          |\\n        \\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle P(y|x)}\\n   and the loss function is the negative log likelihood: \\n  \\n    \\n      \\n        L\\n        (\\n        y\\n        ,\\n        \\n          \\n            \\n              y\\n              ^\\n            \\n          \\n        \\n        )\\n        =\\n        \\u2212\\n        log\\n        \\u2061\\n        P\\n        (\\n        y\\n        \\n          |\\n        \\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle L(y,{\\\\hat {y}})=-\\\\log P(y|x)}\\n  , then empirical risk minimization is equivalent to maximum likelihood estimation.\\nWhen \\n  \\n    \\n      \\n        G\\n      \\n    \\n    {\\\\displaystyle G}\\n   contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well. This is called overfitting.\\n\\n\\n=== Structural risk minimization ===\\nStructural risk minimization seeks to prevent overfitting by incorporating a regularization penalty into the optimization. The regularization penalty can be viewed as implementing a form of Occam's razor that prefers simpler functions over more complex ones.\\nA wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   is a linear function of the form\\n\\n  \\n    \\n      \\n        g\\n        (\\n        x\\n        )\\n        =\\n        \\n          \\u2211\\n          \\n            j\\n            =\\n            1\\n          \\n          \\n            d\\n          \\n        \\n        \\n          \\u03b2\\n          \\n            j\\n          \\n        \\n        \\n          x\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle g(x)=\\\\sum _{j=1}^{d}\\\\beta _{j}x_{j}}\\n  .A popular regularization penalty is \\n  \\n    \\n      \\n        \\n          \\u2211\\n          \\n            j\\n          \\n        \\n        \\n          \\u03b2\\n          \\n            j\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{j}\\\\beta _{j}^{2}}\\n  , which is the squared Euclidean norm of the weights, also known as the \\n  \\n    \\n      \\n        \\n          L\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L_{2}}\\n   norm. Other norms include the \\n  \\n    \\n      \\n        \\n          L\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L_{1}}\\n   norm, \\n  \\n    \\n      \\n        \\n          \\u2211\\n          \\n            j\\n          \\n        \\n        \\n          |\\n        \\n        \\n          \\u03b2\\n          \\n            j\\n          \\n        \\n        \\n          |\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{j}|\\\\beta _{j}|}\\n  , and the \\n  \\n    \\n      \\n        \\n          L\\n          \\n            0\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L_{0}}\\n   \\\"norm\\\", which is the number of non-zero \\n  \\n    \\n      \\n        \\n          \\u03b2\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\beta _{j}}\\n  s. The penalty will be denoted by \\n  \\n    \\n      \\n        C\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle C(g)}\\n  .\\nThe supervised learning optimization problem is to find the function \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   that minimizes\\n\\n  \\n    \\n      \\n        J\\n        (\\n        g\\n        )\\n        =\\n        \\n          R\\n          \\n            e\\n            m\\n            p\\n          \\n        \\n        (\\n        g\\n        )\\n        +\\n        \\u03bb\\n        C\\n        (\\n        g\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle J(g)=R_{emp}(g)+\\\\lambda C(g).}\\n  The parameter \\n  \\n    \\n      \\n        \\u03bb\\n      \\n    \\n    {\\\\displaystyle \\\\lambda }\\n   controls the bias-variance tradeoff. When \\n  \\n    \\n      \\n        \\u03bb\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle \\\\lambda =0}\\n  , this gives empirical risk minimization with low bias and high variance. When \\n  \\n    \\n      \\n        \\u03bb\\n      \\n    \\n    {\\\\displaystyle \\\\lambda }\\n   is large, the learning algorithm will have high bias and low variance. The value of \\n  \\n    \\n      \\n        \\u03bb\\n      \\n    \\n    {\\\\displaystyle \\\\lambda }\\n   can be chosen empirically via cross validation.\\nThe complexity penalty has a Bayesian interpretation as the negative log prior probability of \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  , \\n  \\n    \\n      \\n        \\u2212\\n        log\\n        \\u2061\\n        P\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle -\\\\log P(g)}\\n  , in which case \\n  \\n    \\n      \\n        J\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle J(g)}\\n   is the posterior probability of \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  .\\n\\n\\n== Generative training ==\\nThe training methods described above are discriminative training methods, because they seek to find a function \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   that discriminates well between the different output values (see discriminative model). For the special case where \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        =\\n        P\\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x,y)=P(x,y)}\\n   is a joint probability distribution and the loss function is the negative log likelihood \\n  \\n    \\n      \\n        \\u2212\\n        \\n          \\u2211\\n          \\n            i\\n          \\n        \\n        log\\n        \\u2061\\n        P\\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        )\\n        ,\\n      \\n    \\n    {\\\\displaystyle -\\\\sum _{i}\\\\log P(x_{i},y_{i}),}\\n   a risk minimization algorithm is said to perform generative training, because \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   can be regarded as a generative model that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in naive Bayes and linear discriminant analysis.\\n\\n\\n== Generalizations ==\\nThere are several ways in which the standard supervised learning problem can be generalized:\\n\\nSemi-supervised learning or weak supervision: the desired output values are provided only for a subset of the training data. The remaining data is unlabeled or imprecisely labeled.\\nActive learning: Instead of assuming that all of the training examples are given at the start, active learning algorithms interactively collect new examples, typically by making queries to a human user. Often, the queries are based on unlabeled data, which is a scenario that combines semi-supervised learning with active learning.\\nStructured prediction: When the desired output value is a complex object, such as a parse tree or a labeled graph, then standard methods must be extended.\\nLearning to rank: When the input is a set of objects and the desired output is a ranking of those objects, then again the standard methods must be extended.\\n\\n\\n== Approaches and algorithms ==\\nAnalytical learning\\nArtificial neural network\\nBackpropagation\\nBoosting (meta-algorithm)\\nBayesian statistics\\nCase-based reasoning\\nDecision tree learning\\nInductive logic programming\\nGaussian process regression\\nGenetic programming\\nGroup method of data handling\\nKernel estimators\\nLearning automata\\nLearning classifier systems\\nLearning vector quantization\\nMinimum message length (decision trees, decision graphs, etc.)\\nMultilinear subspace learning\\nNaive Bayes classifier\\nMaximum entropy classifier\\nConditional random field\\nNearest neighbor algorithm\\nProbably approximately correct learning (PAC) learning\\nRipple down rules, a knowledge acquisition methodology\\nSymbolic machine learning algorithms\\nSubsymbolic machine learning algorithms\\nSupport vector machines\\nMinimum complexity machines (MCM)\\nRandom forests\\nEnsembles of classifiers\\nOrdinal classification\\nData pre-processing\\nHandling imbalanced datasets\\nStatistical relational learning\\nProaftn, a multicriteria classification algorithm\\n\\n\\n== Applications ==\\nBioinformatics\\nCheminformatics\\nQuantitative structure\\u2013activity relationship\\nDatabase marketing\\nHandwriting recognition\\nInformation retrieval\\nLearning to rank\\nInformation extraction\\nObject recognition in computer vision\\nOptical character recognition\\nSpam detection\\nPattern recognition\\nSpeech recognition\\nSupervised learning is a special case of downward causation in biological systems\\nLandform classification using satellite imagery\\nSpend classification in procurement processes\\n\\n\\n== General issues ==\\nComputational learning theory\\nInductive bias\\nOverfitting (machine learning)\\n(Uncalibrated) class membership probabilities\\nUnsupervised learning\\nVersion spaces\\n\\n\\n\",\n          \"In statistics, simple linear regression (SLR) is a linear regression model with a single explanatory variable. That is, it concerns two-dimensional sample points with one independent variable and one dependent variable (conventionally, the x and y coordinates in a Cartesian coordinate system) and finds a linear function (a non-vertical straight line) that, as accurately as possible, predicts the dependent variable values as a function of the independent variable.\\nThe adjective simple refers to the fact that the outcome variable is related to a single predictor.\\nIt is common to make the additional stipulation that the ordinary least squares (OLS) method should be used: the accuracy of each predicted value is measured by its squared residual (vertical distance between the point of the data set and the fitted line), and the goal is to make the sum of these squared deviations as small as possible. \\nIn this case, the slope of the fitted line is equal to the correlation between y and x corrected by the ratio of standard deviations of these variables. The intercept of the fitted line is such that the line passes through the center of mass (x, y) of the data points.\\n\\n\\n== Formulation and computation ==\\nConsider the model function\\n\\n  \\n    \\n      \\n        y\\n        =\\n        \\u03b1\\n        +\\n        \\u03b2\\n        x\\n        ,\\n      \\n    \\n    {\\\\displaystyle y=\\\\alpha +\\\\beta x,}\\n  which describes a line with slope \\u03b2 and y-intercept \\u03b1. In general such a relationship may not hold exactly for the largely unobserved population of values of the independent and dependent variables; we call the unobserved deviations from the above equation the errors.   Suppose we observe n data pairs and call them {(xi, yi), i = 1, ..., n}. We can describe the underlying relationship between yi and xi involving this error term \\u03b5i by\\n\\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n        =\\n        \\u03b1\\n        +\\n        \\u03b2\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        +\\n        \\n          \\u03b5\\n          \\n            i\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle y_{i}=\\\\alpha +\\\\beta x_{i}+\\\\varepsilon _{i}.}\\n  This relationship between the true (but unobserved) underlying parameters \\u03b1 and \\u03b2 and the data points is called a linear regression model.\\nThe goal is to find estimated values \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\alpha }}}\\n   and \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}}\\n   for the parameters \\u03b1 and \\u03b2 which would provide the \\\"best\\\" fit in some sense for the data points. As mentioned in the introduction, in this article the \\\"best\\\" fit will be understood as in the least-squares approach: a line that minimizes the sum of squared residuals (see also Errors and residuals) \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\u03b5\\n                ^\\n              \\n            \\n          \\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\varepsilon }}_{i}}\\n   (differences between actual and predicted values of the dependent variable y), each of which is given by, for any candidate parameter values \\n  \\n    \\n      \\n        \\u03b1\\n      \\n    \\n    {\\\\displaystyle \\\\alpha }\\n   and \\n  \\n    \\n      \\n        \\u03b2\\n      \\n    \\n    {\\\\displaystyle \\\\beta }\\n  ,\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\u03b5\\n                ^\\n              \\n            \\n          \\n          \\n            i\\n          \\n        \\n        =\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        \\u2212\\n        \\u03b1\\n        \\u2212\\n        \\u03b2\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\varepsilon }}_{i}=y_{i}-\\\\alpha -\\\\beta x_{i}.}\\n  In other words, \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\alpha }}}\\n   and \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}}\\n   solve the following minimization problem:\\n\\n  \\n    \\n      \\n        (\\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n        ,\\n        \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n        )\\n        =\\n        argmin\\n        \\u2061\\n        \\n          (\\n          \\n            Q\\n            (\\n            \\u03b1\\n            ,\\n            \\u03b2\\n            )\\n          \\n          )\\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle ({\\\\hat {\\\\alpha }},\\\\,{\\\\hat {\\\\beta }})=\\\\operatorname {argmin} \\\\left(Q(\\\\alpha ,\\\\beta )\\\\right),}\\n  where the objective function Q is:\\n\\n  \\n    \\n      \\n        Q\\n        (\\n        \\u03b1\\n        ,\\n        \\u03b2\\n        )\\n        =\\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            n\\n          \\n        \\n        \\n          \\n            \\n              \\n                \\u03b5\\n                ^\\n              \\n            \\n          \\n          \\n            i\\n          \\n          \\n            \\n            2\\n          \\n        \\n        =\\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            n\\n          \\n        \\n        (\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        \\u2212\\n        \\u03b1\\n        \\u2212\\n        \\u03b2\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        \\n          )\\n          \\n            2\\n          \\n        \\n         \\n        .\\n      \\n    \\n    {\\\\displaystyle Q(\\\\alpha ,\\\\beta )=\\\\sum _{i=1}^{n}{\\\\widehat {\\\\varepsilon }}_{i}^{\\\\,2}=\\\\sum _{i=1}^{n}(y_{i}-\\\\alpha -\\\\beta x_{i})^{2}\\\\ .}\\n  By expanding to get a quadratic expression in \\n  \\n    \\n      \\n        \\u03b1\\n      \\n    \\n    {\\\\displaystyle \\\\alpha }\\n   and \\n  \\n    \\n      \\n        \\u03b2\\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\beta ,}\\n   we can derive minimizing values of the function arguments, denoted \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\alpha }}}\\n   and \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}}\\n  :\\nHere we have introduced\\n\\n\\n=== Expanded Formulas ===\\nThe above equations are efficient to use if the mean of the x and y variables  (\\n  \\n    \\n      \\n        \\n          \\n            \\n              x\\n              \\u00af\\n            \\n          \\n        \\n        \\n           and \\n        \\n        \\n          \\n            \\n              y\\n              \\u00af\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\bar {x}}{\\\\text{ and }}{\\\\bar {y}}}\\n  ) are known.  If the means are not known at the time of calculation, it may be more efficient to use the expanded version of the \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n        \\n           and \\n        \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\alpha }}{\\\\text{ and }}{\\\\widehat {\\\\beta }}}\\n   equations.  These expanded equations may be derived from the more general polynomial regression equations by defining the regression polynomial to be of order 1, as follows.\\n\\n  \\n    \\n      \\n        \\n          \\n            [\\n            \\n              \\n                \\n                  n\\n                \\n                \\n                  \\n                    \\u2211\\n                    \\n                      i\\n                      =\\n                      1\\n                    \\n                    \\n                      n\\n                    \\n                  \\n                  \\n                    x\\n                    \\n                      i\\n                    \\n                  \\n                \\n              \\n              \\n                \\n                  \\n                    \\u2211\\n                    \\n                      i\\n                      =\\n                      1\\n                    \\n                    \\n                      n\\n                    \\n                  \\n                  \\n                    x\\n                    \\n                      i\\n                    \\n                  \\n                \\n                \\n                  \\n                    \\u2211\\n                    \\n                      i\\n                      =\\n                      1\\n                    \\n                    \\n                      n\\n                    \\n                  \\n                  \\n                    x\\n                    \\n                      i\\n                    \\n                    \\n                      2\\n                    \\n                  \\n                \\n              \\n            \\n            ]\\n          \\n        \\n        \\n          \\n            [\\n            \\n              \\n                \\n                  \\n                    \\n                      \\n                        \\u03b1\\n                        ^\\n                      \\n                    \\n                  \\n                \\n              \\n              \\n                \\n                  \\n                    \\n                      \\n                        \\u03b2\\n                        ^\\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            ]\\n          \\n        \\n        =\\n        \\n          \\n            [\\n            \\n              \\n                \\n                  \\n                    \\u2211\\n                    \\n                      i\\n                      =\\n                      0\\n                    \\n                    \\n                      n\\n                    \\n                  \\n                  \\n                    y\\n                    \\n                      i\\n                    \\n                  \\n                \\n              \\n              \\n                \\n                  \\n                    \\u2211\\n                    \\n                      i\\n                      =\\n                      0\\n                    \\n                    \\n                      n\\n                    \\n                  \\n                  \\n                    y\\n                    \\n                      i\\n                    \\n                  \\n                  \\n                    x\\n                    \\n                      i\\n                    \\n                  \\n                \\n              \\n            \\n            ]\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{bmatrix}n&\\\\sum _{i=1}^{n}x_{i}\\\\\\\\\\\\sum _{i=1}^{n}x_{i}&\\\\sum _{i=1}^{n}x_{i}^{2}\\\\end{bmatrix}}{\\\\begin{bmatrix}{\\\\widehat {\\\\alpha }}\\\\\\\\{\\\\widehat {\\\\beta }}\\\\end{bmatrix}}={\\\\begin{bmatrix}\\\\sum _{i=0}^{n}y_{i}\\\\\\\\\\\\sum _{i=0}^{n}y_{i}x_{i}\\\\end{bmatrix}}}\\n  \\nThe above system of linear equations may be solved directly, or stand-alone equations for \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n        \\n           and \\n        \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\alpha }}{\\\\text{ and }}{\\\\widehat {\\\\beta }}}\\n   may be derived by expanding the matrix equations above.  The resultant equations are algebraically equivalent to the ones shown in the prior paragraph, and are shown below without proof.\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n              \\n                \\n                \\n                  \\n                    \\n                      \\u03b1\\n                      ^\\n                    \\n                  \\n                \\n                =\\n                \\n                  \\n                    \\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        y\\n                        \\n                          i\\n                        \\n                      \\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                        \\n                          2\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\n                        y\\n                        \\n                          i\\n                        \\n                      \\n                    \\n                    \\n                      n\\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                        \\n                          2\\n                        \\n                      \\n                      \\u2212\\n                      (\\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n            \\n            \\n              \\n              \\n                \\n                \\n                  \\n                    \\n                      \\u03b2\\n                      ^\\n                    \\n                  \\n                \\n                =\\n                \\n                  \\n                    \\n                      n\\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\n                        y\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        y\\n                        \\n                          i\\n                        \\n                      \\n                    \\n                    \\n                      n\\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                        \\n                          2\\n                        \\n                      \\n                      \\u2212\\n                      (\\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}&\\\\qquad {\\\\widehat {\\\\alpha }}={\\\\frac {\\\\sum _{i=1}^{n}y_{i}\\\\sum _{i=1}^{n}x_{i}^{2}-\\\\sum _{i=1}^{n}x_{i}\\\\sum _{i=1}^{n}x_{i}y_{i}}{n\\\\sum _{i=1}^{n}x_{i}^{2}-(\\\\sum _{i=1}^{n}x_{i})^{2}}}\\\\\\\\[5pt]\\\\\\\\&\\\\qquad {\\\\widehat {\\\\beta }}={\\\\frac {n\\\\sum _{i=1}^{n}x_{i}y_{i}-\\\\sum _{i=1}^{n}x_{i}\\\\sum _{i=1}^{n}y_{i}}{n\\\\sum _{i=1}^{n}x_{i}^{2}-(\\\\sum _{i=1}^{n}x_{i})^{2}}}\\\\\\\\&\\\\qquad \\\\end{aligned}}}\\n  \\n\\n\\n== Interpretation ==\\n\\n\\n=== Relationship with the sample covariance matrix ===\\nThe solution can be reformulated using elements of the covariance matrix:\\n\\nwhere\\n\\nSubstituting the above expressions for \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\alpha }}}\\n   and \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}}\\n   into the original solution yields\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  \\n                    y\\n                    ^\\n                  \\n                \\n              \\n              \\u2212\\n              \\n                \\n                  \\n                    y\\n                    \\u00af\\n                  \\n                \\n              \\n            \\n            \\n              s\\n              \\n                y\\n              \\n            \\n          \\n        \\n        =\\n        \\n          r\\n          \\n            x\\n            y\\n          \\n        \\n        \\n          \\n            \\n              x\\n              \\u2212\\n              \\n                \\n                  \\n                    x\\n                    \\u00af\\n                  \\n                \\n              \\n            \\n            \\n              s\\n              \\n                x\\n              \\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {{\\\\hat {y}}-{\\\\bar {y}}}{s_{y}}}=r_{xy}{\\\\frac {x-{\\\\bar {x}}}{s_{x}}}.}\\n  This shows that rxy is the slope of the regression line of the standardized data points (and that this line passes through the origin). Since \\n  \\n    \\n      \\n        \\u2212\\n        1\\n        \\u2264\\n        \\n          r\\n          \\n            x\\n            y\\n          \\n        \\n        \\u2264\\n        1\\n      \\n    \\n    {\\\\displaystyle -1\\\\leq r_{xy}\\\\leq 1}\\n   then we get that if x is some measurement and y is a followup measurement from the same item, then we expect that y (on average) will be closer to the mean measurement than it was to the original value of x. This phenomenon is known as regressions toward the mean.\\nGeneralizing the \\n  \\n    \\n      \\n        \\n          \\n            \\n              x\\n              \\u00af\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\bar {x}}}\\n   notation, we can write a horizontal bar over an expression to indicate the average value of that expression over the set of samples. For example:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              x\\n              y\\n            \\n            \\u00af\\n          \\n        \\n        =\\n        \\n          \\n            1\\n            n\\n          \\n        \\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            n\\n          \\n        \\n        \\n          x\\n          \\n            i\\n          \\n        \\n        \\n          y\\n          \\n            i\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle {\\\\overline {xy}}={\\\\frac {1}{n}}\\\\sum _{i=1}^{n}x_{i}y_{i}.}\\n  This notation allows us a concise formula for rxy:\\n\\n  \\n    \\n      \\n        \\n          r\\n          \\n            x\\n            y\\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                \\n                  \\n                    x\\n                    y\\n                  \\n                  \\u00af\\n                \\n              \\n              \\u2212\\n              \\n                \\n                  \\n                    x\\n                    \\u00af\\n                  \\n                \\n              \\n              \\n                \\n                  \\n                    y\\n                    \\u00af\\n                  \\n                \\n              \\n            \\n            \\n              \\n                (\\n                \\n                  \\n                    \\n                      \\n                        x\\n                        \\n                          2\\n                        \\n                      \\n                      \\u00af\\n                    \\n                  \\n                  \\u2212\\n                  \\n                    \\n                      \\n                        \\n                          x\\n                          \\u00af\\n                        \\n                      \\n                    \\n                    \\n                      2\\n                    \\n                  \\n                \\n                )\\n              \\n              \\n                (\\n                \\n                  \\n                    \\n                      \\n                        y\\n                        \\n                          2\\n                        \\n                      \\n                      \\u00af\\n                    \\n                  \\n                  \\u2212\\n                  \\n                    \\n                      \\n                        \\n                          y\\n                          \\u00af\\n                        \\n                      \\n                    \\n                    \\n                      2\\n                    \\n                  \\n                \\n                )\\n              \\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle r_{xy}={\\\\frac {{\\\\overline {xy}}-{\\\\bar {x}}{\\\\bar {y}}}{\\\\sqrt {\\\\left({\\\\overline {x^{2}}}-{\\\\bar {x}}^{2}\\\\right)\\\\left({\\\\overline {y^{2}}}-{\\\\bar {y}}^{2}\\\\right)}}}.}\\n  The coefficient of determination (\\\"R squared\\\") is equal to \\n  \\n    \\n      \\n        \\n          r\\n          \\n            x\\n            y\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle r_{xy}^{2}}\\n   when the model is linear with a single independent variable. See sample correlation coefficient for additional details.\\n\\n\\n=== Interpretation about the slope ===\\nBy multiplying all members of the summation in the numerator by : \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  \\n                    \\n                      (\\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      )\\n                    \\n                    \\n                      (\\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      )\\n                    \\n                  \\n                \\n                =\\n                1\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}{\\\\frac {(x_{i}-{\\\\bar {x}})}{(x_{i}-{\\\\bar {x}})}}=1\\\\end{aligned}}}\\n   (thereby not changing it):\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  \\n                    \\n                      \\u03b2\\n                      ^\\n                    \\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      (\\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      )\\n                      (\\n                      \\n                        y\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            y\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      )\\n                    \\n                    \\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      (\\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n                =\\n                \\n                  \\n                    \\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      (\\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                      \\n                        \\n                          \\n                            (\\n                            \\n                              y\\n                              \\n                                i\\n                              \\n                            \\n                            \\u2212\\n                            \\n                              \\n                                \\n                                  y\\n                                  \\u00af\\n                                \\n                              \\n                            \\n                            )\\n                          \\n                          \\n                            (\\n                            \\n                              x\\n                              \\n                                i\\n                              \\n                            \\n                            \\u2212\\n                            \\n                              \\n                                \\n                                  x\\n                                  \\u00af\\n                                \\n                              \\n                            \\n                            )\\n                          \\n                        \\n                      \\n                    \\n                    \\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      (\\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n                =\\n                \\n                  \\u2211\\n                  \\n                    i\\n                    =\\n                    1\\n                  \\n                  \\n                    n\\n                  \\n                \\n                \\n                  \\n                    \\n                      (\\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                    \\n                      \\n                        \\u2211\\n                        \\n                          j\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      (\\n                      \\n                        x\\n                        \\n                          j\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n                \\n                  \\n                    \\n                      (\\n                      \\n                        y\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            y\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      )\\n                    \\n                    \\n                      (\\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      )\\n                    \\n                  \\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}{\\\\widehat {\\\\beta }}&={\\\\frac {\\\\sum _{i=1}^{n}(x_{i}-{\\\\bar {x}})(y_{i}-{\\\\bar {y}})}{\\\\sum _{i=1}^{n}(x_{i}-{\\\\bar {x}})^{2}}}={\\\\frac {\\\\sum _{i=1}^{n}(x_{i}-{\\\\bar {x}})^{2}{\\\\frac {(y_{i}-{\\\\bar {y}})}{(x_{i}-{\\\\bar {x}})}}}{\\\\sum _{i=1}^{n}(x_{i}-{\\\\bar {x}})^{2}}}=\\\\sum _{i=1}^{n}{\\\\frac {(x_{i}-{\\\\bar {x}})^{2}}{\\\\sum _{j=1}^{n}(x_{j}-{\\\\bar {x}})^{2}}}{\\\\frac {(y_{i}-{\\\\bar {y}})}{(x_{i}-{\\\\bar {x}})}}\\\\\\\\[6pt]\\\\end{aligned}}}\\n  We can see that the slope (tangent of angle) of the regression line is the weighted average of \\n  \\n    \\n      \\n        \\n          \\n            \\n              (\\n              \\n                y\\n                \\n                  i\\n                \\n              \\n              \\u2212\\n              \\n                \\n                  \\n                    y\\n                    \\u00af\\n                  \\n                \\n              \\n              )\\n            \\n            \\n              (\\n              \\n                x\\n                \\n                  i\\n                \\n              \\n              \\u2212\\n              \\n                \\n                  \\n                    x\\n                    \\u00af\\n                  \\n                \\n              \\n              )\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {(y_{i}-{\\\\bar {y}})}{(x_{i}-{\\\\bar {x}})}}}\\n   that is the slope (tangent of angle) of the line that connects the i-th point to the average of all points, weighted by \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        \\u2212\\n        \\n          \\n            \\n              x\\n              \\u00af\\n            \\n          \\n        \\n        \\n          )\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle (x_{i}-{\\\\bar {x}})^{2}}\\n   because the further the point is the more \\\"important\\\" it is, since small errors in its position will affect the slope connecting it to the center point more.\\n\\n\\n=== Interpretation about the intercept ===\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  \\n                    \\n                      \\u03b1\\n                      ^\\n                    \\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      y\\n                      \\u00af\\n                    \\n                  \\n                \\n                \\u2212\\n                \\n                  \\n                    \\n                      \\u03b2\\n                      ^\\n                    \\n                  \\n                \\n                \\n                \\n                  \\n                    \\n                      x\\n                      \\u00af\\n                    \\n                  \\n                \\n                ,\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}{\\\\widehat {\\\\alpha }}&={\\\\bar {y}}-{\\\\widehat {\\\\beta }}\\\\,{\\\\bar {x}},\\\\\\\\[5pt]\\\\end{aligned}}}\\n  Given \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n        =\\n        tan\\n        \\u2061\\n        (\\n        \\u03b8\\n        )\\n        =\\n        d\\n        y\\n        \\n          /\\n        \\n        d\\n        x\\n        \\u2192\\n        d\\n        y\\n        =\\n        d\\n        x\\n        \\u00d7\\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}=\\\\tan(\\\\theta )=dy/dx\\\\rightarrow dy=dx\\\\times {\\\\widehat {\\\\beta }}}\\n   with \\n  \\n    \\n      \\n        \\u03b8\\n      \\n    \\n    {\\\\displaystyle \\\\theta }\\n   the angle the line makes with the positive x axis, \\nwe have \\n  \\n    \\n      \\n        \\n          y\\n          \\n            \\n              i\\n              n\\n              t\\n              e\\n              r\\n              s\\n              e\\n              c\\n              t\\n              i\\n              o\\n              n\\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              y\\n              \\u00af\\n            \\n          \\n        \\n        \\u2212\\n        d\\n        x\\n        \\u00d7\\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              y\\n              \\u00af\\n            \\n          \\n        \\n        \\u2212\\n        d\\n        y\\n      \\n    \\n    {\\\\displaystyle y_{\\\\rm {intersection}}={\\\\bar {y}}-dx\\\\times {\\\\widehat {\\\\beta }}={\\\\bar {y}}-dy}\\n  \\n\\n\\n=== Interpretation about the correlation ===\\nIn the above formulation, notice that each \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   is a constant (\\\"known upfront\\\") value, while the \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}}\\n   are random variables that depend on the linear function of \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   and the random term \\n  \\n    \\n      \\n        \\n          \\u03b5\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\varepsilon _{i}}\\n  . This assumption is used when deriving the standard error of the slope and showing that it is unbiased.\\nIn this framing, when \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   is not actually a random variable, what type of parameter does the empirical correlation \\n  \\n    \\n      \\n        \\n          r\\n          \\n            x\\n            y\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle r_{xy}}\\n   estimate? The issue is that for each value i we'll have: \\n  \\n    \\n      \\n        E\\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        )\\n        =\\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle E(x_{i})=x_{i}}\\n   and \\n  \\n    \\n      \\n        V\\n        a\\n        r\\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        )\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle Var(x_{i})=0}\\n  . A possible interpretation of \\n  \\n    \\n      \\n        \\n          r\\n          \\n            x\\n            y\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle r_{xy}}\\n   is to imagine that \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   defines a random variable drawn from the empirical distribution of the x values in our sample. For example, if x had 10 values from the natural numbers: [1,2,3...,10], then we can imagine x to be a Discrete uniform distribution. Under this interpretation all \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   have the same expectation and some positive variance. With this interpretation we can think of \\n  \\n    \\n      \\n        \\n          r\\n          \\n            x\\n            y\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle r_{xy}}\\n   as the estimator of the Pearson's correlation between the random variable y and the random variable x (as we just defined it).\\n\\n\\n== Numerical properties ==\\n\\n\\n== Statistical properties ==\\nDescription of the statistical properties of estimators from the simple linear regression estimates requires the use of a statistical model. The following is based on assuming the validity of a model under which the estimates are optimal. It is also possible to evaluate the properties under other assumptions, such as inhomogeneity, but this is discussed elsewhere.\\n\\n\\n=== Unbiasedness ===\\nThe estimators \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\alpha }}}\\n   and \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}}\\n   are unbiased.\\nTo formalize this assertion we must define a framework in which these estimators are random variables. We consider the residuals \\u03b5i as random variables drawn independently from some distribution with mean zero. In other words, for each value of x, the corresponding value of y is generated as a mean response  \\u03b1 + \\u03b2x plus an additional random variable \\u03b5 called the error term, equal to zero on average. Under such interpretation, the least-squares estimators \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\alpha }}}\\n   and \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}}\\n   will themselves be random variables whose means will equal the \\\"true values\\\" \\u03b1 and \\u03b2. This is the definition of an unbiased estimator.\\n\\n\\n=== Confidence intervals ===\\n\\nThe formulas given in the previous section allow one to calculate the point estimates of \\u03b1 and \\u03b2 \\u2014 that is, the coefficients of the regression line for the given set of data. However, those formulas don't tell us how precise the estimates are, i.e., how much the estimators \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\alpha }}}\\n   and \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}}\\n   vary from sample to sample for the specified sample size. Confidence intervals were devised to give a plausible set of values to the estimates one might have if one repeated the experiment a very large number of times.\\nThe standard method of constructing confidence intervals for linear regression coefficients relies on the normality assumption, which is justified if either:\\n\\nthe errors in the regression are normally distributed (the so-called classic regression assumption), or\\nthe number of observations n is sufficiently large, in which case the estimator is approximately normally distributed.The latter case is justified by the central limit theorem.\\n\\n\\n==== Normality assumption ====\\nUnder the first assumption above, that of the normality of the error terms, the estimator of the slope coefficient will itself be normally distributed with mean \\u03b2 and variance \\n  \\n    \\n      \\n        \\n          \\u03c3\\n          \\n            2\\n          \\n        \\n        \\n          /\\n          \\n            \\u2211\\n            (\\n            \\n              x\\n              \\n                i\\n              \\n            \\n            \\u2212\\n            \\n              \\n                \\n                  x\\n                  \\u00af\\n                \\n              \\n            \\n            \\n              )\\n              \\n                2\\n              \\n            \\n          \\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\sigma ^{2}\\\\left/\\\\sum (x_{i}-{\\\\bar {x}})^{2}\\\\right.,}\\n   where \\u03c32 is the variance of the error terms (see Proofs involving ordinary least squares).  At the same time the sum of squared residuals Q is distributed proportionally to \\u03c72 with n \\u2212 2 degrees of freedom, and independently from \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}}\\n  . This allows us to construct a t-value\\n\\n  \\n    \\n      \\n        t\\n        =\\n        \\n          \\n            \\n              \\n                \\n                  \\n                    \\u03b2\\n                    ^\\n                  \\n                \\n              \\n              \\u2212\\n              \\u03b2\\n            \\n            \\n              s\\n              \\n                \\n                  \\n                    \\u03b2\\n                    ^\\n                  \\n                \\n              \\n            \\n          \\n        \\n         \\n        \\u223c\\n         \\n        \\n          t\\n          \\n            n\\n            \\u2212\\n            2\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle t={\\\\frac {{\\\\widehat {\\\\beta }}-\\\\beta }{s_{\\\\widehat {\\\\beta }}}}\\\\ \\\\sim \\\\ t_{n-2},}\\n  where\\n\\n  \\n    \\n      \\n        \\n          s\\n          \\n            \\n              \\n                \\u03b2\\n                ^\\n              \\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                \\n                  \\n                    1\\n                    \\n                      n\\n                      \\u2212\\n                      2\\n                    \\n                  \\n                \\n                \\n                  \\u2211\\n                  \\n                    i\\n                    =\\n                    1\\n                  \\n                  \\n                    n\\n                  \\n                \\n                \\n                  \\n                    \\n                      \\n                        \\u03b5\\n                        ^\\n                      \\n                    \\n                  \\n                  \\n                    i\\n                  \\n                  \\n                    \\n                    2\\n                  \\n                \\n              \\n              \\n                \\n                  \\u2211\\n                  \\n                    i\\n                    =\\n                    1\\n                  \\n                  \\n                    n\\n                  \\n                \\n                (\\n                \\n                  x\\n                  \\n                    i\\n                  \\n                \\n                \\u2212\\n                \\n                  \\n                    \\n                      x\\n                      \\u00af\\n                    \\n                  \\n                \\n                \\n                  )\\n                  \\n                    2\\n                  \\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle s_{\\\\widehat {\\\\beta }}={\\\\sqrt {\\\\frac {{\\\\frac {1}{n-2}}\\\\sum _{i=1}^{n}{\\\\widehat {\\\\varepsilon }}_{i}^{\\\\,2}}{\\\\sum _{i=1}^{n}(x_{i}-{\\\\bar {x}})^{2}}}}}\\n  is the unbiased standard error estimator of the estimator \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}}\\n  .\\nThis t-value has a Student's t-distribution with n \\u2212 2 degrees of freedom. Using it we can construct a confidence interval for \\u03b2:\\n\\n  \\n    \\n      \\n        \\u03b2\\n        \\u2208\\n        \\n          [\\n          \\n            \\n              \\n                \\n                  \\u03b2\\n                  ^\\n                \\n              \\n            \\n            \\u2212\\n            \\n              s\\n              \\n                \\n                  \\n                    \\u03b2\\n                    ^\\n                  \\n                \\n              \\n            \\n            \\n              t\\n              \\n                n\\n                \\u2212\\n                2\\n              \\n              \\n                \\u2217\\n              \\n            \\n            ,\\n             \\n            \\n              \\n                \\n                  \\u03b2\\n                  ^\\n                \\n              \\n            \\n            +\\n            \\n              s\\n              \\n                \\n                  \\n                    \\u03b2\\n                    ^\\n                  \\n                \\n              \\n            \\n            \\n              t\\n              \\n                n\\n                \\u2212\\n                2\\n              \\n              \\n                \\u2217\\n              \\n            \\n          \\n          ]\\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\beta \\\\in \\\\left[{\\\\widehat {\\\\beta }}-s_{\\\\widehat {\\\\beta }}t_{n-2}^{*},\\\\ {\\\\widehat {\\\\beta }}+s_{\\\\widehat {\\\\beta }}t_{n-2}^{*}\\\\right],}\\n  at confidence level (1 \\u2212 \\u03b3), where \\n  \\n    \\n      \\n        \\n          t\\n          \\n            n\\n            \\u2212\\n            2\\n          \\n          \\n            \\u2217\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle t_{n-2}^{*}}\\n   is the \\n  \\n    \\n      \\n        \\n          \\n            (\\n            \\n              1\\n              \\n              \\u2212\\n              \\n              \\n                \\n                  \\u03b3\\n                  2\\n                \\n              \\n            \\n            )\\n          \\n          \\n            -th\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\scriptstyle \\\\left(1\\\\;-\\\\;{\\\\frac {\\\\gamma }{2}}\\\\right){\\\\text{-th}}}\\n   quantile of the tn\\u22122 distribution. For example, if \\u03b3 = 0.05 then the confidence level is 95%.\\nSimilarly, the confidence interval for the intercept coefficient \\u03b1 is given by\\n\\n  \\n    \\n      \\n        \\u03b1\\n        \\u2208\\n        \\n          [\\n          \\n            \\n              \\n                \\n                  \\u03b1\\n                  ^\\n                \\n              \\n            \\n            \\u2212\\n            \\n              s\\n              \\n                \\n                  \\n                    \\u03b1\\n                    ^\\n                  \\n                \\n              \\n            \\n            \\n              t\\n              \\n                n\\n                \\u2212\\n                2\\n              \\n              \\n                \\u2217\\n              \\n            \\n            ,\\n             \\n            \\n              \\n                \\n                  \\u03b1\\n                  ^\\n                \\n              \\n            \\n            +\\n            \\n              s\\n              \\n                \\n                  \\n                    \\u03b1\\n                    ^\\n                  \\n                \\n              \\n            \\n            \\n              t\\n              \\n                n\\n                \\u2212\\n                2\\n              \\n              \\n                \\u2217\\n              \\n            \\n          \\n          ]\\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\alpha \\\\in \\\\left[{\\\\widehat {\\\\alpha }}-s_{\\\\widehat {\\\\alpha }}t_{n-2}^{*},\\\\ {\\\\widehat {\\\\alpha }}+s_{\\\\widehat {\\\\alpha }}t_{n-2}^{*}\\\\right],}\\n  at confidence level (1 \\u2212 \\u03b3), where\\n\\n  \\n    \\n      \\n        \\n          s\\n          \\n            \\n              \\n                \\u03b1\\n                ^\\n              \\n            \\n          \\n        \\n        =\\n        \\n          s\\n          \\n            \\n              \\n                \\u03b2\\n                ^\\n              \\n            \\n          \\n        \\n        \\n          \\n            \\n              \\n                1\\n                n\\n              \\n            \\n            \\n              \\u2211\\n              \\n                i\\n                =\\n                1\\n              \\n              \\n                n\\n              \\n            \\n            \\n              x\\n              \\n                i\\n              \\n              \\n                2\\n              \\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                1\\n                \\n                  n\\n                  (\\n                  n\\n                  \\u2212\\n                  2\\n                  )\\n                \\n              \\n            \\n            \\n              (\\n              \\n                \\n                  \\u2211\\n                  \\n                    i\\n                    =\\n                    1\\n                  \\n                  \\n                    n\\n                  \\n                \\n                \\n                  \\n                    \\n                      \\n                        \\u03b5\\n                        ^\\n                      \\n                    \\n                  \\n                  \\n                    i\\n                  \\n                  \\n                    \\n                    2\\n                  \\n                \\n              \\n              )\\n            \\n            \\n              \\n                \\n                  \\n                    \\u2211\\n                    \\n                      i\\n                      =\\n                      1\\n                    \\n                    \\n                      n\\n                    \\n                  \\n                  \\n                    x\\n                    \\n                      i\\n                    \\n                    \\n                      2\\n                    \\n                  \\n                \\n                \\n                  \\n                    \\u2211\\n                    \\n                      i\\n                      =\\n                      1\\n                    \\n                    \\n                      n\\n                    \\n                  \\n                  (\\n                  \\n                    x\\n                    \\n                      i\\n                    \\n                  \\n                  \\u2212\\n                  \\n                    \\n                      \\n                        x\\n                        \\u00af\\n                      \\n                    \\n                  \\n                  \\n                    )\\n                    \\n                      2\\n                    \\n                  \\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle s_{\\\\widehat {\\\\alpha }}=s_{\\\\widehat {\\\\beta }}{\\\\sqrt {{\\\\frac {1}{n}}\\\\sum _{i=1}^{n}x_{i}^{2}}}={\\\\sqrt {{\\\\frac {1}{n(n-2)}}\\\\left(\\\\sum _{i=1}^{n}{\\\\widehat {\\\\varepsilon }}_{i}^{\\\\,2}\\\\right){\\\\frac {\\\\sum _{i=1}^{n}x_{i}^{2}}{\\\\sum _{i=1}^{n}(x_{i}-{\\\\bar {x}})^{2}}}}}}\\n  The confidence intervals for \\u03b1 and \\u03b2 give us the general idea where these regression coefficients are most likely to be. For example, in the Okun's law regression shown here the point estimates are\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b1\\n              ^\\n            \\n          \\n        \\n        =\\n        0.859\\n        ,\\n        \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n        =\\n        \\u2212\\n        1.817.\\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\alpha }}=0.859,\\\\qquad {\\\\widehat {\\\\beta }}=-1.817.}\\n  The 95% confidence intervals for these estimates are\\n\\n  \\n    \\n      \\n        \\u03b1\\n        \\u2208\\n        \\n          [\\n          \\n            \\n            0.76\\n            ,\\n            0.96\\n          \\n          ]\\n        \\n        ,\\n        \\n        \\u03b2\\n        \\u2208\\n        \\n          [\\n          \\n            \\u2212\\n            2.06\\n            ,\\n            \\u2212\\n            1.58\\n            \\n          \\n          ]\\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\alpha \\\\in \\\\left[\\\\,0.76,0.96\\\\right],\\\\qquad \\\\beta \\\\in \\\\left[-2.06,-1.58\\\\,\\\\right].}\\n  In order to represent this information graphically, in the form of the confidence bands around the regression line, one has to proceed carefully and account for the joint distribution of the estimators. It can be shown that at confidence level (1 \\u2212 \\u03b3) the confidence band has hyperbolic form given by the equation\\n\\n  \\n    \\n      \\n        (\\n        \\u03b1\\n        +\\n        \\u03b2\\n        \\u03be\\n        )\\n        \\u2208\\n        \\n          [\\n          \\n            \\n            \\n              \\n                \\n                  \\u03b1\\n                  ^\\n                \\n              \\n            \\n            +\\n            \\n              \\n                \\n                  \\u03b2\\n                  ^\\n                \\n              \\n            \\n            \\u03be\\n            \\u00b1\\n            \\n              t\\n              \\n                n\\n                \\u2212\\n                2\\n              \\n              \\n                \\u2217\\n              \\n            \\n            \\n              \\n                \\n                  (\\n                  \\n                    \\n                      \\n                        1\\n                        \\n                          n\\n                          \\u2212\\n                          2\\n                        \\n                      \\n                    \\n                    \\u2211\\n                    \\n                      \\n                        \\n                          \\n                            \\u03b5\\n                            ^\\n                          \\n                        \\n                      \\n                      \\n                        i\\n                      \\n                      \\n                        \\n                        2\\n                      \\n                    \\n                  \\n                  )\\n                \\n                \\u22c5\\n                \\n                  (\\n                  \\n                    \\n                      \\n                        1\\n                        n\\n                      \\n                    \\n                    +\\n                    \\n                      \\n                        \\n                          (\\n                          \\u03be\\n                          \\u2212\\n                          \\n                            \\n                              \\n                                x\\n                                \\u00af\\n                              \\n                            \\n                          \\n                          \\n                            )\\n                            \\n                              2\\n                            \\n                          \\n                        \\n                        \\n                          \\u2211\\n                          (\\n                          \\n                            x\\n                            \\n                              i\\n                            \\n                          \\n                          \\u2212\\n                          \\n                            \\n                              \\n                                x\\n                                \\u00af\\n                              \\n                            \\n                          \\n                          \\n                            )\\n                            \\n                              2\\n                            \\n                          \\n                        \\n                      \\n                    \\n                  \\n                  )\\n                \\n              \\n            \\n            \\n          \\n          ]\\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle (\\\\alpha +\\\\beta \\\\xi )\\\\in \\\\left[\\\\,{\\\\widehat {\\\\alpha }}+{\\\\widehat {\\\\beta }}\\\\xi \\\\pm t_{n-2}^{*}{\\\\sqrt {\\\\left({\\\\frac {1}{n-2}}\\\\sum {\\\\widehat {\\\\varepsilon }}_{i}^{\\\\,2}\\\\right)\\\\cdot \\\\left({\\\\frac {1}{n}}+{\\\\frac {(\\\\xi -{\\\\bar {x}})^{2}}{\\\\sum (x_{i}-{\\\\bar {x}})^{2}}}\\\\right)}}\\\\,\\\\right].}\\n  When the model assumed the intercept is fixed and equal to 0 (\\n  \\n    \\n      \\n        \\u03b1\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle \\\\alpha =0}\\n  ), the standard error of the slope turns into:\\n\\n  \\n    \\n      \\n        \\n          s\\n          \\n            \\n              \\n                \\u03b2\\n                ^\\n              \\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                1\\n                \\n                  n\\n                  \\u2212\\n                  1\\n                \\n              \\n            \\n            \\n              \\n                \\n                  \\n                    \\u2211\\n                    \\n                      i\\n                      =\\n                      1\\n                    \\n                    \\n                      n\\n                    \\n                  \\n                  \\n                    \\n                      \\n                        \\n                          \\u03b5\\n                          ^\\n                        \\n                      \\n                    \\n                    \\n                      i\\n                    \\n                    \\n                      \\n                      2\\n                    \\n                  \\n                \\n                \\n                  \\n                    \\u2211\\n                    \\n                      i\\n                      =\\n                      1\\n                    \\n                    \\n                      n\\n                    \\n                  \\n                  \\n                    x\\n                    \\n                      i\\n                    \\n                    \\n                      2\\n                    \\n                  \\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle s_{\\\\widehat {\\\\beta }}={\\\\sqrt {{\\\\frac {1}{n-1}}{\\\\frac {\\\\sum _{i=1}^{n}{\\\\widehat {\\\\varepsilon }}_{i}^{\\\\,2}}{\\\\sum _{i=1}^{n}x_{i}^{2}}}}}}\\n  With: \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\u03b5\\n                ^\\n              \\n            \\n          \\n          \\n            i\\n          \\n        \\n        =\\n        \\n          y\\n          \\n            i\\n          \\n        \\n        \\u2212\\n        \\n          \\n            \\n              \\n                y\\n                ^\\n              \\n            \\n          \\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\hat {\\\\varepsilon }}_{i}=y_{i}-{\\\\hat {y}}_{i}}\\n  \\n\\n\\n==== Asymptotic assumption ====\\nThe alternative second assumption states that when the number of points in the dataset is \\\"large enough\\\", the law of large numbers and the central limit theorem become applicable, and then the distribution of the estimators is approximately normal. Under this assumption all formulas derived in the previous section remain valid, with the only exception that the quantile t*n\\u22122 of Student's t distribution is replaced with the quantile q* of the standard normal distribution. Occasionally the fraction 1/n\\u22122 is replaced with 1/n. When n is large such a change does not alter the results appreciably.\\n\\n\\n== Numerical example ==\\n\\nThis data set gives average masses for women as a function of their height in a sample of American women of age 30\\u201339. Although the OLS article argues that it would be more appropriate to run a quadratic regression for this data, the simple linear regression model is applied here instead.\\n\\nThere are n = 15 points in this data set. Hand calculations would be started by finding the following five sums:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  S\\n                  \\n                    x\\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\u2211\\n                \\n                  x\\n                  \\n                    i\\n                  \\n                \\n                \\n                =\\n                24.76\\n                ,\\n                \\n                \\n                  S\\n                  \\n                    y\\n                  \\n                \\n                =\\n                \\u2211\\n                \\n                  y\\n                  \\n                    i\\n                  \\n                \\n                \\n                =\\n                931.17\\n                ,\\n              \\n            \\n            \\n              \\n                \\n                  S\\n                  \\n                    x\\n                    x\\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\u2211\\n                \\n                  x\\n                  \\n                    i\\n                  \\n                  \\n                    2\\n                  \\n                \\n                =\\n                41.0532\\n                ,\\n                \\n                \\n                \\n                \\n                  S\\n                  \\n                    y\\n                    y\\n                  \\n                \\n                =\\n                \\u2211\\n                \\n                  y\\n                  \\n                    i\\n                  \\n                  \\n                    2\\n                  \\n                \\n                =\\n                58498.5439\\n                ,\\n              \\n            \\n            \\n              \\n                \\n                  S\\n                  \\n                    x\\n                    y\\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\u2211\\n                \\n                  x\\n                  \\n                    i\\n                  \\n                \\n                \\n                  y\\n                  \\n                    i\\n                  \\n                \\n                =\\n                1548.2453\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}S_{x}&=\\\\sum x_{i}\\\\,=24.76,\\\\qquad S_{y}=\\\\sum y_{i}\\\\,=931.17,\\\\\\\\[5pt]S_{xx}&=\\\\sum x_{i}^{2}=41.0532,\\\\;\\\\;\\\\,S_{yy}=\\\\sum y_{i}^{2}=58498.5439,\\\\\\\\[5pt]S_{xy}&=\\\\sum x_{i}y_{i}=1548.2453\\\\end{aligned}}}\\n  These quantities would be used to calculate the estimates of the regression coefficients, and their standard errors.\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  \\n                    \\n                      \\u03b2\\n                      ^\\n                    \\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      n\\n                      \\n                        S\\n                        \\n                          x\\n                          y\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        S\\n                        \\n                          x\\n                        \\n                      \\n                      \\n                        S\\n                        \\n                          y\\n                        \\n                      \\n                    \\n                    \\n                      n\\n                      \\n                        S\\n                        \\n                          x\\n                          x\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        S\\n                        \\n                          x\\n                        \\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n                =\\n                61.272\\n              \\n            \\n            \\n              \\n                \\n                  \\n                    \\n                      \\u03b1\\n                      ^\\n                    \\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    1\\n                    n\\n                  \\n                \\n                \\n                  S\\n                  \\n                    y\\n                  \\n                \\n                \\u2212\\n                \\n                  \\n                    \\n                      \\u03b2\\n                      ^\\n                    \\n                  \\n                \\n                \\n                  \\n                    1\\n                    n\\n                  \\n                \\n                \\n                  S\\n                  \\n                    x\\n                  \\n                \\n                =\\n                \\u2212\\n                39.062\\n              \\n            \\n            \\n              \\n                \\n                  s\\n                  \\n                    \\u03b5\\n                  \\n                  \\n                    2\\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    1\\n                    \\n                      n\\n                      (\\n                      n\\n                      \\u2212\\n                      2\\n                      )\\n                    \\n                  \\n                \\n                \\n                  [\\n                  \\n                    n\\n                    \\n                      S\\n                      \\n                        y\\n                        y\\n                      \\n                    \\n                    \\u2212\\n                    \\n                      S\\n                      \\n                        y\\n                      \\n                      \\n                        2\\n                      \\n                    \\n                    \\u2212\\n                    \\n                      \\n                        \\n                          \\n                            \\u03b2\\n                            ^\\n                          \\n                        \\n                      \\n                      \\n                        2\\n                      \\n                    \\n                    (\\n                    n\\n                    \\n                      S\\n                      \\n                        x\\n                        x\\n                      \\n                    \\n                    \\u2212\\n                    \\n                      S\\n                      \\n                        x\\n                      \\n                      \\n                        2\\n                      \\n                    \\n                    )\\n                  \\n                  ]\\n                \\n                =\\n                0.5762\\n              \\n            \\n            \\n              \\n                \\n                  s\\n                  \\n                    \\n                      \\n                        \\u03b2\\n                        ^\\n                      \\n                    \\n                  \\n                  \\n                    2\\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      n\\n                      \\n                        s\\n                        \\n                          \\u03b5\\n                        \\n                        \\n                          2\\n                        \\n                      \\n                    \\n                    \\n                      n\\n                      \\n                        S\\n                        \\n                          x\\n                          x\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        S\\n                        \\n                          x\\n                        \\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n                =\\n                3.1539\\n              \\n            \\n            \\n              \\n                \\n                  s\\n                  \\n                    \\n                      \\n                        \\u03b1\\n                        ^\\n                      \\n                    \\n                  \\n                  \\n                    2\\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\n                  s\\n                  \\n                    \\n                      \\n                        \\u03b2\\n                        ^\\n                      \\n                    \\n                  \\n                  \\n                    2\\n                  \\n                \\n                \\n                  \\n                    1\\n                    n\\n                  \\n                \\n                \\n                  S\\n                  \\n                    x\\n                    x\\n                  \\n                \\n                =\\n                8.63185\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}{\\\\widehat {\\\\beta }}&={\\\\frac {nS_{xy}-S_{x}S_{y}}{nS_{xx}-S_{x}^{2}}}=61.272\\\\\\\\[8pt]{\\\\widehat {\\\\alpha }}&={\\\\frac {1}{n}}S_{y}-{\\\\widehat {\\\\beta }}{\\\\frac {1}{n}}S_{x}=-39.062\\\\\\\\[8pt]s_{\\\\varepsilon }^{2}&={\\\\frac {1}{n(n-2)}}\\\\left[nS_{yy}-S_{y}^{2}-{\\\\widehat {\\\\beta }}^{2}(nS_{xx}-S_{x}^{2})\\\\right]=0.5762\\\\\\\\[8pt]s_{\\\\widehat {\\\\beta }}^{2}&={\\\\frac {ns_{\\\\varepsilon }^{2}}{nS_{xx}-S_{x}^{2}}}=3.1539\\\\\\\\[8pt]s_{\\\\widehat {\\\\alpha }}^{2}&=s_{\\\\widehat {\\\\beta }}^{2}{\\\\frac {1}{n}}S_{xx}=8.63185\\\\end{aligned}}}\\n  The 0.975 quantile of Student's t-distribution with 13 degrees of freedom is t*13 = 2.1604, and thus the 95% confidence intervals for \\u03b1 and \\u03b2 are\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n              \\n                \\u03b1\\n                \\u2208\\n                [\\n                \\n                \\n                  \\n                    \\n                      \\u03b1\\n                      ^\\n                    \\n                  \\n                \\n                \\u2213\\n                \\n                  t\\n                  \\n                    13\\n                  \\n                  \\n                    \\u2217\\n                  \\n                \\n                \\n                  s\\n                  \\n                    \\u03b1\\n                  \\n                \\n                \\n                ]\\n                =\\n                [\\n                \\n                \\n                  \\u2212\\n                  45.4\\n                \\n                ,\\n                 \\n                \\n                  \\u2212\\n                  32.7\\n                \\n                \\n                ]\\n              \\n            \\n            \\n              \\n              \\n                \\u03b2\\n                \\u2208\\n                [\\n                \\n                \\n                  \\n                    \\n                      \\u03b2\\n                      ^\\n                    \\n                  \\n                \\n                \\u2213\\n                \\n                  t\\n                  \\n                    13\\n                  \\n                  \\n                    \\u2217\\n                  \\n                \\n                \\n                  s\\n                  \\n                    \\u03b2\\n                  \\n                \\n                \\n                ]\\n                =\\n                [\\n                \\n                57.4\\n                ,\\n                 \\n                65.1\\n                \\n                ]\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}&\\\\alpha \\\\in [\\\\,{\\\\widehat {\\\\alpha }}\\\\mp t_{13}^{*}s_{\\\\alpha }\\\\,]=[\\\\,{-45.4},\\\\ {-32.7}\\\\,]\\\\\\\\[5pt]&\\\\beta \\\\in [\\\\,{\\\\widehat {\\\\beta }}\\\\mp t_{13}^{*}s_{\\\\beta }\\\\,]=[\\\\,57.4,\\\\ 65.1\\\\,]\\\\end{aligned}}}\\n  The product-moment correlation coefficient might also be calculated:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              r\\n              ^\\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              n\\n              \\n                S\\n                \\n                  x\\n                  y\\n                \\n              \\n              \\u2212\\n              \\n                S\\n                \\n                  x\\n                \\n              \\n              \\n                S\\n                \\n                  y\\n                \\n              \\n            \\n            \\n              (\\n              n\\n              \\n                S\\n                \\n                  x\\n                  x\\n                \\n              \\n              \\u2212\\n              \\n                S\\n                \\n                  x\\n                \\n                \\n                  2\\n                \\n              \\n              )\\n              (\\n              n\\n              \\n                S\\n                \\n                  y\\n                  y\\n                \\n              \\n              \\u2212\\n              \\n                S\\n                \\n                  y\\n                \\n                \\n                  2\\n                \\n              \\n              )\\n            \\n          \\n        \\n        =\\n        0.9946\\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {r}}={\\\\frac {nS_{xy}-S_{x}S_{y}}{\\\\sqrt {(nS_{xx}-S_{x}^{2})(nS_{yy}-S_{y}^{2})}}}=0.9946}\\n  \\n\\n\\n== Alternatives ==\\nIn SLR, there is an underlying assumption that only the dependent variable contains measurement error; if the explanatory variable is also measured with error, then simple regression is not appropriate for estimating the underlying relationship because it will be biased due to regression dilution. \\nOther estimation methods that can be used in place of ordinary least squares include least absolute deviations (minimizing the sum of absolute values of residuals) and the Theil\\u2013Sen estimator (which chooses a line whose slope is the median of the slopes determined by pairs of sample points). \\nDeming regression (total least squares) also finds a line that fits a set of two-dimensional sample points, but (unlike ordinary least squares, least absolute deviations, and median slope regression) it is not really an instance of simple linear regression, because it does not separate the coordinates into one dependent and one independent variable and could potentially return a vertical line as its fit. can lead to a model that attempts to fit the outliers more than the data.\\n\\n\\n=== Line fitting ===\\n\\n\\n=== Simple linear regression without the intercept term (single regressor) ===\\nSometimes it is appropriate to force the regression line to pass through the origin, because x and y are assumed to be proportional. For the model without the intercept term, y = \\u03b2x, the OLS estimator for \\u03b2 simplifies to\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\u03b2\\n              ^\\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                \\u2211\\n                \\n                  i\\n                  =\\n                  1\\n                \\n                \\n                  n\\n                \\n              \\n              \\n                x\\n                \\n                  i\\n                \\n              \\n              \\n                y\\n                \\n                  i\\n                \\n              \\n            \\n            \\n              \\n                \\u2211\\n                \\n                  i\\n                  =\\n                  1\\n                \\n                \\n                  n\\n                \\n              \\n              \\n                x\\n                \\n                  i\\n                \\n                \\n                  2\\n                \\n              \\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                x\\n                y\\n              \\n              \\u00af\\n            \\n            \\n              \\n                x\\n                \\n                  2\\n                \\n              \\n              \\u00af\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\widehat {\\\\beta }}={\\\\frac {\\\\sum _{i=1}^{n}x_{i}y_{i}}{\\\\sum _{i=1}^{n}x_{i}^{2}}}={\\\\frac {\\\\overline {xy}}{\\\\overline {x^{2}}}}}\\n  Substituting (x \\u2212 h, y \\u2212 k) in place of (x, y) gives the regression through (h, k):\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  \\n                    \\n                      \\u03b2\\n                      ^\\n                    \\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      (\\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      h\\n                      )\\n                      (\\n                      \\n                        y\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      k\\n                      )\\n                    \\n                    \\n                      \\n                        \\u2211\\n                        \\n                          i\\n                          =\\n                          1\\n                        \\n                        \\n                          n\\n                        \\n                      \\n                      (\\n                      \\n                        x\\n                        \\n                          i\\n                        \\n                      \\n                      \\u2212\\n                      h\\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n                =\\n                \\n                  \\n                    \\n                      \\n                        (\\n                        x\\n                        \\u2212\\n                        h\\n                        )\\n                        (\\n                        y\\n                        \\u2212\\n                        k\\n                        )\\n                      \\n                      \\u00af\\n                    \\n                    \\n                      \\n                        (\\n                        x\\n                        \\u2212\\n                        h\\n                        \\n                          )\\n                          \\n                            2\\n                          \\n                        \\n                      \\n                      \\u00af\\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      \\n                        \\n                          \\n                            x\\n                            y\\n                          \\n                          \\u00af\\n                        \\n                      \\n                      \\u2212\\n                      k\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\u2212\\n                      h\\n                      \\n                        \\n                          \\n                            y\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      +\\n                      h\\n                      k\\n                    \\n                    \\n                      \\n                        \\n                          \\n                            x\\n                            \\n                              2\\n                            \\n                          \\n                          \\u00af\\n                        \\n                      \\n                      \\u2212\\n                      2\\n                      h\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      +\\n                      \\n                        h\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      \\n                        \\n                          \\n                            x\\n                            y\\n                          \\n                          \\u00af\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\n                        \\n                          \\n                            y\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      +\\n                      (\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\u2212\\n                      h\\n                      )\\n                      (\\n                      \\n                        \\n                          \\n                            y\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\u2212\\n                      k\\n                      )\\n                    \\n                    \\n                      \\n                        \\n                          \\n                            x\\n                            \\n                              2\\n                            \\n                          \\n                          \\u00af\\n                        \\n                      \\n                      \\u2212\\n                      \\n                        \\n                          \\n                            \\n                              x\\n                              \\u00af\\n                            \\n                          \\n                        \\n                        \\n                          2\\n                        \\n                      \\n                      +\\n                      (\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\u2212\\n                      h\\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      Cov\\n                      \\u2061\\n                      (\\n                      x\\n                      ,\\n                      y\\n                      )\\n                      +\\n                      (\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\u2212\\n                      h\\n                      )\\n                      (\\n                      \\n                        \\n                          \\n                            y\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\u2212\\n                      k\\n                      )\\n                    \\n                    \\n                      Var\\n                      \\u2061\\n                      (\\n                      x\\n                      )\\n                      +\\n                      (\\n                      \\n                        \\n                          \\n                            x\\n                            \\u00af\\n                          \\n                        \\n                      \\n                      \\u2212\\n                      h\\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n                ,\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}{\\\\widehat {\\\\beta }}&={\\\\frac {\\\\sum _{i=1}^{n}(x_{i}-h)(y_{i}-k)}{\\\\sum _{i=1}^{n}(x_{i}-h)^{2}}}={\\\\frac {\\\\overline {(x-h)(y-k)}}{\\\\overline {(x-h)^{2}}}}\\\\\\\\[6pt]&={\\\\frac {{\\\\overline {xy}}-k{\\\\bar {x}}-h{\\\\bar {y}}+hk}{{\\\\overline {x^{2}}}-2h{\\\\bar {x}}+h^{2}}}\\\\\\\\[6pt]&={\\\\frac {{\\\\overline {xy}}-{\\\\bar {x}}{\\\\bar {y}}+({\\\\bar {x}}-h)({\\\\bar {y}}-k)}{{\\\\overline {x^{2}}}-{\\\\bar {x}}^{2}+({\\\\bar {x}}-h)^{2}}}\\\\\\\\[6pt]&={\\\\frac {\\\\operatorname {Cov} (x,y)+({\\\\bar {x}}-h)({\\\\bar {y}}-k)}{\\\\operatorname {Var} (x)+({\\\\bar {x}}-h)^{2}}},\\\\end{aligned}}}\\n  where Cov and Var refer to the covariance and variance of the sample data (uncorrected for bias).\\nThe last form above demonstrates how moving the line away from the center of mass of the data points affects the slope.\\n\\n\\n\",\n          \"In computer programming, machine code is computer code consisting of machine language instructions, which are used to control a computer's central processing unit (CPU). Although decimal computers were once common, the contemporary marketplace is dominated by binary computers; for those computers, machine code is \\\"the binary representation of a computer program which is actually read and interpreted by the computer. A program in machine code consists of a sequence of machine instructions (possibly interspersed with data).\\\"Each instruction causes the CPU to perform a very specific task, such as a load, a store, a jump, or an arithmetic logic unit (ALU) operation on one or more units of data in the CPU's registers or memory.\\nEarly CPUs had specific machine code that might break backward compatibility with each new CPU released. The notion of an instruction set architecture (ISA) defines and specifies the behavior and encoding in memory of the instruction set of the system, without specifying its exact implementation. This acts as an abstraction layer, enabling compatibility within the same family of CPUs, so that machine code written or generated according to the ISA for the family will run on all CPUs in the family, including future CPUs.\\nIn general, each architecture family (e.g. x86, ARM) has its own ISA, and hence its own specific machine code language. There are exceptions, such as the VAX architecture, which included optional support of the PDP-11 instruction set and IA-64, which included optional support of the IA-32 instruction set. Another example is the PowerPC 615, a processor designed to natively process both PowerPC and x86 instructions. \\nMachine code is a strictly numerical language, and is the lowest-level interface to the CPU intended for a programmer. Assembly language provides a direct mapping between the numerical machine code and a human-readable version where numerical opcodes and operands are replaced by readable strings (e.g. 0x90 is the NOP instruction on x86). While it is possible to write programs directly in machine code, managing individual bits and calculating numerical addresses and constants manually is tedious and error-prone. For this reason, programs are very rarely written directly in machine code in modern contexts, but may be done for low-level debugging, program patching (especially when assembler source is not available) and assembly language disassembly.\\nThe majority of practical programs today are written in higher-level languages. Those programs are either translated into machine code by a compiler, or are interpreted by an interpreter, usually after being translated into an intermediate code, such as a bytecode, that is then interpreted.Machine code is by definition the lowest level of programming detail visible to the programmer, but internally many processors use microcode or optimize and transform machine code instructions into sequences of micro-ops. Microcode and micro-ops are not generally considered to be machine code; except on some machines, the user cannot write microcode or micro-ops, and the operation of microcode and the transformation of machine-code instructions into micro-ops happens transparently to the programmer except for performance related side effects.\\n\\n\\n== Instruction set ==\\n\\nEvery processor or processor family has its own instruction set. Instructions are patterns of bits, digits, or characters that correspond to machine commands. Thus, the instruction set is specific to a class of processors using (mostly) the same architecture. Successor or derivative processor designs often include instructions of a predecessor and may add new additional instructions. Occasionally, a successor design will discontinue or alter the meaning of some instruction code (typically because it is needed for new purposes), affecting code compatibility to some extent; even compatible processors may show slightly different behavior for some instructions, but this is rarely a problem. Systems may also differ in other details, such as memory arrangement, operating systems, or peripheral devices. Because a program normally relies on such factors, different systems will typically not run the same machine code, even when the same type of processor is used.\\nA processor's instruction set may have fixed-length or variable-length instructions. How the patterns are organized varies with the particular architecture and type of instruction. Most instructions have one or more opcode fields that specify the basic instruction type (such as arithmetic, logical, jump, etc.), the operation (such as add or compare), and other fields that may give the type of the operand(s), the addressing mode(s), the addressing offset(s) or index, or the operand value itself (such constant operands contained in an instruction are called immediate).Not all machines or individual instructions have explicit operands. On a machine with a single accumulator, the accumulator is implicitly both the left operand and result of most arithmetic instructions. Some other architectures, such as the x86 architecture, have accumulator versions of common instructions, with the accumulator regarded as one of the general registers by longer instructions. A stack machine has most or all of its operands on an implicit stack. Special purpose instructions also often lack explicit operands; for example, CPUID in the x86 architecture writes values into four implicit destination registers. This distinction between explicit and implicit operands is important in code generators, especially in the register allocation and live range tracking parts. A good code optimizer can track implicit as well as explicit operands which may allow more frequent constant propagation, constant folding of registers (a register assigned the result of a constant expression freed up by replacing it by that constant) and other code enhancements.\\n\\n\\n== Programs ==\\nA computer program is a list of instructions that can be executed by a central processing unit (CPU). A program's execution is done in order for the CPU that is executing it to solve a problem and thus accomplish a result. While simple processors are able to execute instructions one after another, superscalar processors are able under certain circumstances (when the pipeline is full) of executing two or more instructions simultaneously. As an example, the original Intel Pentium from 1993 can execute at most two instructions per clock cycle when its pipeline is full.\\nProgram flow may be influenced by special 'jump' instructions that transfer execution to an address (and hence instruction) other than the next numerically sequential address. Whether these conditional jumps occur is dependent upon a condition such as a value being greater than, less than, or equal to another value.\\n\\n\\n== Assembly languages ==\\n\\nA much more human friendly rendition of machine language, called assembly language, uses mnemonic codes to refer to machine code instructions, rather than using the instructions' numeric values directly, and uses symbolic names to refer to storage locations and sometimes registers. For example, on the Zilog Z80 processor, the machine code 00000101, which causes the CPU to decrement the B general-purpose register, would be represented in assembly language as DEC B.\\n\\n\\n== Example ==\\nThe MIPS architecture provides a specific example for a machine code whose instructions are always 32 bits long.:\\u200a299\\u200a The general type of instruction is given by the op (operation) field, the highest 6 bits. J-type (jump) and I-type (immediate) instructions are fully specified by op. R-type (register) instructions include an additional field funct to determine the exact operation. The fields used in these types are:\\n\\n   6      5     5     5     5      6 bits\\n[  op  |  rs |  rt |  rd |shamt| funct]  R-type\\n[  op  |  rs |  rt | address/immediate]  I-type\\n[  op  |        target address        ]  J-type\\n\\nrs, rt, and rd indicate register operands; shamt gives a shift amount; and the address or immediate fields contain an operand directly.:\\u200a299\\u2013301\\u200aFor example, adding the registers 1 and 2 and placing the result in register 6 is encoded::\\u200a554\\u200a\\n[  op  |  rs |  rt |  rd |shamt| funct]\\n    0     1     2     6     0     32     decimal\\n 000000 00001 00010 00110 00000 100000   binary\\n\\nLoad a value into register 8, taken from the memory cell 68 cells after the location listed in register 3::\\u200a552\\u200a\\n[  op  |  rs |  rt | address/immediate]\\n   35     3     8           68           decimal\\n 100011 00011 01000 00000 00001 000100   binary\\n\\nJumping to the address 1024::\\u200a552\\u200a\\n[  op  |        target address        ]\\n    2                 1024               decimal\\n 000010 00000 00000 00000 10000 000000   binary\\n\\n\\n== Overlapping instructions ==\\nOn processor architectures with variable-length instruction sets (such as Intel's x86 processor family) it is, within the limits of the control-flow resynchronizing phenomenon known as the Kruskal count, sometimes possible through opcode-level programming to deliberately arrange the resulting code so that two code paths share a common fragment of opcode sequences. These are called overlapping instructions, overlapping opcodes, overlapping code, overlapped code, instruction scission, or jump into the middle of an instruction, and represent a form of superposition.In the 1970s and 1980s, overlapping instructions were sometimes used to preserve memory space. One example were in the implementation of error tables in Microsoft's Altair BASIC, where interleaved instructions mutually shared their instruction bytes. The technique is rarely used today, but might still be necessary to resort to in areas where extreme optimization for size is necessary on byte-level such as in the implementation of boot loaders which have to fit into boot sectors.It is also sometimes used as a code obfuscation technique as a measure against disassembly and tampering.The principle is also utilized in shared code sequences of fat binaries which must run on multiple instruction-set-incompatible processor platforms.This property is also used to find unintended instructions called gadgets in existing code repositories and is utilized in return-oriented programming as alternative to code injection for exploits such as return-to-libc attacks.\\n\\n\\n== Relationship to microcode ==\\nIn some computers, the machine code of the  architecture is implemented by an even more fundamental underlying layer called microcode, providing a common machine language interface across a line or family of different models of computer with widely different underlying dataflows. This is done to facilitate porting of machine language programs between different models. An example of this use is the IBM System/360 family of computers and their successors. With dataflow path widths of 8 bits to 64 bits and beyond, they nevertheless present a common architecture at the machine language level across the entire line.\\nUsing microcode to implement an emulator enables the computer to present the architecture of an entirely different computer. The System/360 line used this to allow porting programs from earlier IBM machines to the new family of computers, e.g. an IBM 1401/1440/1460 emulator on the IBM S/360 model 40.\\n\\n\\n== Relationship to bytecode ==\\nMachine code is generally different from bytecode (also known as p-code), which is either executed by an interpreter or itself compiled into machine code for faster (direct) execution. An exception is when a processor is designed to use a particular bytecode directly as its machine code, such as is the case with Java processors.\\nMachine code and assembly code are sometimes called native code when referring to platform-dependent parts of language features or libraries.\\n\\n\\n== Storing in memory ==\\nFrom the point of view of the CPU, machine code is stored in RAM, but is typically also kept in a set of caches for performance reasons. There may be different caches for instructions and data, depending on the architecture.\\nThe CPU knows what machine code to execute, based on its internal program counter. The program counter points to a memory address and is changed based on special instructions which may cause programmatic branches. The program counter is typically set to a hard coded value when the CPU is first powered on, and will hence execute whatever machine code happens to be at this address.\\nSimilarly, the program counter can be set to execute whatever machine code is at some arbitrary address, even if this is not valid machine code. This will typically trigger an architecture specific protection fault.\\nThe CPU is oftentimes told, by page permissions in a paging based system, if the current page actually holds machine code by an execute bit \\u2014 pages have multiple such permission bits (readable, writable, etc.) for various housekeeping functionality. E.g. on Unix-like systems memory pages can be toggled to be executable with the mprotect() system call, and on Windows, VirtualProtect() can be used to achieve a similar result. If an attempt is made to execute machine code on a non-executable page, an architecture specific fault will typically occur. Treating data as machine code, or finding new ways to use existing machine code, by various techniques, is the basis of some security vulnerabilities.\\nSimilarly, in a segment based system, segment descriptors can indicate whether a segment can contain executable code and in what rings that code can run.\\nFrom the point of view of a process, the code space is the part of its address space where the code in execution is stored. In multitasking systems this comprises the program's code segment and usually shared libraries. In multi-threading environment, different threads of one process share code space along with data space, which reduces the overhead of context switching considerably as compared to process switching.\\n\\n\\n== Readability by humans ==\\nPamela Samuelson wrote that machine code is so unreadable that the United States Copyright Office cannot identify whether a particular encoded program is an original work of authorship; however, the US Copyright Office does allow for copyright registration of computer programs and a program's machine code can sometimes be decompiled in order to make its functioning more easily understandable to humans. However, the output of a decompiler or disassembler will be missing the comments and symbolic references, so while the output may be easier to read than the object code, it will still be more difficult than the original source code. This problem does not exist for object-code formats like SQUOZE, where the source code is included in the file.\\nCognitive science professor Douglas Hofstadter has compared machine code to genetic code, saying that \\\"Looking at a program written in machine language is vaguely comparable to looking at a DNA molecule atom by atom.\\\"\\n\\n\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"human_summary\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"\\ufeffAn ALU (Arithmetic and Logic Unit) is the \\\"mathematical brain\\\" of a computer that performs calculations and logical operations. The arithmetic unit handles all arithmetic and bitwise operations on integer binary numbers in the computer. \\n\\nThe ALU is made up of smaller circuits that make up the functionalities of the component. The simplest adding circuit is a \\\"half adder\\\" that adds two binary digits and produces a sum and carry bit. A \\\"full adder\\\" is a more complex circuit that adds three binary digits and handles the carry bit. An 8-bit ripple carry adder adds two 8-bit binary numbers by connecting half and full adders in a chain. \\n\\nWhen two 8-bit numbers are added and there is a carry into the 9th bit, it means the sum of the two numbers is too large to fit into 8-bits. This is called an overflow. If we want to avoid overflows, we can extend our circuit with more full adders, allowing us to add 16 or 32 bit numbers, at the cost of speed. Modern computers use a slightly different adding circuit called a \\u2018carry-look-ahead\\u2019 adder which is faster, but ultimately does exactly the same thing-- adds binary numbers\\n\\nThe arithmetic unit of an ALU also performs other math operations, such as subtraction and incrementing, by combining logic gates. Some of the arithmetic operations are as follows:\\n* Add: A and B are summed and the sum appears at Y and carry-out.\\n* Add with carry: A, B and carry-in are summed and the sum appears at Y and carry-out.\\n* Subtract: B is subtracted from A (or vice versa) and the difference appears at Y and carry-out. For this function, carry-out is effectively a \\\"borrow\\\" indicator. \\n* Subtract with borrow: B is subtracted from A (or vice versa) with borrow (carry-in) and the difference appears at Y and carry-out (borrow out).\\n* Two's complement (negate): A (or B) is subtracted from zero and the difference appears at Y.\\n* Increment: A (or B) is increased by one and the resulting value appears at Y.\\n* Decrement: A (or B) is decreased by one and the resulting value appears at Y.\\n* Pass through: all bits of A (or B) appear unmodified at Y. This operation is typically used to determine the parity of the operand or whether it is zero or negative, or to load the operand into a processor register.\\n\\nThe logic unit of an ALU performs logical operations, such as AND, OR, and NOT, and tests numerical conditions. Some operations are as follows: \\n* AND: the bitwise AND of A and B appears at Y.\\n* OR: the bitwise OR of A and B appears at Y.\\n* Exclusive-OR: the bitwise XOR of A and B appears at Y.\\n* Ones' complement: all bits of A (or B) are inverted and appear at Y.\\n\\nALUs use flags to indicate specific statuses, such as zero, negative, and overflow, which are used to control program execution.\\n* Carry-out, which conveys the carry resulting from an addition operation, the borrow resulting from a subtraction operation, or the overflow bit resulting from a binary shift operation.\\n* Zero, which indicates all bits of Y are logic zero.\\n* Negative, which indicates the result of an arithmetic operation is negative.\\n* Overflow, which indicates the result of an arithmetic operation has exceeded the numeric range of Y.\\n* Parity, which indicates whether an even or odd number of bits in Y are logic one.\\n\",\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"data_all"},"text/html":["\n","  <div id=\"df-5b3df700-690e-42b8-bcc2-3cc287ffc23a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>transcript</th>\n","      <th>reading_material</th>\n","      <th>human_summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>in this video I'm going to define whether it's...</td>\n","      <td>Supervised learning (SL) is a paradigm in mach...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>our first learning algorithm will be linear re...</td>\n","      <td>In statistics, simple linear regression (SLR) ...</td>\n","      <td>An ALU (Arithmetic and Logic Unit) is the \"ma...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Welcome to the WebAudio API lesson! I personna...</td>\n","      <td>HTML5 Audio is a subject of the HTML5 specific...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Hi, Im Carrie Anne and welcome to Crash Cours...</td>\n","      <td>In mathematics and mathematical logic, Boolean...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Expression is a combination of objects and ope...</td>\n","      <td>Python is a high-level, general-purpose progra...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Hi, Im Carrie Anne and this is Crash Course C...</td>\n","      <td>In computer programming, machine code is compu...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Hi, Im Carrie Anne and welcome to CrashCourse...</td>\n","      <td>In electronics, computer science and computer ...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Hi, I'm Carrie Anne and welcome to Crash Cours...</td>\n","      <td>A programming language is a system of notation...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>You use Linux every day, whether you know it o...</td>\n","      <td>Linux ( LIN-uuks) is a family of open-source U...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Our story begins 20 years ago. Boris Yeltsin w...</td>\n","      <td>Linux ( LIN-uuks) is a family of open-source U...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>what this machine learning in this video we wi...</td>\n","      <td>Machine learning (ML) is a field of study in a...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Hi, Im Carrie Anne, this is Crash Course Comp...</td>\n","      <td>A central processing unit (CPU)also called a ...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>in this video we'll talk about the second majo...</td>\n","      <td>Unsupervised learning is a method in machine l...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Hello world, Im Carrie Anne, and welcome to C...</td>\n","      <td>The history of computing hardware covers the d...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Hi Im Carrie Anne, this is Crash Course Compu...</td>\n","      <td>A computer number format is the internal repre...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>&gt;&gt; Classical machine learning can take you so ...</td>\n","      <td>Deep learning is the subset of machine learnin...</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Hi, Im Carrie Ann and this is Crash Course Co...</td>\n","      <td>A computer is a machine that can be programmed...</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b3df700-690e-42b8-bcc2-3cc287ffc23a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-5b3df700-690e-42b8-bcc2-3cc287ffc23a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-5b3df700-690e-42b8-bcc2-3cc287ffc23a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-be2b5b93-c39d-4735-afe4-15362ca1297c\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-be2b5b93-c39d-4735-afe4-15362ca1297c')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-be2b5b93-c39d-4735-afe4-15362ca1297c button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["                                           transcript  \\\n","0   in this video I'm going to define whether it's...   \n","1   our first learning algorithm will be linear re...   \n","2   Welcome to the WebAudio API lesson! I personna...   \n","3   Hi, Im Carrie Anne and welcome to Crash Cours...   \n","4   Expression is a combination of objects and ope...   \n","5   Hi, Im Carrie Anne and this is Crash Course C...   \n","6   Hi, Im Carrie Anne and welcome to CrashCourse...   \n","7   Hi, I'm Carrie Anne and welcome to Crash Cours...   \n","8   You use Linux every day, whether you know it o...   \n","9   Our story begins 20 years ago. Boris Yeltsin w...   \n","10  what this machine learning in this video we wi...   \n","11  Hi, Im Carrie Anne, this is Crash Course Comp...   \n","12  in this video we'll talk about the second majo...   \n","13  Hello world, Im Carrie Anne, and welcome to C...   \n","14  Hi Im Carrie Anne, this is Crash Course Compu...   \n","15  >> Classical machine learning can take you so ...   \n","16  Hi, Im Carrie Ann and this is Crash Course Co...   \n","\n","                                     reading_material  \\\n","0   Supervised learning (SL) is a paradigm in mach...   \n","1   In statistics, simple linear regression (SLR) ...   \n","2   HTML5 Audio is a subject of the HTML5 specific...   \n","3   In mathematics and mathematical logic, Boolean...   \n","4   Python is a high-level, general-purpose progra...   \n","5   In computer programming, machine code is compu...   \n","6   In electronics, computer science and computer ...   \n","7   A programming language is a system of notation...   \n","8   Linux ( LIN-uuks) is a family of open-source U...   \n","9   Linux ( LIN-uuks) is a family of open-source U...   \n","10  Machine learning (ML) is a field of study in a...   \n","11  A central processing unit (CPU)also called a ...   \n","12  Unsupervised learning is a method in machine l...   \n","13  The history of computing hardware covers the d...   \n","14  A computer number format is the internal repre...   \n","15  Deep learning is the subset of machine learnin...   \n","16  A computer is a machine that can be programmed...   \n","\n","                                        human_summary  \n","0                                                      \n","1   An ALU (Arithmetic and Logic Unit) is the \"ma...  \n","2                                                 NaN  \n","3                                                 NaN  \n","4                                                 NaN  \n","5                                                 NaN  \n","6                                                 NaN  \n","7                                                 NaN  \n","8                                                 NaN  \n","9                                                 NaN  \n","10                                                NaN  \n","11                                                NaN  \n","12                                                NaN  \n","13                                                NaN  \n","14                                                NaN  \n","15                                                NaN  \n","16                                                NaN  "]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["data_all"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":6710,"status":"ok","timestamp":1709650662649,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"a9Ftu46k9trF","outputId":"0f6ab539-ba20-47bb-fc7d-8d85539a4a91"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["zero_shot_model_summ = generate_summary(data_all.iloc[4])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1709650663869,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"ZyZJo_bG-HtA","outputId":"a1fb7974-63ed-4ccd-e116-0eb33998fe89"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["The transcript discusses the concept of boolean data types in Python and explores boolean operations such as \"or\", \"and\", and \"not\". It explains how these operations work with True and False values, emphasizing the importance of capitalizing boolean type objects in Python. The transcript also delves into comparison operations in Python, highlighting that comparisons result in boolean types of True or False. It showcases examples of comparisons with numbers and lists, illustrating how Python determines equality and identity between objects. Furthermore, the transcript touches upon the distinction between object identity and object contents when comparing data structures like lists.\n","\n","From the reading material, we understand that Python is a high-level programming language known for its readability and comprehensive standard library. Developed in the late 1980s by Guido van Rossum, Python supports multiple programming paradigms and is dynamically typed. It has gained popularity in various domains, including machine learning and scientific computing. Python follows a design philosophy that promotes simplicity, and its syntax is user-friendly and uncluttered. The language's extensive library and community support make it suitable for web development, scientific computing, artificial intelligence, and more.\n","\n","To summarize, the transcript focuses on boolean operations and comparisons in Python, elaborating on how True and False values interact based on logical operators. On the other hand, the reading material provides an overview of Python's history, design philosophy, features, and popularity, highlighting its versatility and widespread adoption in various fields. Both sources underscore Python's strengths in readability, flexibility, and extensive library support.\n"]}],"source":["print(zero_shot_model_summ)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":7285,"status":"ok","timestamp":1709650676357,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"18TMryBz-LVM","outputId":"ec373364-e686-4a49-95f4-56d1685612f5"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["few_shot_model_summ_11 = generate_summary(data_all.iloc[4], model='ft:gpt-3.5-turbo-1106:ucsc:11fewshot20240304:8zG6iQyx')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":659},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1709650676357,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"3dR2P3WD-X3n","outputId":"b84da9fd-fc74-4002-9d11-fd221782b85a"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Expression in Python is a combination of objects and operators that compute a value. The boolean data type represents only two values, True and False. In Python, boolean values must be capitalized as True and False. When performing operations involving logic, such as boolean operations, you use operators to return boolean values; there are three boolean operators: \"or\", \"and\", and \"not\" in Python.\n","\n","Boolean operators:\n","- \"or\" returns True if either x is True or y is True, or both are True (e.g., True or False is True)\n","- \"and\" returns True only if both x and y are True (e.g., True and False is False)\n","- \"not\" negates the value of the object (e.g., not True is False)\n","\n","Comparison operations:\n","- Used to compare objects, resulting in boolean values\n","- A total of eight different comparison operations in Python\n","- Commonly used for numeric types and applies to other types, carrying out element-wise comparisons between sequences\n","\n","Example of numeric comparisons:\n","- '=' Tests for equality\n","- '<'\tStrictly less than\n","- '<='\tLess than or equal to\n","- '>'\tStrictly greater than\n","- '>='\tGreater than or equal to\n","- '!='\tNot equal\n","- 'is'\tObject identity\n","- 'is not'\tNegated object identity\n","\n","Determining object sameness:\n","- '==' tests whether two objects are equal\n","- 'is' tests whether two objects are the same object\n","\n","Comparing floating-point numbers and integers:\n","- Test equality (e.g., 2.0 == 2.0)\n","- Note that Python will implicitly convert the integer to a floating-point number to make the comparison\n","\n","Comparing list contents and object identity:\n","- Check if two lists are the same (e.g., list1 == list2)\n","- Check if two lists are the same object (e.g., list1 is list2)\n","\n","This methodology for determining sameness and equality applies to all types, not just numbers and lists. When asking whether two things are \"the same,\" they mean the same object. If they're asking whether they are not the same object, then they mean \"not the same object\"\n"]}],"source":["print(few_shot_model_summ_11)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":8846,"status":"ok","timestamp":1709653462986,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"I6O0S2EAHWbK","outputId":"13e7c9ab-12a8-4350-a354-8a876b177585"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["few_shot_model_summ_17 = generate_summary(data_all.iloc[4], model='ft:gpt-3.5-turbo-1106:ucsc:17fewshot20240304:8zHZ2yJ7')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":885},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1709653462987,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"iK8ixuGxH5lF","outputId":"f0b6480a-6dd8-454c-98f8-4fd74c5ec719"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Expressions are a combination of objects and operators that computes a value. Many expressions involve what is known as the boolean data type. In Python, you need to capitalize these words, True and False, for Python to understand them as boolean type.  If you fail to capitalize these words, Python will not recognize them as a boolean.\n","\n","Booleans are used in boolean operations, these operations take in one or more boolean object and then they return one boolean object back to you. There are only three boolean operations, which are `or`, `and`, and `not`. `Or` between x and y is going to be True if either x is True or y is True, or both are True. `And` is only true if both objects are True. `Not` simply negates the value of the object.\n","\n","```python\n","x = True\n","y = False\n","\n","x or y\n","# Output: True\n","\n","x and y\n","# Output: False\n","\n","not x\n","# Output: False\n","```\n","\n","We often need to compare objects in our programs. There are a total of eight different comparison operations in Python. Although these are commonly used for numeric types, we can actually apply them to other types as well. The result of a comparison like this is always a boolean type, either True or False.\n","\n","The comparion operators are as follows:\n","\n","- `==`: Equals\n","- `!=`: Not Equal\n","- `<`: Less than\n","- `<=`: Less than or equal to\n","- `>`: Greater than\n","- `>=`: Greater than or equal to\n","- `is`:  Object Identity\n","- `is not`: Negated Object Identity\n","\n","\n","```python\n","a = 2\n","b = 4\n","c = 4\n","\n","a < b\n","# Output: True\n","\n","b == c\n","# Output: True\n","\n","a is b\n","# Output: False\n","```\n","\n"]}],"source":["print(few_shot_model_summ_17)"]},{"cell_type":"markdown","metadata":{"id":"BnQ7tvo4JqIt"},"source":["### Fetch all the predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":201,"status":"ok","timestamp":1709652235546,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"PonB89tZJoxm","outputId":"e6b6b0e1-00cd-4fdc-8498-a7a9b162cc06"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["summaries = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["4fceddbca43f4bacb137ff72ccf3bccd","a3c986f728da4295a91b6f6ed7d3e202","cebf1183e400418d9e83bc820dcda22a","2bef13ca98234446b048668b1800b564","e74923dde44247c0bf427450a389bc07","3de2f1a4617348c08e29b520b3faf78f","ad466cf4a5c3407e9d76687def0da8cb","c8a9278bb5ff442bb08c2418c5549770","f8dbd021c197444792a4cd7ba45464fb","cc56e0f881ca46209df7409d823a6a1d","39dea7a67d6145a99fb8efb4d4131983"]},"executionInfo":{"elapsed":114460,"status":"ok","timestamp":1709652371126,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"CPDLaht-KjPP","outputId":"9039a482-4dc5-48c3-a2e8-d5ba421c96ff"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4fceddbca43f4bacb137ff72ccf3bccd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["human = []\n","zero_shot = []\n","few_shot_11 = []\n","few_shot_17 = []\n","\n","for i in tqdm(range(data.shape[0]-5, data.shape[0])):\n","    row = data.iloc[i]\n","    human.append(row['human_summary'])\n","    zero_shot.append(generate_summary(row, model = 'gpt-3.5-turbo-1106'))\n","    few_shot_11.append(generate_summary(row, model = 'ft:gpt-3.5-turbo-1106:ucsc:11fewshot20240304:8zG6iQyx'))\n","    few_shot_17.append(generate_summary(row, model = 'ft:gpt-3.5-turbo-1106:ucsc:17fewshot20240304:8zHZ2yJ7'))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":203,"status":"ok","timestamp":1709652387321,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"pJod45YQNeX2","outputId":"f021c1c8-8878-4e22-a4d5-ec62e0d064e9"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["summaries['human'] = human\n","summaries['zero_shot'] = zero_shot\n","summaries['few_shot_11'] = few_shot_11\n","summaries['few_shot_17'] = few_shot_17"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RLBgqDCeNsmv"},"outputs":[],"source":["summaries"]},{"cell_type":"markdown","metadata":{"id":"XFVhqUrlId3W"},"source":["### Graph on model scores"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":222,"status":"ok","timestamp":1709652453861,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"l4QzrIYII2z9","outputId":"920a129d-9326-4aea-c1a6-395fc621d0a9"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["models = ['zero_shot', 'few_shot_11', 'few_shot_17']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lsvJiZEiIgU9"},"outputs":[],"source":["scorer = BERTScorer(lang=\"en\")\n","\n","def get_bert_scores(text1, text2):\n","    _, _, sc = scorer.score(text1.tolist(), text2.tolist())\n","    return sc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["3512199abd034019ab65954d3d788111","f9b60d551cb94ad9b898756c12ad7e9b","1c223f1da6af4a5b9452df88b096c8b2","0152efe4647f48f3ab92aee2128d0ff6","db2386a80db54235b8b8a85f88509a71","e866d78d05eb45aba8e2aeaeadb8ccd2","acd75f5b614a4f28b7c8fcc4ed707f78","0fddb664de2a4a5cab653a285b679c1c","67f6e8cc10384687831c4360e873fb6c","368ae727c4bd4f2c91abe1ceb37403ec","9e61f4bfdc614321aaf0bb00469be38c"]},"executionInfo":{"elapsed":125404,"status":"ok","timestamp":1709652595724,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"wloUuQPoIpmM","outputId":"0cbcce6f-2ee9-4b61-832e-5fdde7e2b0d2"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3512199abd034019ab65954d3d788111","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_bertscore = pd.DataFrame()\n","\n","for mod in tqdm(models):\n","    model_bertscore[mod] = get_bert_scores(summaries['human'], summaries[mod])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UR3tZdnhK-WB"},"outputs":[],"source":["model_bertscore"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1120,"status":"ok","timestamp":1709652874959,"user":{"displayName":"Rithesh Kumar","userId":"04727885612481170992"},"user_tz":480},"id":"l1kjNTtlPdoP","outputId":"02b86cc8-d666-46fa-9d19-9ccc2d832163"},"outputs":[{"data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<Axes: >"]},"execution_count":91,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["model_bertscore.plot(kind='bar', ylim=[0.8, 1])"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOvjbWuW5EubZsy9CiHmU1B","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0152efe4647f48f3ab92aee2128d0ff6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_368ae727c4bd4f2c91abe1ceb37403ec","placeholder":"","style":"IPY_MODEL_9e61f4bfdc614321aaf0bb00469be38c","value":"3/3[02:05&lt;00:00,41.57s/it]"}},"0fddb664de2a4a5cab653a285b679c1c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c223f1da6af4a5b9452df88b096c8b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fddb664de2a4a5cab653a285b679c1c","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_67f6e8cc10384687831c4360e873fb6c","value":3}},"2bef13ca98234446b048668b1800b564":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc56e0f881ca46209df7409d823a6a1d","placeholder":"","style":"IPY_MODEL_39dea7a67d6145a99fb8efb4d4131983","value":"5/5[01:54&lt;00:00,21.13s/it]"}},"3512199abd034019ab65954d3d788111":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f9b60d551cb94ad9b898756c12ad7e9b","IPY_MODEL_1c223f1da6af4a5b9452df88b096c8b2","IPY_MODEL_0152efe4647f48f3ab92aee2128d0ff6"],"layout":"IPY_MODEL_db2386a80db54235b8b8a85f88509a71"}},"368ae727c4bd4f2c91abe1ceb37403ec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39dea7a67d6145a99fb8efb4d4131983":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3de2f1a4617348c08e29b520b3faf78f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fceddbca43f4bacb137ff72ccf3bccd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a3c986f728da4295a91b6f6ed7d3e202","IPY_MODEL_cebf1183e400418d9e83bc820dcda22a","IPY_MODEL_2bef13ca98234446b048668b1800b564"],"layout":"IPY_MODEL_e74923dde44247c0bf427450a389bc07"}},"67f6e8cc10384687831c4360e873fb6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9e61f4bfdc614321aaf0bb00469be38c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3c986f728da4295a91b6f6ed7d3e202":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3de2f1a4617348c08e29b520b3faf78f","placeholder":"","style":"IPY_MODEL_ad466cf4a5c3407e9d76687def0da8cb","value":"100%"}},"acd75f5b614a4f28b7c8fcc4ed707f78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ad466cf4a5c3407e9d76687def0da8cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c8a9278bb5ff442bb08c2418c5549770":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc56e0f881ca46209df7409d823a6a1d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cebf1183e400418d9e83bc820dcda22a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8a9278bb5ff442bb08c2418c5549770","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f8dbd021c197444792a4cd7ba45464fb","value":5}},"db2386a80db54235b8b8a85f88509a71":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e74923dde44247c0bf427450a389bc07":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e866d78d05eb45aba8e2aeaeadb8ccd2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8dbd021c197444792a4cd7ba45464fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f9b60d551cb94ad9b898756c12ad7e9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e866d78d05eb45aba8e2aeaeadb8ccd2","placeholder":"","style":"IPY_MODEL_acd75f5b614a4f28b7c8fcc4ed707f78","value":"100%"}}}}},"nbformat":4,"nbformat_minor":0}
