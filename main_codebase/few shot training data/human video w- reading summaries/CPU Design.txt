The instruction set contains expensive operations like divide, graphics, operations, encryption, etc. are implemented into the ALU hardware which does make design harder, but makes the system more capable. This extra circuitry makes the ALU bigger and more complicated to design, but also more capable - a complexity-for-speed tradeoff that has been made many times in computing history. 

High clock speeds and fancy instructions set lead to another problem: getting data in and out of the CPU quickly off. In this case, the bottleneck is RAM as data between RAM and CPU has to travel through bus lines. One solution to this problem is by connecting a small piece of RAM onto the CPU, which we call the cache. By having a cache, the RAM can transmit not just one single value, but a whole block of data. Instead of going back and forth between RAM, we have a chunk of saved data that is much easier for the CPU to access. When the requested data from the RAM already exists on the cache, we call that a cache hit, whereas we call it a cache miss when the information we want to access is not on the cache. Cache can also be used as a scratch space for intermediary value when performing large computations. Generally speaking, more cache means more performance, due to reduced stalling. 

With the implementation of the cache, we can also update the information much faster by changing the values in the cache rather than going back to the RAM, however, there now arises a different issue. Changes made to the data in cache are not replicated into RAM. The computer needs some way of knowing when the cache information is changed in order to change the information within RAM accordingly. The solution to this is a dirty bit. Each block of memory that the cache stores is given a flag known as the dirty bit which tells us whether we need to update the RAM data once we free the cache.

Another trick to boosting CPU performance is through instruction pipelining. Pipelining improves performance by allowing a number of instructions to work their way through the processor at the same time. We can utilize parallelization across the pipeline to improve throughput. Doing this allows us to better utilize the CPU to make sure we are doing as much work at once as possible. 

With this method, then comes the issues of dependence between instructions. Pipelined processors have to look ahead for data dependencies, and if necessary, stall their pipelines to avoid problems. A technique to combat this is called out-of-order execution, in which we dynamically reorder instructions with dependencies in order to minimize stalls and keep the pipeline moving.

Another big hazard with pipelining are conditional jump operations, which change the execution flow of a program depending on a value. Rather than performing a long stall, high end processors perform what is called speculative execution, in which the processor guesses which way they are going to go, and start filling their pipeline with instructions based on that guess. If they make a wrong guess however, they have to flush the pipeline. To minimize these effects of flushes, branch prediction is done, where the hardware makes educated guesses on whether a particular branch will be taken 

Outside of the techniques, there also comes the superscalar, multi-core and multi-processor systems, in which we increase the amount of cores, functional units like ALUs, or CPUs to increase the amount of instruction that can be run. 